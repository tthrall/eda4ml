# Dimension Reduction {#sec-reduce-dim}

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(
  echo    = FALSE, 
  error   = FALSE, 
  message = FALSE, 
  warning = FALSE
)
```

```{r}
#| label: CRAN-libraries

library(assertthat)
library(beans)
library(Cairo)
# library(cairoDevice)
library(corrplot)
# library(dslabs)
library(GGally)
# library(ggimg)
library(HistData)
library(here)
library(ISLR2)
# library(keras)
library(knitr)
# library(latex2exp)
# library(OECD)
# library(patchwork)
library(plotly)
# library(regclass)
# library(rgl)
# library(R.utils)
# library(reticulate)
# library(scatterplot3d)
# library(tensorflow)
library(tidymodels)
library(tidyverse)
# library(tinytex)
# library(UsingR)
```

```{r}
#| label: local-source

source(here("code", "d_ball.R"))
source(here("code", "galton_ht_data.R"))
# source(here("code", "mnist_file_mgmt.R"))
# source(here("code", "NCI60_helper.R"))
# source(here("code", "oecd_bli.R"))
# source(here("code", "pgram_grid.R"))
source(here("code", "vc_per_state.R"))
source(here("code", "wine_quality_uci.R"))
# source(here("code", "z_score.R"))
```

This chapter addresses the problem of finding and visualizing structure in high-dimensional data.  We begin the discussion with example data sets.

## Data Examples {#sec-dim-r-data-examples}

### US Arrests

```{r}
#| label: stats-per-us-state-1970s

state_stats_lst <- list_state_stats()

# (x, y) = (lat, long)
st_ctr_tbl <- 
  state_stats_lst$ st_ctr_tbl

# 8 state stats: Population, ..., Area
st_77_tbl <- 
  state_stats_lst$ st_77_tbl

# UrbanPop + 3 violent crimes: Assault, Murder, Rape
arrests_wide <- 
  state_stats_lst$ arrests_wide
arrests_long <- 
  state_stats_lst$ arrests_long

# compilation of the above
st_wide <- 
  state_stats_lst$ st_wide
st_wide_dscr <- 
  state_stats_lst$ st_wide_dscr
```

Data analysis often begins with an exploration, using tables and figures to learn more about the data before formulating specific questions or hypotheses.  The exploration becomes more difficult as the dimensions of the data set increase.  Here's an example.

@McNeil_1977 reviewed the relationship among violent crime statistics per US state and the percent of the population living in urban areas, variables described in @tbl-arrests-per-us-state-1973 below.  With just four variables, we can visualize the full data structure (e.g., using a scatter plot matrix as shown in @fig-arrests-2d-scatter-matrix).  However, the initial exploratory data analysis reveals strong correlations among the three crime variables (Murder, Assault, Rape), suggesting that the data may effectively lie in fewer than four dimensions.  Dimension reduction techniques can help us identify these underlying patterns, enabling us to (1) visualize state-level crime patterns in 2D or 3D, (2) identify states having similar crime profiles, and (3) understand whether crime variation is driven by one dominant factor or multiple independent factors.

@tbl-stats-per-us-state-1970s below describes related state-level statistics from the same period (the 1970s).  The data from both tables are available from the core `R` package `datasets`.

```{r}
#| label: tbl-arrests-per-us-state-1973

st_wide_dscr |> 
  dplyr::filter(
    var %in% c("Assault", "Rape", "Murder", "UrbanPop")
  ) |> 
  dplyr::rename(variable = var, description = dscr) |> 
  knitr::kable(
    caption = "Arrests per US State (1973)")

```

```{r}
#| label: tbl-stats-per-us-state-1970s

st_wide_dscr |> 
  dplyr::filter(
    ! (var %in% c("Assault", "Rape", "Murder", "UrbanPop"))
  ) |> 
  dplyr::rename(variable = var, description = dscr) |> 
  knitr::kable(
    caption = "Statistics per US State (1970s)")
```

@fig-arrests-violin below shows the distribution across the 50 states of each type of violent crime.  Rates are calcuated as arrests per 100,000 population.  The figure shows these rates on a $\log_{10}$ scale, since assault arrests are many times more common than rape or murder arrests.

```{r}
#| label: fig-arrests-violin
#| fig-cap: "Violent Crime Rates per US State (1973)"

g_arrests_violin <- arrests_long |> 
  dplyr::filter(var != "UrbanPop") |> 
  dplyr::mutate(var = forcats::as_factor(var)) |> 
  ggplot2::ggplot(mapping = aes(
    x = var, y = rate, color = var, fill = var
  )) + 
  geom_violin(show.legend = FALSE) + 
  scale_y_log10() + 
  labs(
    title = "Violent Crime Rates per US State (1973)", 
    subtitle = "arrests per 100K population"
  )

g_arrests_violin

```

The figure above gives a view of three data variables, that is, three columns of the data matrix.  @fig-arrests-3d-scatter below, a 3D scatterplot, gives a complementary view of each of the 50 states as a point whose coordinates are the respective arrest rates for assault, rape, and murder, with each point colored by the value of `UrbanPop` the percentage of the state's population living in an urban area.  Since individual states correspond to the rows of the data matrix, this figure is a row-based perspective on the data matrix.

```{r}
#| label: fig-arrests-3d-scatter
#| fig-cap: "Violent Crime Rates in each US State (1973)"

g_arrests_3d_scatter <- arrests_wide |> 
  plot_ly(
    x = ~Assault, 
    y = ~Rape, 
    z = ~Murder,
    type = "scatter3d",
    mode = "markers",
    text = ~arrests_wide$ st_abb,  # State abbreviation for hover
    marker = list(
      size = 5,
      color = ~UrbanPop,  # Optional: color by UrbanPop
      colorscale = "Viridis",
      showscale = TRUE,
      colorbar = list(title = "UrbanPop"))) |> 
  layout(scene = list(
    xaxis = list(title = "Assault"),
    yaxis = list(title = "Rape"),
    zaxis = list(title = "Murder")
  ),
  title = "Violent Crime Rates in each US State (1973)")

g_arrests_3d_scatter

```

@fig-arrests-2d-scatter-matrix below exemplifies a matrix of 2D scatter plots, a display method that accommodates more than 3 variables (but not many more, practically speaking). 

```{r}
#| label: fig-arrests-2d-scatter-matrix
#| fig-cap: "Arrests Variables: Relationships and Correlations"

g_arrests_2d_scatter_matrix <- arrests_wide |> 
  GGally::ggpairs(
    columns = c("Assault", "Rape", "Murder", "UrbanPop"),
    upper = list(continuous = wrap("cor", size = 3)),
    lower = list(continuous = wrap("points", alpha = 0.5, size = 0.8)),
    title = "Arrests Variables: Relationships and Correlations")

g_arrests_2d_scatter_matrix

```

This figure provides both column-based and row-based views of the data matrix, with the scatter diagrams for each pair of variables providing the row-based perspective.

### Dry Beans

@Koklu_Ozkan_2020_multiclass published a dataset of visual characteristics of dried beans "... in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera."  The resulting dataset contains 16 morphological features extracted from each bean image, including measures of area, perimeter, compactness, length, width, and various shape factors.

The classification goal is to predict bean variety from these morphological measurements.  Successful classification could have many applications, e.g., to improve sorting systems in agricultural processing.  However, many of these 16 features are inherently redundant: for example, area and perimeter are strongly correlated, as are various length and width measurements.  Dimension reduction can reveal whether bean varieties differ primarily in size, shape, or both, and whether a smaller subset of derived features might achieve comparable classification accuracy with greater interpretability and computational efficiency.

The data are available `R` package `beans`, containing a data matrix (tibble, more precisely) of dimension $13611 \times 17$.  The last column of the data matrix is response variable `class` that assigns one of seven types of bean to each bean-image.  The remaining 16 columns of the data matrix are morphological measurements (of shape and size).

@Kuhn_Silge_tmwr develop and evaluate different classification models for these data. [^beans-ttv-split]  They begin by examining the correlation coefficients for each pair of the 16 feature vectors.  @fig-beans-corrplot follows their example.

[^beans-ttv-split]: To develop and evaluate the classification models, the authors split the observations into three non-overlapping sets: training ($n = 10206$, nearly 75%), testing $(n = 1703)$, and validation $(n = 1702)$.

```{r}
#| label: beans-tbl

beans_tbl <- beans::beans

```

```{r}
#| label: fig-beans-corrplot
#| fig-cap: "beans: correlation among features"

# from tmwr:
# tmwr_cols <- grDevices::colorRampPalette(c("#91CBD765", "#CA225E"))
# col = tmwr_cols(200)

b_corr_colors <- grDevices::colorRampPalette(c("blue", "red"))

beans_corr_lst <- beans_tbl |> 
  dplyr::select(-class) |> 
  cor() |> 
  corrplot::corrplot(
    method        = "ellipse", 
    type          = "lower", 
    diag          = FALSE, 
    # order         = "hclust", 
    # hclust.method = "complete", 
    col           = b_corr_colors(200), 
    tl.col        = "black"
  )

# from vignette("corrplot-intro"): 
# (method = 'square', order = 'FPC', type = 'lower', diag = FALSE)

```

In this figure, the absolute value of each correlation coefficient is represented by the narrowness of the drawn ellipse and the depth of its color.  The sign of the correlation coefficient is represented by both the color and direction of the ellipse.

These size and shape features measure similar concepts. Consequently, several pairs of feature vectors are highly correlated, [^beans-correlation-xmpl] which offers the possibility of transforming the 16 original features into a smaller set without sacrificing classification power.

[^beans-correlation-xmpl]: For example, the features `area` and `convex_area` can differ in principle, but for the beans data the correlation is `r beans_corr_lst$corr[["convex_area", "area"]] |> round(digits = 5)`.

### Wine Quality

```{r}
#| label: get-wine-quality

wine_quality <- get_wine_quality()

# lubridate::now()
# [1] "2025-11-04 12:46:12 GMT"

# wine_quality |> str()
# spc_tbl_ [6,497 Ã— 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
#  $ fixed acidity       : num [1:6497] 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...
#  $ volatile acidity    : num [1:6497] 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...
#  $ citric acid         : num [1:6497] 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...
#  $ residual sugar      : num [1:6497] 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...
#  $ chlorides           : num [1:6497] 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...
#  $ free sulfur dioxide : num [1:6497] 11 25 15 17 11 13 15 15 9 17 ...
#  $ total sulfur dioxide: num [1:6497] 34 67 54 60 34 40 59 21 18 102 ...
#  $ density             : num [1:6497] 0.998 0.997 0.997 0.998 0.998 ...
#  $ pH                  : num [1:6497] 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
#  $ sulphates           : num [1:6497] 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
#  $ alcohol             : num [1:6497] 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
#  $ quality             : num [1:6497] 5 5 5 6 5 5 5 7 7 5 ...
#  $ color               : chr [1:6497] "red" "red" "red" "red" ...
#  - attr(*, "spec")=
#   .. cols(
#   ..   `fixed acidity` = col_double(),
#   ..   `volatile acidity` = col_double(),
#   ..   `citric acid` = col_double(),
#   ..   `residual sugar` = col_double(),
#   ..   chlorides = col_double(),
#   ..   `free sulfur dioxide` = col_double(),
#   ..   `total sulfur dioxide` = col_double(),
#   ..   density = col_double(),
#   ..   pH = col_double(),
#   ..   sulphates = col_double(),
#   ..   alcohol = col_double(),
#   ..   quality = col_double()
#   .. )
#  - attr(*, "problems")=<externalptr>
```

```{r}
#| label: wq-dimensions

# capture dimensions to embed in narrative
wq_nrow <- nrow(wine_quality)
wq_ncol <- ncol(wine_quality)
wq_n_red <- wine_quality |> 
  dplyr::filter(color == "red") |> nrow()
wq_n_white <- wine_quality |> 
  dplyr::filter(color == "white") |> nrow()
```

@Cortez_2009_ModelingWP model wine preference as a function of `r wq_ncol - 1L` physicochemical properties of wine, which are listed in @tbl-wq-var-dscr. The `quality` score is the median of three expert tastings. The data consist of `r wq_nrow` wines, `r wq_n_red` red and `r wq_n_white` white. [^wq-uci-repo] [^red-v-white-wine]

[^wq-uci-repo]: The authors contributed the data (now archived) to the UC Irvine Machine Learning Repository.  See @wine_quality_186.

[^red-v-white-wine]: The wines in this study are all from the Minho (northwest) region of Portugal.  Medium in alcohol, it is appreciated for its freshness (especially in summer). At the time of writing the authors report that Minho wine accounts for 15% of total Portuguese wine production, of which some 10% is exported, mostly white wine.

The modeling goal is to understand the chemical properties influencing wine quality ratings and to predict quality from objective laboratory measurements.  Apart from wine color (red or white), the remaining `r wq_ncol - 2L` physicochemical features include related measurements, e.g., pH, fixed acidity, and citric acid are chemically interrelated, as are free and total sulfur dioxide.  Dimension reduction can help us: (1) visualize the chemical space that wines occupy in 2D or 3D, (2) identify whether wine quality varies along a small number of chemical gradients (e.g., acidity vs. alcohol content), (3) understand the chemical differences between red and white wines, and (4) determine whether a smaller set of derived features might predict quality as well as the full set of measurements.

```{r}
#| label: tbl-wq-var-dscr

wq_var_dscr <- describe_wq_vars()

wq_var_dscr |> 
  dplyr::rename(variable = var) |> 
  knitr::kable(
    caption = "Wine: physicochemical properties")
```

Correlations among the `r wq_ncol - 2L` numeric features are shown in @fig-rw-wq-corrplot below for red and white wines, respectively.

```{r}
#| label: fig-rw-wq-corrplot
#| fig-cap: "Wine: correlation among numeric features"
#| fig-subcap: 
#|   - "Red wines"
#|   - "White wines"
#| layout-ncol: 2

wq_corr_colors <- grDevices::colorRampPalette(c("blue", "red"))

red_wq_corr_lst <- wine_quality |> 
  dplyr::filter(color == "red") |> 
  dplyr::select(-c(quality, color)) |> 
  cor() |> 
  corrplot::corrplot(
    method        = "ellipse", 
    type          = "lower", 
    diag          = FALSE, 
    col           = wq_corr_colors(200), 
    tl.col        = "black"
  )

white_wq_corr_lst <- wine_quality |> 
  dplyr::filter(color == "white") |> 
  dplyr::select(-c(quality, color)) |> 
  cor() |> 
  corrplot::corrplot(
    method        = "ellipse", 
    type          = "lower", 
    diag          = FALSE, 
    col           = wq_corr_colors(200), 
    tl.col        = "black"
  )

```

These figures reveal some substantial correlations, with distinct patterns per wine color.

@tbl-rw-quality-corr below shows feature-quality correlations for red and white wines, respectively.

```{r}
#| label: rw-quality-corr-tbl

rw_quality_corr_tbl <- wine_quality |> 
  get_rw_quality_corr_tbl()

```

```{r}
#| label: tbl-rw-quality-corr
#| tbl-cap: "Feature-quality correleations among (red, white) wines"

rw_quality_corr_tbl |> 
  knitr::kable(
    caption = "Feature-quality correleations", 
    digits = 2
  )

```

We see that alcohol volume is a prominent indicator of quality for both red and white wines.  On the other hand, citric acid and sulphates are strongly correlated with the quality of red wine, but not white.

As demonstrated by @Cortez_2009_ModelingWP, these data patterns offer the possibility of developing a smaller set of features as predictors of wine quality.

### Cancer Genomics (NCI60)

```{r}
#| label: NCI60-structure

# data dimensions
nrow_NCI60       <- ISLR2::NCI60$ data |> nrow()
ncol_NCI60       <- ISLR2::NCI60$ data |> ncol()

# row labels
lbl_NCI60        <- ISLR2::NCI60$ labs
lbl_unique_NCI60 <- lbl_NCI60 |> 
  unique() |> sort()
n_u_labels_NCI60 <- length(lbl_unique_NCI60)
lbl_ct_NCI610    <- lbl_NCI60 |> 
  tibble::as_tibble_col(column_name = "label") |> 
  dplyr::summarise(.by = "label", ct = n()) |> 
  dplyr::arrange(label)
lbl_ct_vec_NCI610 <- lbl_ct_NCI610 |> 
  dplyr::pull(name = label)
```

The NCI60 data [^ISLR2-NCI60] [^Ross-2000-citation] consists of gene expression measurements from `r nrow_NCI60` cancer cell lines. For each cell line, expression levels were measured for `r ncol_NCI60` genes using microarray technology. The cell lines represent `r n_u_labels_NCI60` different cancer types, including leukemia, melanoma, and cancers of the colon, breast, ovary, lung, and central nervous system. This data set is a canonical example of a high-dimensional data matrix where the number of features $(d)$ greatly exceeds the number $n$ of data cases: $d \gg n$.

[^ISLR2-NCI60]: The data are available from `R` package `ISLR2`.  The vector of labels (cancer type per observation) can be accessed as `ISLR2::NCI60$labs`.  The data can be accessed as `ISLR2::NCI60$data` in the form of a $64 \times 6830$ matrix with column names "1", "2", ..., "6830" rather than gene identifiers.  While this simplification is adequate for illustrating dimension reduction methods, readers requiring gene-level biological interpretation should consult the `rcellminer` and `rcellminerData` Bioconductor packages (Luna et al., 2016; Reinhold et al., 2019), which provide the same data with complete gene annotations and represent the current standard for programmatic access to NCI-60 molecular profiling data.

[^Ross-2000-citation]: The `ISLR2` package cites @Ross_2000_Systematic as the source of `ISLR2::NCI60`.

The data set is part of the NCI-60 panel and associated datasets, which are maintained by the Frederick National Laboratory for Cancer Research (FNLCR) and the National Cancer Institute's (NCI) Developmental Therapeutics Program (DTP).  The data serve as a publicly available platform for the global cancer research community to study tumor biology, evaluate new bioinformatics approaches, and select appropriate cell models for specific research questions.

These data present challenges that require dimension reduction, that is, the transformation of the set of `r ncol_NCI60` genomic features into a smaller set that capture the dominant patterns of variation.  We proceed to describe such challenges.

## Dimensionality: Curse and Blessing {#sec-dim-r-curse}

As just noted, the NCI60 data set, with $d =$ `r ncol_NCI60` and $n =$ `r nrow_NCI60`, presents challenges that make dimension reduction essential. Several issues arise with any "wide" data set in which $d \gg n$, and include the following.

  - _Visualization_: We cannot plot points in  $d$ dimensions to explore patterns or detect outliers.
  
  - _Over-fitting_: With $d \gg n$, infinitely many coefficient vectors $\beta_\bullet$ produce identical predictions that replicate the response variable within the training data.  This makes model selection impossible without regularization.
  
  - _Computation_: Storing and manipulating a $d \times d$ covariance matrix becomes prohibitively expensive.

In 1957 Richard Bellman characterized such challenges as "the curse of dimensionality". [^dim-curse]

[^dim-curse]: See @Bellman_1957_dynamic, @Donoho_2000_High_Dimensional_Data_Analysis, and @wiki_curse_dimensionality.

@Donoho_2000_High_Dimensional_Data_Analysis points out that: (1) many established statistical methods assume $d < n$; and (2) we can expect $d \gg n$ to occur more and more often as data-collection is increasingly automated to retrieve all potentially useful details for subsequent screening.

In addition, @wiki_curse_dimensionality notes that our geometric intuition is grounded in $d \le 3$ and is overturned as $d$ increases.  To illustrate, consider a unit hypercube $[0,1]^d$ containing the largest possible inscribed ball. The ratio $r(d)$ of their volumes shrinks dramatically: $r(3) \approx 0.52$, $r(10) \approx 0.0025$, and $r(100) \approx 2 \times 10^{-70}$. In high dimensions, the preponderance of randomly generated points within the cube land far from the center!

Yet @Donoho_2000_High_Dimensional_Data_Analysis also sees opportunities.   While randomly generated data in high dimensions behaves pathologically, actual data tend to be more coherent.  Consider the example data sets:

- _US Arrests_: The three crime variables (assault, rape, murder) are 
  highly correlated; they don't independently span 3D space (@fig-arrests-3d-scatter).
- _Dry_Beans_: The 16 shape measurements aren't independent; they reflect underlying bean geometry.  
- _Wine quality_: The 11 chemical properties are constrained by 
  fermentation chemistry.
- _NCI60_: The 6,830 genes participate in shared biological pathways.

In each case, the data likely occupies a much lower-dimensional structure within the high-dimensional feature space. If we can identify this structure, dimension reduction may actually _improve_ modeling by revealing the true degrees of freedom in the data.

The remainder of this chapter develops methods that exploit such opportunities. We begin with Principal Component Analysis (@sec-dim-r-pca), which finds low-dimensional approximations to high-dimensional data through eigen-decomposition. We then explore how supervision (@sec-dim-r-supervised) can guide dimension reduction when prediction is the goal. Finally, we examine computational strategies (@sec-dim-r-computation) for extreme cases like NCI60 where $d \gg n$.

## Principal Component Analysis {#sec-dim-r-pca}

Recall the US Arrests dataset with four crime-related variables. Can we represent these 50 states in fewer than four dimensions while preserving most of the information? Principal Component Analysis (PCA) answers this by finding new variables, linear combinations of the original features, that capture maximum variance.

### 2D Example

To develop some geometric intuition we begin with a 2D example.  @fig-fs-pca below illustrates PCA for (father, son) centered heights from the Galton data presented in @sec-la-intro.

```{r}
#| label: fms-galton

galton_3d_lst <- get_galton_3d()

# tibble(father, mother, child, gender)
# (child, gender) refers to oldest child
galton_3d <- galton_3d_lst$ galton_3d

fms_tbl <- galton_3d |> 
  dplyr::filter(gender == "male") |> 
  dplyr::select(- gender) |> 
  dplyr::rename(son = child)

# center data variables using base::scale()
fms_ctr <- fms_tbl |> 
  dplyr::mutate(dplyr::across(
    .cols = c(mother, father, son), 
    .fns  = scale, scale = FALSE
  )) |> 
  # change scale() output from matrix to vector
  dplyr::mutate(dplyr::across(
    .cols = c(mother, father, son), 
    .fns  = as.vector
  ))

```

```{r}
#| label: fs-pca

# PCA(father, son)
fs_pca_lst <- fms_ctr |> 
  dplyr::select(father, son) |> 
  stats::prcomp(center = FALSE)

# rotation matrix
fs_rot_mat <- fs_pca_lst$ rotation
# fs_rot_mat
#               PC1        PC2
# father -0.7041940  0.7100076
# son    -0.7100076 -0.7041940

# slope coefficients for geom_abline()
fs_slope_1 <- 
  fs_rot_mat [["son",    "PC1"]] / 
  fs_rot_mat [["father", "PC1"]]
fs_slope_2 <- 
  fs_rot_mat [["son",    "PC2"]] / 
  fs_rot_mat [["father", "PC2"]]

```

```{r}
#| label: fig-fs-pca
#| fig-cap: "Principal Components: (father, son) centered heights"

g_fs_pcs <- fms_ctr |> 
  dplyr::select(father, son) |> 
  ggplot2::ggplot(mapping = aes(
    x = father, y = son
  )) + 
  ggplot2::geom_point() + 
  ggplot2::coord_fixed(ratio = 1) + 
  ggplot2::geom_abline(
    intercept = 0, 
    slope = fs_slope_1, 
    color = "red") + 
  ggplot2::geom_abline(
    intercept = 0, 
    slope = fs_slope_2, 
    color = "blue") + 
  ggplot2::labs(
    title = "PCA: (father, son) centered heights") + 
  ggplot2::labs(
    subtitle = bquote(
      z[1] * " in red, " * z[2] * " in blue"
    )
  )

g_fs_pcs

```

The figure represents two principal components, $(z_1, z_2)$, as respective red and blue lines, which are perpendicular to one another.

The red line $(z_1)$ points in the direction where the data varies most. Projecting points onto this line captures the maximum possible variance in a single dimension. The blue line $(z_2)$ is perpendicular to $z_1$ and captures the maximum remaining variance. Together, they form a rotated coordinate system that aligns with the data's natural variation.

### Linear Algebra Formulation

To define principal components we will describe a step-wise reconstruction of the $n \times d$ feature matrix $X_{\bullet, \bullet}$.

#### Preliminaries

First, to simplify notation, we assume $X_{\bullet, \bullet}$ to have already been centered: for each column of the original feature matrix the average value of the column elements has been calculated and subtracted.  Therefore the average (or equivalently the sum) of each column of $X_{\bullet, \bullet}$ equals zero.

$$
\begin{align} 
  X_{\bullet, \bullet} &= (x_{\bullet, 1}, \ldots, x_{\bullet, d}) \\ \\ 
  & \text{with} \\ \\ 
  1_\bullet^\top \; x_{\bullet, k} &= \sum_{i = 1}^n x_{i, k} = 0 \\ \\ 
  &\text{for } k \in \{ 1, \ldots, d \} \\ \\ 
  & \text{so that} \\ \\ 
  1_\bullet^\top \; X_{\bullet, \bullet} &= 0_\bullet \in \mathbb{R}^d
\end{align} 
$$ {#eq-X-ctr-cols}

As a consequence, the $d \times d$ matrix $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$ is a multiple of the sample covariance matrix among features.

$$
\begin{align} 
  cov \left ( X_{\bullet, \bullet} \right ) 
  &= \frac{1}{n - 1}
  \left ( 
  X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}
  \right )
\end{align} 
$$ {#eq-cov-X-mat}

In particular, the variance of each feature vector is proportional to its squared norm.

$$
\begin{align} 
  var( x_{\bullet, k} ) 
  &= \frac{1}{n - 1}
  \lVert 
    x_{\bullet, k}
  \rVert^2
\end{align} 
$$ {#eq-feature-variance-equals-norm2}

Although matrix $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$ is a square matrix having dimensions $d \times d$, its _rank_ may be less than $d$.  Let $r = rank \left ( X_{\bullet, \bullet} \right )$.  Then $r$ is also the rank of $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$.  It is the dimension of the subspace of $\mathbb{R}^d$ spanned by the rows of $X_{\bullet, \bullet}$, as well as the dimension of the subspace of $\mathbb{R}^n$ spanned by the columns of $X_{\bullet, \bullet}$.  Consequently $r \le \min(n, d)$.

Our construction of principal components also relies on the notion of an orthogonal projection $P$ from $\mathbb{R}^d$ to some subspace of $\mathbb{R}^d$.  Recall that $P$ is an orthogonal projection if and only if: it is idempotent $(P^2 = P)$; and symmetric  $(P^\top = P)$.  Also, if $P$ is an orthogonal projection then so is its complement $\mathcal{I} - P$.

The simplest example is the $d \times d$ matrix $v_\bullet \; v_\bullet^\top$, where $v_\bullet \in \mathbb{R}^d$ is a unit vector (a vector of unit norm).  This sends $x_\bullet \in \mathbb{R}^d$ to a scalar multiple of $v_\bullet$, namely $\left < x_\bullet, v_\bullet \right > v_\bullet$. [^inner-product-notation]

[^inner-product-notation]: Among the notations for the inner product (scalar product) of vectors $x, y$ belonging to the same Euclidean space are: (1) $\left < x, y \right>$; and (2) $x^\top \; y$.  The former is less ambiguous.  The latter conforms to the notation of matrix multiplication; it is useful and perhaps more intuitive.  The ambiguity may occur when at least one of $x, y$ is a row or a column of some matrix.  If $x$ is a row vector and $y$ is a column vector, then the inner product of $x, y$ should be written as $x \; y$ according to matrix notation.   Nevertheless the matrix multiplication notation is useful.  For example we use $u \; u^\top$ to denote an orthogonal projection matrix, not a scalar product, when $u$ is a unit vector.

To any projection $P$ defined on $\mathbb{R}^d$ we apply the following loss function $\mathcal{L} (\cdot)$ in order to measure how well $P$ reproduces the rows of the feature matrix $X_{\bullet, \bullet}$.

$$
\begin{align} 
  \mathcal{L} \left ( P \right ) 
  &= 
    \sum_{i = 1}^n 
    \; \left \lVert 
      x_{i, \bullet} \; - \; P \; x_{i, \bullet}
    \right \rVert^2 \\ 
  &= 
    \sum_{i = 1}^n 
    \; \left \lVert 
    \left ( 
      \mathcal{I} \; - \; P 
    \right )
      \; x_{i, \bullet}
    \right \rVert^2 \\ 
  &= 
    \sum_{i = 1}^n 
      \left < 
        x_{i, \bullet}, \; 
        \left (
          \mathcal{I} \; - \; P
        \right ) \; 
        x_{i, \bullet}
      \right > \\ 
  &= 
    \sum_{i = 1}^n 
      \left < 
        x_{i, \bullet}, \; 
        x_{i, \bullet}
      \right > \; - \; 
    \sum_{i = 1}^n 
      \left < 
        x_{i, \bullet}, \; 
        P \; 
        x_{i, \bullet}
      \right >  \\ 
  &= 
    \sum_{i = 1}^n 
      \lVert 
        x_{i, \bullet}
      \rVert^2 \; - \; 
    \sum_{i = 1}^n 
      \left < 
        x_{i, \bullet}, \; 
        P \; 
        x_{i, \bullet}
      \right >  \\ 
  &= 
    \mathrm{Tr} \left \{ 
      X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} 
      \right \} \; - \; 
    \sum_{i = 1}^n 
      \left < 
        x_{i, \bullet}, \; 
        P \; 
        x_{i, \bullet}
      \right > \\ \\ 
    & \text{subject to the constraint:} \\ \\ 
    & P \text{ is an orthogonal projection defined on } \mathbb{R}^d
\end{align} 
$$ {#eq-row-projection-squared-error}

Thus $\mathcal{L} (P)$ is a sum over row index $i$ of squared residuals in the approximation of $x_{i, \bullet}$ by $P \; x_{i, \bullet}$.

Now let $\mathcal{P}$ be a set of projections.  From @eq-row-projection-squared-error we see that projection $\hat{P} \in \mathcal{P}$ minimizes $\mathcal{L} (P)$ if and only if it maximizes a corresponding quadratic form: 

$$
\begin{align} 
  &  
  \arg \min_{ P \in \mathcal{P} } \; 
  \mathcal{L} ( P ) \\ 
  &= 
  \arg \max_{ P \in \mathcal{P} } \; 
  \sum_{i = 1}^n 
    \left < 
      x_{i, \bullet}, \; 
      P \; 
      x_{i, \bullet}
    \right > 
\end{align} 
$$ {#eq-min-loss-equals-max-quad-form}

#### Outline: reconstructing the feature matrix

Here's an outline of the reconstruction processs.  At step $j$ we define vector $v_{\bullet, j} \in \mathbb{R}^d$ that has unit norm and is orthogonal to any vector $v_{\bullet, \eta}$ defined in any previous step.  At the end of step $j$ we have accumulated an orthonormal set of vectors $(v_{\bullet, 1}, \ldots, v_{\bullet, j})$ that span a $j-$dimensional subspace, say $\mathcal{R}^{(j)}$, of $\mathbb{R}^d$.  This orthonormal set defines the following orthogonal projection $P^{(j)}$ from $\mathbb{R}^d$ to $\mathcal{R}^{(j)}$.

$$
\begin{align} 
  P^{(j)} \; x_\bullet 
  &= 
    \sum_{\eta = 1}^j 
      \left < x_\bullet, v_{\bullet, \eta} \right > 
      \times v_{\bullet, \eta} \\ 
  &= 
    \sum_{\eta = 1}^j 
      v_{\bullet, \eta} \times 
      \left < v_{\bullet, \eta}, x_\bullet \right >  \\ 
  &= 
    \sum_{\eta = 1}^j 
      v_{\bullet, \eta} \; v_{\bullet, \eta}^\top \; x_\bullet
\end{align} 
$$ {#eq-row-supspace-ortho-projection}

In other words, $P^{(j)}$ is the sum of orthogonal projections, $v_{\bullet, \eta} \; v_{\bullet, \eta}^\top$, to 1-dimensional subspaces that are mutually orthogonal.

Since we will apply our loss function $\mathcal{L} (P)$ to $P^{(j)}$, it will be useful to re-express the quadratic form appearing in @eq-min-loss-equals-max-quad-form. [^row-vector-of-feature-matrix]

[^row-vector-of-feature-matrix]: Note that $x_{i, \bullet} \in \mathbb{R}^d$ is a row vector, namely the $i^{th}$ row of feature matrix $X_{\bullet, \bullet}$.  Therefore the inner product $\left < x_{i, \bullet}, \; P \; x_{i, \bullet} \right >$ is expressed as $x_{i, \bullet} \; P \; x_{i, \bullet}^\top$ in matrix multiplication notation.

$$
\begin{align} 
  & 
  \sum_{i = 1}^n 
    \left < 
      x_{i, \bullet}, \; 
      P^{(j)} \; 
      x_{i, \bullet}
    \right > \\ 
  &= 
  \sum_{i = 1}^n 
      x_{i, \bullet} \; 
      P^{(j)} \; 
      x_{i, \bullet}^\top \\ 
  &= 
  \sum_{i = 1}^n 
      x_{i, \bullet} \; 
      \left ( 
        \sum_{\eta = 1}^j 
            v_{\bullet, \eta} \; v_{\bullet, \eta}^\top \; 
      \right )
      x_{i, \bullet}^\top \\ 
  &= 
  \sum_{i = 1}^n 
    \sum_{\eta = 1}^j 
      x_{i, \bullet} \; 
      v_{\bullet, \eta} \; 
      v_{\bullet, \eta}^\top \; 
      x_{i, \bullet}^\top \\ 
  &= 
  \sum_{\eta = 1}^j 
    \sum_{i = 1}^n  \; 
      v_{\bullet, \eta}^\top \; 
      x_{i, \bullet}^\top
      x_{i, \bullet} \; 
      v_{\bullet, \eta} \\ 
  &= 
  \sum_{\eta = 1}^j 
    v_{\bullet, \eta}^\top \; 
    \left ( 
    \sum_{i = 1}^n  \; 
      x_{i, \bullet}^\top
      x_{i, \bullet} \; 
    \right )
    v_{\bullet, \eta} \\ 
  &= 
  \sum_{\eta = 1}^j 
    v_{\bullet, \eta}^\top \; 
    \left ( 
      X_{\bullet, \bullet}^\top \; 
      X_{\bullet, \bullet}
    \right )
    v_{\bullet, \eta}
\end{align} 
$$ {#eq-projection-quad-form-as-sum}

#### Details: reconstructing the feature matrix

Here are the distinctive details of the reconstruction process.  At step 1 we define vector $v_{\bullet, 1}$ as the unit vector that minimizes $\mathcal{L} (v_\bullet \; v_\bullet^\top)$ among all unit vectors $v_\bullet \in \mathbb{R}^d$.  From @eq-min-loss-equals-max-quad-form and @eq-projection-quad-form-as-sum we have: 

$$
\begin{align} 
  v_{\bullet, 1}
  &=  
  \arg \min_{ \Vert v_\bullet \rVert = 1 } \; 
  \mathcal{L} ( v_\bullet \; v_\bullet^\top ) \\ 
  &= 
  \arg \max_{ \Vert v_\bullet \rVert = 1 } \; 
  \sum_{i = 1}^n 
    \left < 
      x_{i, \bullet}, \; 
      v_\bullet \; v_\bullet^\top \; 
      x_{i, \bullet} 
    \right > \\ 
  &= 
  \arg \max_{ \Vert v_\bullet \rVert = 1 } \; 
  v_\bullet^\top \; 
  \left ( 
    X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} 
  \right ) \;
  v_\bullet
\end{align} 
$$ {#eq-max-quad-form-step-1}

The solution to @eq-max-quad-form-step-1 is well known.  The maximal value of $v_\bullet^\top \; X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} \; v_\bullet$ is the largest eigenvalue, say $\sigma_1^2$, of $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$.  And the vector $v_{\bullet, 1}$ that achieves that maximal value is the eigenvector corresponding to $\sigma_1^2$.

Having defined $v_{\bullet, 1}$, we proceed inductively, again with $r$ defined as $r = rank \left ( X_{\bullet, \bullet} \right )$.  Suppose that we have defined an orthonormal set of vectors $v_{\bullet, 1}, \ldots, v_{\bullet, j}$, where $1 \le j \le r$.  As previously noted, this orthonormal set of vectors spans a $j-$dimensional $\mathbb{R}^d$ subspace, denoted $\mathcal{R}^{(j)}$, and also defines an orthogonal projection $P^{(j)}$ from $\mathbb{R}^d$ to $\mathcal{R}^{(j)}$.

If $j = r$ we terminate the reconstruction.  Otherwise we define the next vector, $v_{\bullet, j + 1}$, as the unit vector that minimizes $\mathcal{L} (Q^{(j + 1)})$, where $Q^{(j + 1)} = P^{(j)} \; + \; v_{\bullet} \; v_{\bullet}^\top$.

$$
\begin{align} 
  \mathcal{L} (Q^{(j + 1)})
  &= 
    \sum_{i = 1}^n 
    \left \lVert 
    \left ( 
      \mathcal{I} \; - \; 
      Q^{(j + 1)} (v_{\bullet} \; v_\bullet^\top) \; 
    \right ) 
    x_{i, \bullet} 
    \right \rVert^2 \\ \\ 
  &\text{where} \\ \\
  Q^{(j + 1)} (v_{\bullet} \; v_{\bullet}^\top) &= P^{(j)} \; + \; v_{\bullet} \; v_{\bullet}^\top \\ \\ 
  & \text{subject to the constraints: } \\ \\ 
  & \lVert v_\bullet \rVert = 1 \; \text{ and } \; v_\bullet \perp v_{\bullet, \eta} \; \text{ for } \; \eta \le j
\end{align} 
$$ {#eq-next-row-projection-error-a}

Note that the constraints on $v_\bullet$ ensure that $Q^{(j + 1)} (v_{\bullet} \; v_{\bullet}^\top)$ is an orthogonal projection.  Then, abbreviating and expanding @eq-next-row-projection-error-a, we have: 

$$
\begin{align} 
  \mathcal{L}_{j + 1} \left (
    v_{\bullet} \; v_{\bullet}^\top
  \right ) 
  &= 
    \sum_{i = 1}^n 
    \left \lVert 
    \left ( 
      \mathcal{I} \; - \; 
      Q^{(j + 1)} \; 
    \right ) 
    x_{i, \bullet} 
    \right \rVert^2 \\ 
  &= 
    \mathrm{Tr} \left \{ 
      X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} 
      \right \} \; - \; 
    \sum_{\eta = 1}^j
    v_{\bullet, \eta}^\top \; 
      X_{\bullet, \bullet}^\top \;
      X_{\bullet, \bullet} \; 
    v_{\bullet, \eta} \; - \; 
    v_\bullet^\top \; 
      X_{\bullet, \bullet}^\top \;
      X_{\bullet, \bullet} \; 
    v_\bullet
\end{align} 
$$ {#eq-next-row-projection-error-b}

It now follows that 

$$
\begin{align} 
  v_{\bullet, j + 1} 
  &= 
  \arg \max \; 
  \left \{ 
    v_\bullet^\top \; 
      X_{\bullet, \bullet}^\top \;
      X_{\bullet, \bullet} \; 
    v_\bullet 
  \right \} \\ \\ 
  &\text{subject to: } \\ \\ 
    &\lVert v_\bullet \rVert = 1 \; 
    \text{ and } \; 
    v_\bullet \perp v_{\bullet, \eta} \; \text{ for } \eta \le j
\end{align} 
$$ {#eq-maximize-quad-form-b}

Again, this problem of constrained maximization of a quadratic form is well known.  The maximum value is $\sigma_{j + 1}^2$, the eigenvalue of $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$ of index $j + 1$ in descending order.  The vector $v_{\bullet, j + 1}$ achieving this maximum is the eigenvector corresponding to $\sigma_{j + 1}^2$.

This reconstruction process terminates with an orthonormal set of $d-$dimensional vectors, $(v_{\bullet, 1}, \ldots, v_{\bullet, r})$, that span subspace $\mathcal{R}^{(r)}$ that is spanned by the rows of $X_{\bullet, \bullet}$.

Now let $D$ denote the $r \times r$ diagonal matrix $diag(\sigma_1, \ldots, \sigma_r)$.  From @eq-feature-variance-equals-norm2 and @eq-next-row-projection-error-b we have 

$$
\begin{align} 
  & 
    (n - 1) \; 
    \sum_{k = 1}^d var(x_{\bullet, k})  \\ 
  &= 
    \sum_{k = 1}^d \; \lVert x_{\bullet, k} \rVert^2  \\ 
  &= 
    \mathrm{Tr} 
    \left ( 
      X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} 
    \right )  \\ 
  &= 
    \sum_{\eta = 1}^r 
    v_{\bullet, \eta}^\top \; 
      X_{\bullet, \bullet}^\top \;
      X_{\bullet, \bullet} \; 
    v_{\bullet, \eta} \\ 
  &= 
    \sum_{\eta = 1}^r 
    v_{\bullet, \eta}^\top \; 
    ( \sigma_\eta^2 \; v_{\bullet, \eta} ) \\ 
  &= 
    \sum_{\eta = 1}^r \sigma_\eta^2 \\ 
  &= \mathrm{Tr} \left ( D^2 \right )  \\ \\ 
  &\text{where} \\ \\ 
  & D = diag(\sigma_1, \ldots, \sigma_r) 
\end{align} 
$$ {#eq-cov-trace-equals-eigen-sum}

That is, the sum of the eigenvalues $\sigma_\eta^2$ is equal to the sum of squared feature-norms, $\lVert x_{\bullet, k} \rVert^2$.

#### Matrix decompositions

In the previous section we reconstructed rows of the feature matrix $X_{\bullet, \bullet}$ using a sequence of orthogonal projections $v_{\bullet, \eta} \; v_{\bullet, \eta}^\top$.  This development gave us a partial eigen-decomposition of the $d \times d$ matrix $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$, the scaled feature covariance matrix.  With $r = rank(X_{\bullet, \bullet})$, and for $\eta \in \{1, \ldots, r\}$ the $d-$dimensional vector $v_{\bullet, \eta}$ is an eigenvector of $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$, having corresponding eigenvector $\sigma_\eta^2$, indexed in descending magnitude.  In matrix notation we have: 

$$
\begin{align} 
  & \left ( 
    X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} 
  \right ) \; 
    (v_{\bullet, 1}, \ldots, v_{\bullet, r}) \\ 
  &= 
  (v_{\bullet, 1}, \ldots, v_{\bullet, r}) \; D^2
\end{align} 
$$ {#eq-cov-partial-eigen-decomp}

Matrix $X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}$ is non-negative definite, which means that it has $d$ non-negative eigenvalues and corresponding eigenvectors.  The first $r$ eigenvalues are positive.  Arranged in descending order, they are the diagonal elements of $D^2 = diag(\sigma_1^2, \ldots, \sigma_r^2)$.  If $r = d$ the matrix is strictly positive-definite, and its complete eigen-decomposition is given by @eq-cov-partial-eigen-decomp.

Suppose now that $r < d$.  Then any unit vector orthogonal to $\mathcal{R}^{(r)}$, the $d-$dimensional subspace spanned by $(v_{\bullet, 1}, \ldots, v_{\bullet, r})$, qualifies as an eigenvector (a "null" eigenvector, we'll say) having an eigenvalue of zero.  Therefore we can complement eigenvectors $(v_{\bullet, 1}, \ldots, v_{\bullet, r})$ with an orthonormal set of null eigenvectors $(v_{\bullet, r + 1}, \ldots, v_{\bullet, d})$.  The combined set of eigenvectors is a complete orthonormal basis for $\mathbb{R}^d$.

We thus have two cases to consider: $r < d$; or $r = d$.  In either case we now have a complete orthonormal basis represented by the $d \times d$ matrix $V = (v_{\bullet, 1}, \ldots, v_{\bullet, d})$.  We delineate the two cases as follows.

$$
\begin{align} 
  V_1 &= (v_{\bullet, 1}, \ldots, v_{\bullet, r}) \\ 
  V_2 &= 
    \begin{cases}
      (v_{\bullet, r + 1}, \ldots, v_{\bullet, d}) & \text{if } r < d \\ 
      \emptyset & \text{if } r = d
    \end{cases} \\ 
  V &= (V_1, V_2)
\end{align} 
$$ {#eq-cov-all-eigen-vectors-1}

We also record the full set of eigenvalues in the $d \times d$ diagonal matrix $\Sigma^2$ as follows.

$$
\begin{align} 
  \Sigma^2 &= 
    \begin{cases}
      \begin{pmatrix}
        D^2 & 0 \\
        0   & 0
    \end{pmatrix} 
    & \text{if } r < d \\ 
      D^2 & \text{if } r = d
    \end{cases}
\end{align} 
$$ {#eq-cov-all-eigen-values-1}

This enables us to re-express @eq-cov-partial-eigen-decomp as follows.

$$
\begin{align} 
  \left ( 
    X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} 
  \right ) \; 
    V 
  &= 
    V \; \Sigma^2
\end{align} 
$$ {#eq-cov-full-eigen-decomp-1}

Matrix $V$ is an orthonormal rotation of $\mathbb{R}^d$, so that $V^\top = V^{-1}$.  Multiplying @eq-cov-full-eigen-decomp-1 on the right by $V^\top$ we obtain: 

$$
\begin{align} 
  X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}
  &= 
    V \; \Sigma^2 \; V^\top 
\end{align} 
$$ {#eq-cov-full-eigen-decomp-2}

This last equation can be re-expressed as an eigen-decomposition of the feature covariance matrix: 

$$
\begin{align} 
  & cov(X_{\bullet, \bullet}) \\ 
  &= 
    \frac{1}{n-1} X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} \\ 
  &= V \; 
    \left ( 
      \frac{1}{n-1} \Sigma^2 \; 
    \right )
    V^\top
\end{align} 
$$ {#eq-cov-full-eigen-decomp-3}

We now define $(c_{\bullet, 1}, \ldots, c_{\bullet, r})$, the _principal components_ of $X_{\bullet, \bullet}$, as the following respective linear combinations of the columns of $X_{\bullet, \bullet}$.

$$
\begin{align} 
  c_{\bullet, \eta} 
  &= 
    X_{\bullet, \bullet} \; v_{\bullet, \eta} 
    & \text{ for } \eta \in \{ 1, \ldots, r \}
\end{align} 
$$ {#eq-pc-defn-1}

Note that distinct principal components are orthogonal, and the squared magnitude of $c_{\bullet, \eta}$ is $\sigma_\eta^2$:

$$
\begin{align} 
  & 
  c_{\bullet, \kappa}^\top \; c_{\bullet, \eta} \\ 
  &= 
    v_{\bullet, \kappa}^\top \; 
    X_{\bullet, \bullet}^\top \; 
    X_{\bullet, \bullet} \; 
    v_{\bullet, \eta} \\ 
  &= 
    v_{\bullet, \kappa}^\top \; 
    \left ( 
      X_{\bullet, \bullet}^\top \; 
      X_{\bullet, \bullet} \; 
      v_{\bullet, \eta}
    \right ) \\ 
  &= 
    v_{\bullet, \kappa}^\top \; 
    \left ( 
      \sigma_\eta^2 \; 
      v_{\bullet, \eta}
    \right ) \\ 
  &= 
    \sigma_\eta^2 \; 
    v_{\bullet, \kappa}^\top \; 
    v_{\bullet, \eta}  \\ 
  &= 
    \begin{cases}
      \sigma_\eta^2 & \text{ if } \kappa = \eta \\ 
      0 & \text{ if } \kappa \ne \eta
    \end{cases}
\end{align} 
$$ {#eq-orthogonal-PCs}

Dividing $c_{\bullet, \eta}$ by its magnitude $\sigma_\eta$, we obtain a unit vector $u_{\bullet, \eta}$ that represents the direction of $c_{\bullet, \eta}$.

$$
\begin{align} 
  u_{\bullet, \eta}
  &= 
    \frac{ c _{\bullet, \eta} }{ \lVert c _{\bullet, \eta} \rVert } \\ 
  &= 
    \frac{ c _{\bullet, \eta} }{ \sigma_\eta } 
    & \text{ for } \eta \in \{ 1, \ldots, r \}
\end{align} 
$$ {#eq-u-equals-pc-normalized}

Since the principal components are pair-wise orthogonal, the unit vectors just defined make up an orthonormal set of $n-$dimensional vectors.  Note that 

$$
\begin{align} 
  u_{\bullet, \eta}^\top \; X_{\bullet, \bullet}
  &= 
    \frac{ 1 }{ \sigma_\eta } \; 
    c_{\bullet, \eta}^\top \; X_{\bullet, \bullet} \\ 
  &= 
    \frac{ 1 }{ \sigma_\eta } \; 
    \left ( 
      X_{\bullet, \bullet} \; v_{\bullet, \eta} 
    \right )^\top   \;
    X_{\bullet, \bullet} \\ 
  &= 
    \frac{ 1 }{ \sigma_\eta } \; 
    \left ( 
      v_{\bullet, \eta}^\top \; X_{\bullet, \bullet}^\top  
    \right )   \;
    X_{\bullet, \bullet} \\ 
  &= 
    \frac{ 1 }{ \sigma_\eta } \; 
      v_{\bullet, \eta}^\top \; 
      X_{\bullet, \bullet}^\top \;
      X_{\bullet, \bullet} \\ 
  &= 
    \frac{ 1 }{ \sigma_\eta } \; 
    \left ( 
      X_{\bullet, \bullet}^\top \;
      X_{\bullet, \bullet} \; 
      v_{\bullet, \eta} 
    \right )^\top \\ 
  &= 
    \frac{ 1 }{ \sigma_\eta } \; 
    \left ( 
      \sigma_\eta^2 \; 
      v_{\bullet, \eta} 
    \right )^\top  \\ 
  &= 
    \sigma_\eta \; 
    v_{\bullet, \eta}^\top 
\end{align} 
$$ {#eq-svd-1}

We now define $U_1 = ( u_{\bullet, 1}, \ldots, u_{\bullet, r} )$ to be the $n \times r$ matrix having these $r$ unit vectors as its columns.  And we extend, if needed, this set of unit vectors to create an orthonormal basis for $\mathbb{R}^n$.  That is, if $r < n$ then the vector space spanned by the columns of $U_1$ is a proper subspace of $\mathbb{R}^n$.  The orthogonal complement to this subspace has dimension $n - r$, and thus admits an orthonormal basis, which we label as $( u_{\bullet, r + 1}, \ldots, u_{\bullet, n} )$.  And we define $U_2$ to be the matrix having these vectors as its columns.  Finally, we define the $n \times n$ orthogonal matrix $U = ( u_{\bullet, 1}, \ldots, u_{\bullet, n} )$.

$$
\begin{align} 
  U_1 &= (u_{\bullet, 1}, \ldots, u_{\bullet, r}) \\ 
  U_2 &= 
    \begin{cases}
      (u_{\bullet, r + 1}, \ldots, u_{\bullet, n}) & \text{if } r < n \\ 
      \emptyset & \text{if } r = n
    \end{cases} \\ 
  U &= (U_1, U_2)
\end{align} 
$$ {#eq-all-left-eigen-vectors-1}

We can now re-express @eq-svd-1 as follows.

$$
\begin{align} 
  U^\top \; X_{\bullet, \bullet} 
  &= 
    \Sigma \; V^\top
\end{align} 
$$ {#eq-svd-2}

Multiplying this last equation on the left by matrix $U$ we obtain: 

$$
\begin{align} 
  X_{\bullet, \bullet} 
  &= 
    U \; \Sigma \; V^\top
\end{align} 
$$ {#eq-svd-3}

This factorization of the feature matrix $X_{\bullet, \bullet}$ is called a _singular value decomposition_ (SVD).

### Definitions [OLD]

We now define PCA mathematically in vector-matrix notation.  For convenience of notation assume that the feature matrix $X_{\bullet, \bullet}$ has been centered, that is, for each column of the original feature matrix, the average value has been calculated and subtracted.   Therefore the average (or equivalently the sum) of each column of $X_{\bullet, \bullet}$ equals zero.

$$
\begin{align} 
  X_{\bullet, \bullet} &= (x_{\bullet, 1}, \ldots, x_{\bullet, d}) \\ \\ 
  & \text{with} \\ \\ 
  1_\bullet^\top \; x_{\bullet, k} &= \sum_{i = 1}^n x_{i, k} = 0 \\ \\ 
  &\text{for } k \in \{ 1, \ldots, d \} \\ \\ 
  & \text{so that} \\ \\ 
  1_\bullet^\top \; X_{\bullet, \bullet} &= 0_\bullet \in \mathbb{R}^d
\end{align} 
$$ {#eq-X-ctr-cols-OLD}

The first step in this reconstruction is to seek a vector in feature space, say $z_\bullet = X_{\bullet, \bullet} \; \gamma_\bullet$ that minimizes the following squared error loss function, say $\mathcal{L} (\gamma_\bullet)$.

$$
\begin{align} 
  \mathcal{L} (\gamma_\bullet) &= 
    \sum_{i = 1}^n 
    \lVert 
      x_{i, \bullet} \; - \; 
    \rVert
\end{align} 
$$ {#eq-pc-1-sq-error-OLD}

The first principal component of $X_{\bullet, \bullet}$ is determined by the "direction" in feature space of maximum variance.

To elaborate, each vector of coefficients $\gamma_\bullet \in \mathbb{R}^d$ can be mapped to a linear combination of feature vectors, $X_{\bullet, \bullet} \gamma_\bullet$.  Such linear combinations constitute a subspace of $\mathbb{R}^n$ called "feature space".  Each non-zero coefficient vector can be expressed as the product of a scalar magnitude and a unit vector, namely: 

$$
\begin{align} 
  \gamma_\bullet &= \lVert \gamma_\bullet \rVert \times 
  \left ( 
    \frac{\gamma_\bullet}{\lVert \gamma_\bullet \rVert} 
  \right )
    & \text{ for } \gamma_\bullet \ne 0_\bullet
\end{align} 
$$ {#eq-gamma-mag-direction}

The above unit vector is called the direction of vector $\gamma_\bullet$.  Now the first principal component of $X_{\bullet, \bullet}$, is defined by the coefficient unit vector, say $u_{\bullet, 1}$, that maximizes the norm of $X_{\bullet, \bullet} u_\bullet$ among all unit vectors $u_\bullet \in \mathbb{R}^d$.

$$
\begin{align} 
  u_{\bullet, 1} &= 
  \arg \max_{\lVert u_\bullet \rVert = 1} 
  \Vert X_{\bullet, \bullet} \; u_\bullet \rVert 
\end{align} 
$$ {#eq-coeff-first-pc}

Then the first principal component, denoted $z_{\bullet, 1}$, is the feature space vector defined by $u_{\bullet, 1}$.

$$
\begin{align} 
  z_{\bullet, 1} &= X_{\bullet, \bullet} \; u_{\bullet, 1}
\end{align} 
$$ {#eq-first-pc}

Note that the elements of the first principal component $z_{\bullet, 1}$ sum to zero, since this is true of each feature vector, and $z_{\bullet, 1}$ is a linear combination of the feature vectors.

$$
\begin{align} 
  1_\bullet^\top z_{\bullet, 1} &= 
  1_\bullet^\top X_{\bullet, \bullet} \; u_{\bullet, 1} = 0 \\ \\ 
  & \text{so that} \\ \\ 
  \bar{z}_1 &= \frac{1}{n} \sum_{i = 1}^n z_{i, 1} \\ 
  &= \frac{1}{n} \; 1_\bullet^\top \;  z_{\bullet, 1} \\ 
  &= 0
\end{align} 
$$ {#eq-first-pc-ctr}

Therefore, the sample variance of $z_{\bullet, 1}$ is: 

$$
\begin{align} 
  var ( z_{\bullet, 1} ) &= 
  \frac{1}{n - 1} \sum_{i = 1}^n (z_{i, 1} - \bar{z}_1)^2 \\ 
  &= 
  \frac{1}{n - 1} \sum_{i = 1}^n z_{i, 1}^2 \\ 
  &= \frac{1}{n - 1} \; \Vert z_{\bullet, 1} \rVert^2
\end{align} 
$$ {#eq-first-pc-var}

That is, by maximizing the norm $\Vert z_{\bullet, 1} \rVert$ the coefficient vector $u_{\bullet, 1}$ maximizes the sample variance of $z_{\bullet, 1}$, which was the intended outcome.

We define the remaining principal components inductively as follows.  Suppose the we have defined principal components $z_{\bullet, 1}, \ldots, z_{\bullet, k}$, where $1 \le k \le d$.  These vectors span a $k-$dimensional subspace of feature space.  If the subspace is in fact _all_ of feature space, we seek no additional principal components.  Otherwise we define the next principal component as the linear combination $X_{\bullet, \bullet} \; u_{\bullet, k + 1}$ of maximal norm among all such linear combinations that are orthogonal to each of the preceding $k$ principal components.

$$
\begin{align} 
  u_{\bullet, k + 1} &= 
  \arg \max_{\lVert u_\bullet \rVert = 1} 
  \left \{ 
  \Vert X_{\bullet, \bullet} \; u_\bullet \rVert : \; 
    X_{\bullet, \bullet} \; u_\bullet \; \perp \; z_{\bullet, \kappa}, \forall \kappa
  \right \}
\end{align} 
$$ {#eq-coeff-next-pc}

Let's take a moment to describe this constraint that the new principal component be orthogonal to its predecessors.  The constraint amounts to setting to zero the following quadratic form defined for pairs of coefficient vectors:

$$
\begin{align} 
  0 &= \left < 
  X_{\bullet, \bullet} \; u_\bullet, \; z_{\bullet, \kappa}
  \right > \\ 
  &= u_\bullet^\top \; X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} \; u_{\bullet, \kappa} \\ 
  &= u_\bullet^\top \; \left ( X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} \right ) \; u_{\bullet, \kappa}
\end{align} 
$$ {#eq-pc-coeff-quad-form}

Because $X_{\bullet, \bullet}$ is centered, the $d \times d$ matrix defining the quadratic form above is a multiple of the sample covariance matrix among features.

$$
\begin{align} 
  cov \left ( X_{\bullet, \bullet} \right ) 
  &= \frac{1}{n - 1}
  \left ( 
  X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}
  \right )
\end{align} 
$$ {#eq-cov-X-mat-OLD}

We use this identity in @sec-dim-r-computation to discuss algorithms for calculating principal components.

This inductive process terminates with a complete set of $r$ principal components, $\{ z_{\bullet, 1}, \ldots, z_{\bullet, r} \}$, where $r$ is the dimension of feature space, or equivalently $r = rank(X_{\bullet, \bullet})$, the rank of feature matrix $X_{\bullet, \bullet}$, with $r \le \min(n, d)$.

The principal components thus span feature space.  In fact they constitute an ordered orthogonal basis of decreasing norms.  Therefore each feature vector can be expressed as a linear combination of the principal components:

$$
\begin{align} 
  x_{\bullet, j} 
  &= 
    \sum_{k = 1}^r 
      \left < x_{\bullet, j}, z_{\bullet, k} \right > \times 
      \frac{ z_{\bullet, k} } { \Vert z_{\bullet, k} \rVert^2 } \\ 
  &= 
    \sum_{k = 1}^r 
      \frac{ z_{\bullet, k} } { \Vert z_{\bullet, k} \rVert^2 } \times
      \left < z_{\bullet, k}, x_{\bullet, j} \right >  \\ 
  &= 
  (z_{\bullet, 1}, \ldots, z_{\bullet, r}) \; 
  D^{-2} \; 
  (z_{\bullet, 1}, \ldots, z_{\bullet, r})^\top \; 
  x_{\bullet, j} \\ \\ 
  & \text{where} \\ \\ 
  D &= diag 
  \left ( 
    \lVert z_{\bullet, 1} \rVert, \ldots, \lVert z_{\bullet, r} \rVert 
  \right )
\end{align} 
$$ {#eq-col-X-per-pc-basis}

## Supervised Dimension Reduction {#sec-dim-r-supervised}

## Computational Considerations {#sec-dim-r-computation}

## Choosing and Evaluating Methods {#sec-dim-r-choose-method}

## Other Methods {#sec-dim-r-other-methods}

## Summary {#sec-dim-r-summary}

## Exercises {#sec-dim-r-exercises}


