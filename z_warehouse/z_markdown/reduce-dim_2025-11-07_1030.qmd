# Dimension Reduction {#sec-reduce-dim}

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(
  echo    = FALSE, 
  error   = FALSE, 
  message = FALSE, 
  warning = FALSE
)
```

```{r}
#| label: CRAN-libraries

library(assertthat)
library(beans)
library(Cairo)
# library(cairoDevice)
library(corrplot)
# library(dslabs)
library(GGally)
# library(ggimg)
library(HistData)
library(here)
library(ISLR2)
# library(keras)
library(knitr)
# library(latex2exp)
# library(OECD)
# library(patchwork)
library(plotly)
# library(regclass)
# library(rgl)
# library(R.utils)
# library(reticulate)
# library(scatterplot3d)
# library(tensorflow)
library(tidymodels)
library(tidyverse)
# library(tinytex)
# library(UsingR)
```

```{r}
#| label: local-source

source(here("code", "d_ball.R"))
source(here("code", "galton_ht_data.R"))
# source(here("code", "mnist_file_mgmt.R"))
# source(here("code", "NCI60_helper.R"))
# source(here("code", "oecd_bli.R"))
# source(here("code", "pgram_grid.R"))
source(here("code", "vc_per_state.R"))
source(here("code", "wine_quality_uci.R"))
# source(here("code", "z_score.R"))
```

This chapter addresses the problem of finding and visualizing structure in high-dimensional data.  We begin the discussion with example data sets.

## Data Examples {#sec-dim-r-data-examples}

### US Arrests

```{r}
#| label: stats-per-us-state-1970s

state_stats_lst <- list_state_stats()

# (x, y) = (lat, long)
st_ctr_tbl <- 
  state_stats_lst$ st_ctr_tbl

# 8 state stats: Population, ..., Area
st_77_tbl <- 
  state_stats_lst$ st_77_tbl

# UrbanPop + 3 violent crimes: Assault, Murder, Rape
arrests_wide <- 
  state_stats_lst$ arrests_wide
arrests_long <- 
  state_stats_lst$ arrests_long

# compilation of the above
st_wide <- 
  state_stats_lst$ st_wide
st_wide_dscr <- 
  state_stats_lst$ st_wide_dscr
```

Data analysis often begins with an exploration, using tables and figures to learn more about the data before formulating specific questions or hypotheses.  The exploration becomes more difficult as the dimensions of the data set increase.  Here's an example.

@McNeil_1977 reviewed the relationship among violent crime statistics per US state and the percent of the population living in urban areas, variables described in @tbl-arrests-per-us-state-1973 below.  With just four variables, we can visualize the full data structure (e.g., using a scatter plot matrix as shown in @fig-arrests-2d-scatter-matrix).  However, the initial exploratory data analysis reveals strong correlations among the three crime variables (Murder, Assault, Rape), suggesting that the data may effectively lie in fewer than four dimensions.  Dimension reduction techniques can help us identify these underlying patterns, enabling us to (1) visualize state-level crime patterns in 2D or 3D, (2) identify states having similar crime profiles, and (3) understand whether crime variation is driven by one dominant factor or multiple independent factors.

@tbl-stats-per-us-state-1970s below describes related state-level statistics from the same period (the 1970s).  The data from both tables are available from the core `R` package `datasets`.

```{r}
#| label: tbl-arrests-per-us-state-1973

st_wide_dscr |> 
  dplyr::filter(
    var %in% c("Assault", "Rape", "Murder", "UrbanPop")
  ) |> 
  dplyr::rename(variable = var, description = dscr) |> 
  knitr::kable(
    caption = "Arrests per US State (1973)")

```

```{r}
#| label: tbl-stats-per-us-state-1970s

st_wide_dscr |> 
  dplyr::filter(
    ! (var %in% c("Assault", "Rape", "Murder", "UrbanPop"))
  ) |> 
  dplyr::rename(variable = var, description = dscr) |> 
  knitr::kable(
    caption = "Statistics per US State (1970s)")
```

@fig-arrests-violin below shows the distribution across the 50 states of each type of violent crime.  Rates are calcuated as arrests per 100,000 population.  The figure shows these rates on a $\log_{10}$ scale, since assault arrests are many times more common than rape or murder arrests.

```{r}
#| label: fig-arrests-violin
#| fig-cap: "Violent Crime Rates per US State (1973)"

g_arrests_violin <- arrests_long |> 
  dplyr::filter(var != "UrbanPop") |> 
  dplyr::mutate(var = forcats::as_factor(var)) |> 
  ggplot2::ggplot(mapping = aes(
    x = var, y = rate, color = var, fill = var
  )) + 
  geom_violin(show.legend = FALSE) + 
  scale_y_log10() + 
  labs(
    title = "Violent Crime Rates per US State (1973)", 
    subtitle = "arrests per 100K population"
  )

g_arrests_violin

```

The figure above gives a view of three data variables, that is, three columns of the data matrix.  @fig-arrests-3d-scatter below, a 3D scatterplot, gives a complementary view of each of the 50 states as a point whose coordinates are the respective arrest rates for assault, rape, and murder, with each point colored by the value of `UrbanPop` the percentage of the state's population living in an urban area.  Since individual states correspond to the rows of the data matrix, this figure is a row-based perspective on the data matrix.

```{r}
#| label: fig-arrests-3d-scatter
#| fig-cap: "Violent Crime Rates in each US State (1973)"

g_arrests_3d_scatter <- arrests_wide |> 
  plot_ly(
    x = ~Assault, 
    y = ~Rape, 
    z = ~Murder,
    type = "scatter3d",
    mode = "markers",
    text = ~arrests_wide$ st_abb,  # State abbreviation for hover
    marker = list(
      size = 5,
      color = ~UrbanPop,  # Optional: color by UrbanPop
      colorscale = "Viridis",
      showscale = TRUE,
      colorbar = list(title = "UrbanPop"))) |> 
  layout(scene = list(
    xaxis = list(title = "Assault"),
    yaxis = list(title = "Rape"),
    zaxis = list(title = "Murder")
  ),
  title = "Violent Crime Rates in each US State (1973)")

g_arrests_3d_scatter

```

@fig-arrests-2d-scatter-matrix below exemplifies a matrix of 2D scatter plots, a display method that accommodates more than 3 variables (but not many more, practically speaking). 

```{r}
#| label: fig-arrests-2d-scatter-matrix
#| fig-cap: "Arrests Variables: Relationships and Correlations"

g_arrests_2d_scatter_matrix <- arrests_wide |> 
  GGally::ggpairs(
    columns = c("Assault", "Rape", "Murder", "UrbanPop"),
    upper = list(continuous = wrap("cor", size = 3)),
    lower = list(continuous = wrap("points", alpha = 0.5, size = 0.8)),
    title = "Arrests Variables: Relationships and Correlations")

g_arrests_2d_scatter_matrix

```

This figure provides both column-based and row-based views of the data matrix, with the scatter diagrams for each pair of variables providing the row-based perspective.

### Dry Beans

@Koklu_Ozkan_2020_multiclass published a dataset of visual characteristics of dried beans "... in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera."  The resulting dataset contains 16 morphological features extracted from each bean image, including measures of area, perimeter, compactness, length, width, and various shape factors.

The classification goal is to predict bean variety from these morphological measurements.  Successful classification could have many applications, e.g., to improve sorting systems in agricultural processing.  However, many of these 16 features are inherently redundant: for example, area and perimeter are strongly correlated, as are various length and width measurements.  Dimension reduction can reveal whether bean varieties differ primarily in size, shape, or both, and whether a smaller subset of derived features might achieve comparable classification accuracy with greater interpretability and computational efficiency.

The data are available `R` package `beans`, containing a data matrix (tibble, more precisely) of dimension $13611 \times 17$.  The last column of the data matrix is response variable `class` that assigns one of seven types of bean to each bean-image.  The remaining 16 columns of the data matrix are morphological measurements (of shape and size).

@Kuhn_Silge_tmwr develop and evaluate different classification models for these data. [^beans-ttv-split]  They begin by examining the correlation coefficients for each pair of the 16 feature vectors.  @fig-beans-corrplot follows their example.

[^beans-ttv-split]: To develop and evaluate the classification models, the authors split the observations into three non-overlapping sets: training ($n = 10206$, nearly 75%), testing $(n = 1703)$, and validation $(n = 1702)$.

```{r}
#| label: beans-tbl

beans_tbl <- beans::beans

```

```{r}
#| label: fig-beans-corrplot
#| fig-cap: "beans: correlation among features"

# from tmwr:
# tmwr_cols <- grDevices::colorRampPalette(c("#91CBD765", "#CA225E"))
# col = tmwr_cols(200)

b_corr_colors <- grDevices::colorRampPalette(c("blue", "red"))

beans_corr_lst <- beans_tbl |> 
  dplyr::select(-class) |> 
  cor() |> 
  corrplot::corrplot(
    method        = "ellipse", 
    type          = "lower", 
    diag          = FALSE, 
    # order         = "hclust", 
    # hclust.method = "complete", 
    col           = b_corr_colors(200), 
    tl.col        = "black"
  )

# from vignette("corrplot-intro"): 
# (method = 'square', order = 'FPC', type = 'lower', diag = FALSE)

```

In this figure, the absolute value of each correlation coefficient is represented by the narrowness of the drawn ellipse and the depth of its color.  The sign of the correlation coefficient is represented by both the color and direction of the ellipse.

These size and shape features measure similar concepts. Consequently, several pairs of feature vectors are highly correlated, [^beans-correlation-xmpl] which offers the possibility of transforming the 16 original features into a smaller set without sacrificing classification power.

[^beans-correlation-xmpl]: For example, the features `area` and `convex_area` can differ in principle, but for the beans data the correlation is `r beans_corr_lst$corr[["convex_area", "area"]] |> round(digits = 5)`.

### Wine Quality

```{r}
#| label: get-wine-quality

wine_quality <- get_wine_quality()

# lubridate::now()
# [1] "2025-11-04 12:46:12 GMT"

# wine_quality |> str()
# spc_tbl_ [6,497 Ã— 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
#  $ fixed acidity       : num [1:6497] 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...
#  $ volatile acidity    : num [1:6497] 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...
#  $ citric acid         : num [1:6497] 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...
#  $ residual sugar      : num [1:6497] 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...
#  $ chlorides           : num [1:6497] 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...
#  $ free sulfur dioxide : num [1:6497] 11 25 15 17 11 13 15 15 9 17 ...
#  $ total sulfur dioxide: num [1:6497] 34 67 54 60 34 40 59 21 18 102 ...
#  $ density             : num [1:6497] 0.998 0.997 0.997 0.998 0.998 ...
#  $ pH                  : num [1:6497] 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
#  $ sulphates           : num [1:6497] 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
#  $ alcohol             : num [1:6497] 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
#  $ quality             : num [1:6497] 5 5 5 6 5 5 5 7 7 5 ...
#  $ color               : chr [1:6497] "red" "red" "red" "red" ...
#  - attr(*, "spec")=
#   .. cols(
#   ..   `fixed acidity` = col_double(),
#   ..   `volatile acidity` = col_double(),
#   ..   `citric acid` = col_double(),
#   ..   `residual sugar` = col_double(),
#   ..   chlorides = col_double(),
#   ..   `free sulfur dioxide` = col_double(),
#   ..   `total sulfur dioxide` = col_double(),
#   ..   density = col_double(),
#   ..   pH = col_double(),
#   ..   sulphates = col_double(),
#   ..   alcohol = col_double(),
#   ..   quality = col_double()
#   .. )
#  - attr(*, "problems")=<externalptr>
```

```{r}
#| label: wq-dimensions

# capture dimensions to embed in narrative
wq_nrow <- nrow(wine_quality)
wq_ncol <- ncol(wine_quality)
wq_n_red <- wine_quality |> 
  dplyr::filter(color == "red") |> nrow()
wq_n_white <- wine_quality |> 
  dplyr::filter(color == "white") |> nrow()
```

@Cortez_2009_ModelingWP model wine preference as a function of `r wq_ncol - 1L` physicochemical properties of wine, which are listed in @tbl-wq-var-dscr. The `quality` score is the median of three expert tastings. The data consist of `r wq_nrow` wines, `r wq_n_red` red and `r wq_n_white` white. [^wq-uci-repo] [^red-v-white-wine]

[^wq-uci-repo]: The authors contributed the data (now archived) to the UC Irvine Machine Learning Repository.  See @wine_quality_186.

[^red-v-white-wine]: The wines in this study are all from the Minho (northwest) region of Portugal.  Medium in alcohol, it is appreciated for its freshness (especially in summer). At the time of writing the authors report that Minho wine accounts for 15% of total Portuguese wine production, of which some 10% is exported, mostly white wine.

The modeling goal is to understand the chemical properties influencing wine quality ratings and to predict quality from objective laboratory measurements.  Apart from wine color (red or white), the remaining `r wq_ncol - 2L` physicochemical features include related measurements, e.g., pH, fixed acidity, and citric acid are chemically interrelated, as are free and total sulfur dioxide.  Dimension reduction can help us: (1) visualize the chemical space that wines occupy in 2D or 3D, (2) identify whether wine quality varies along a small number of chemical gradients (e.g., acidity vs. alcohol content), (3) understand the chemical differences between red and white wines, and (4) determine whether a smaller set of derived features might predict quality as well as the full set of measurements.

```{r}
#| label: tbl-wq-var-dscr

wq_var_dscr <- describe_wq_vars()

wq_var_dscr |> 
  dplyr::rename(variable = var) |> 
  knitr::kable(
    caption = "Wine: physicochemical properties")
```

Correlations among the `r wq_ncol - 2L` numeric features are shown in @fig-rw-wq-corrplot below for red and white wines, respectively.

```{r}
#| label: fig-rw-wq-corrplot
#| fig-cap: "Wine: correlation among numeric features"
#| fig-subcap: 
#|   - "Red wines"
#|   - "White wines"
#| layout-ncol: 2

wq_corr_colors <- grDevices::colorRampPalette(c("blue", "red"))

red_wq_corr_lst <- wine_quality |> 
  dplyr::filter(color == "red") |> 
  dplyr::select(-c(quality, color)) |> 
  cor() |> 
  corrplot::corrplot(
    method        = "ellipse", 
    type          = "lower", 
    diag          = FALSE, 
    col           = wq_corr_colors(200), 
    tl.col        = "black"
  )

white_wq_corr_lst <- wine_quality |> 
  dplyr::filter(color == "white") |> 
  dplyr::select(-c(quality, color)) |> 
  cor() |> 
  corrplot::corrplot(
    method        = "ellipse", 
    type          = "lower", 
    diag          = FALSE, 
    col           = wq_corr_colors(200), 
    tl.col        = "black"
  )

```

These figures reveal some substantial correlations, with distinct patterns per wine color.

@tbl-rw-quality-corr below shows feature-quality correlations for red and white wines, respectively.

```{r}
#| label: rw-quality-corr-tbl

rw_quality_corr_tbl <- wine_quality |> 
  get_rw_quality_corr_tbl()

```

```{r}
#| label: tbl-rw-quality-corr
#| tbl-cap: "Feature-quality correleations among (red, white) wines"

rw_quality_corr_tbl |> 
  knitr::kable(
    caption = "Feature-quality correleations", 
    digits = 2
  )

```

We see that alcohol volume is a prominent indicator of quality for both red and white wines.  On the other hand, citric acid and sulphates are strongly correlated with the quality of red wine, but not white.

As demonstrated by @Cortez_2009_ModelingWP, these data patterns offer the possibility of developing a smaller set of features as predictors of wine quality.

### Cancer Genomics (NCI60)

```{r}
#| label: NCI60-structure

# data dimensions
nrow_NCI60       <- ISLR2::NCI60$ data |> nrow()
ncol_NCI60       <- ISLR2::NCI60$ data |> ncol()

# row labels
lbl_NCI60        <- ISLR2::NCI60$ labs
lbl_unique_NCI60 <- lbl_NCI60 |> 
  unique() |> sort()
n_u_labels_NCI60 <- length(lbl_unique_NCI60)
lbl_ct_NCI610    <- lbl_NCI60 |> 
  tibble::as_tibble_col(column_name = "label") |> 
  dplyr::summarise(.by = "label", ct = n()) |> 
  dplyr::arrange(label)
lbl_ct_vec_NCI610 <- lbl_ct_NCI610 |> 
  dplyr::pull(name = label)
```

The NCI60 data [^ISLR2-NCI60] [^Ross-2000-citation] consists of gene expression measurements from `r nrow_NCI60` cancer cell lines. For each cell line, expression levels were measured for `r ncol_NCI60` genes using microarray technology. The cell lines represent `r n_u_labels_NCI60` different cancer types, including leukemia, melanoma, and cancers of the colon, breast, ovary, lung, and central nervous system. This data set is a canonical example of a high-dimensional data matrix where the number of features $(d)$ greatly exceeds the number $n$ of data cases: $d \gg n$.

[^ISLR2-NCI60]: The data are available from `R` package `ISLR2`.  The vector of labels (cancer type per observation) can be accessed as `ISLR2::NCI60$labs`.  The data can be accessed as `ISLR2::NCI60$data` in the form of a $64 \times 6830$ matrix with column names "1", "2", ..., "6830" rather than gene identifiers.  While this simplification is adequate for illustrating dimension reduction methods, readers requiring gene-level biological interpretation should consult the `rcellminer` and `rcellminerData` Bioconductor packages (Luna et al., 2016; Reinhold et al., 2019), which provide the same data with complete gene annotations and represent the current standard for programmatic access to NCI-60 molecular profiling data.

[^Ross-2000-citation]: The `ISLR2` package cites @Ross_2000_Systematic as the source of `ISLR2::NCI60`.

The data set is part of the NCI-60 panel and associated datasets, which are maintained by the Frederick National Laboratory for Cancer Research (FNLCR) and the National Cancer Institute's (NCI) Developmental Therapeutics Program (DTP).  The data serve as a publicly available platform for the global cancer research community to study tumor biology, evaluate new bioinformatics approaches, and select appropriate cell models for specific research questions.

These data present challenges that require dimension reduction, that is, the transformation of the set of `r ncol_NCI60` genomic features into a smaller set that capture the dominant patterns of variation.  We proceed to describe such challenges.

## Dimensionality: Curse and Blessing {#sec-dim-r-curse}

As just noted, the NCI60 data set, with $d =$ `r ncol_NCI60` and $n =$ `r nrow_NCI60`, presents challenges that make dimension reduction essential. Several issues arise with any "wide" data set in which $d \gg n$, and include the following.

  - _Visualization_: We cannot plot points in  $d$ dimensions to explore patterns or detect outliers.
  
  - _Over-fitting_: With $d \gg n$, infinitely many coefficient vectors $\beta_\bullet$ produce identical predictions that replicate the response variable within the training data.  This makes model selection impossible without regularization.
  
  - _Computation_: Storing and manipulating a $d \times d$ covariance matrix becomes prohibitively expensive.

In 1957 Richard Bellman characterized such challenges as "the curse of dimensionality". [^dim-curse]

[^dim-curse]: See @Bellman_1957_dynamic, @Donoho_2000_High_Dimensional_Data_Analysis, and @wiki_curse_dimensionality.

@Donoho_2000_High_Dimensional_Data_Analysis points out that: (1) many established statistical methods assume $d < n$; and (2) we can expect $d \gg n$ to occur more and more often as data-collection is increasingly automated to retrieve all potentially useful details for subsequent screening.

In addition, @wiki_curse_dimensionality notes that our geometric intuition is grounded in $d \le 3$ and is overturned as $d$ increases.  To illustrate, consider a unit hypercube $[0,1]^d$ containing the largest possible inscribed ball. The ratio $r(d)$ of their volumes shrinks dramatically: $r(3) \approx 0.52$, $r(10) \approx 0.0025$, and $r(100) \approx 2 \times 10^{-70}$. In high dimensions, the preponderance of randomly generated points within the cube land far from the center!

Yet @Donoho_2000_High_Dimensional_Data_Analysis also sees opportunities.   While randomly generated data in high dimensions behaves pathologically, actual data tend to be more coherent.  Consider the example data sets:

- _US Arrests_: The three crime variables (assault, rape, murder) are 
  highly correlated; they don't independently span 3D space (@fig-arrests-3d-scatter).
- _Dry_Beans_: The 16 shape measurements aren't independent; they reflect underlying bean geometry.  
- _Wine quality_: The 11 chemical properties are constrained by 
  fermentation chemistry.
- _NCI60_: The 6,830 genes participate in shared biological pathways.

In each case, the data likely occupies a much lower-dimensional structure within the high-dimensional feature space. If we can identify this structure, dimension reduction may actually _improve_ modeling by revealing the true degrees of freedom in the data.

The remainder of this chapter develops methods that exploit such opportunities. We begin with Principal Component Analysis (@sec-dim-r-pca), which finds low-dimensional approximations to high-dimensional data through eigen-decomposition. We then explore how supervision (@sec-dim-r-supervised) can guide dimension reduction when prediction is the goal. Finally, we examine computational strategies (@sec-dim-r-computation) for extreme cases like NCI60 where $d \gg n$.

## Principal Component Analysis {#sec-dim-r-pca}

@fig-fs-pca below illustrates PCA for (father, son) centered heights from the Galton data.

```{r}
#| label: fms-galton

galton_3d_lst <- get_galton_3d()

# tibble(father, mother, child, gender)
# (child, gender) refers to oldest child
galton_3d <- galton_3d_lst$ galton_3d

fms_tbl <- galton_3d |> 
  dplyr::filter(gender == "male") |> 
  dplyr::select(- gender) |> 
  dplyr::rename(son = child)

# center data variables using base::scale()
fms_ctr <- fms_tbl |> 
  dplyr::mutate(dplyr::across(
    .cols = c(mother, father, son), 
    .fns  = scale, scale = FALSE
  )) |> 
  # change scale() output from matrix to vector
  dplyr::mutate(dplyr::across(
    .cols = c(mother, father, son), 
    .fns  = as.vector
  ))

```

```{r}
#| label: fs-pca

# PCA(father, son)
fs_pca_lst <- fms_ctr |> 
  dplyr::select(father, son) |> 
  stats::prcomp(center = FALSE)

# rotation matrix
fs_rot_mat <- fs_pca_lst$ rotation
# fs_rot_mat
#               PC1        PC2
# father -0.7041940  0.7100076
# son    -0.7100076 -0.7041940

# slope coefficients for geom_abline()
fs_slope_1 <- 
  fs_rot_mat [["son",    "PC1"]] / 
  fs_rot_mat [["father", "PC1"]]
fs_slope_2 <- 
  fs_rot_mat [["son",    "PC2"]] / 
  fs_rot_mat [["father", "PC2"]]

```

```{r}
#| label: fig-fs-pca
#| fig-cap: "Principal Components: (father, son) centered heights"

g_fs_pcs <- fms_ctr |> 
  dplyr::select(father, son) |> 
  ggplot2::ggplot(mapping = aes(
    x = father, y = son
  )) + 
  ggplot2::geom_point() + 
  ggplot2::coord_fixed(ratio = 1) + 
  ggplot2::geom_abline(
    intercept = 0, 
    slope = fs_slope_1, 
    color = "red") + 
  ggplot2::geom_abline(
    intercept = 0, 
    slope = fs_slope_2, 
    color = "blue") + 
  ggplot2::labs(
    title = "PCA: (father, son) centered heights", 
    subtitle = "PC_1 in red, PC_2 in blue"
  )

g_fs_pcs

```

There are two principal components represented in the figure by the red and blue lines, $(PC_1, PC_2)$.  These lines are perpendicular to one another.  Each component is a linear combination of the (father, son) centered heights.

The orthogonal projection of each (father, son) data point to the $PC_1$ line gives the $PC_1$ coordinate of the data point.  The vector of these coordinates constitutes the fist principal component, say $c_{\bullet, 1}$.  The line represents perfect, positive correlation, and corresponds to the positive correlation in the data.

The orthogonal projection to the $PC_2$ line gives the $PC_2$ coordinate of the data point, which represents the deviation of the data point from perfect, positive correlation.  The vector of these coordinates constitutes the second principal component, $c_{\bullet, 2}$.  That is, the $PC_2$ line represents perfect, negative correlation.

In vector-matrix notation we have, for some set of coefficients $\gamma_{\bullet, \bullet}$, 

$$
\begin{align} 
  c_{\bullet, 1} &= \gamma_{1, 1} \; \dot{f}_\bullet \; + \; \gamma_{2, 1} \; \dot{s}_\bullet \\ 
  c_{\bullet, 2} &= \gamma_{1, 2} \; \dot{f}_\bullet \; + \; \gamma_{2, 2} \; \dot{s}_\bullet \\ \\ 
  &\text{so that} \\ \\ 
  \begin{pmatrix}
    c_{\bullet, 1} & c_{\bullet, 2}
  \end{pmatrix}
   &= 
  \begin{pmatrix}
    \dot{f}_\bullet & \dot{s}_\bullet
  \end{pmatrix} \; 
  \begin{pmatrix}
    \gamma_{1, 1} & \gamma_{1, 2} \\
    \gamma_{2, 1} & \gamma_{2, 2} 
  \end{pmatrix} \\ 
  &= 
  \begin{pmatrix}
    \dot{f}_\bullet & \dot{s}_\bullet
  \end{pmatrix} \; 
  \begin{pmatrix}
    \gamma_{\bullet, 1} & \gamma_{\bullet, 2}
  \end{pmatrix}
\end{align} 
$$ {#eq-fs-pca-1}

It turns out that the coefficient vectors, $(\gamma_{\bullet, 1}, \gamma_{\bullet, 2})$ are eigen-vectors of the sample covariance matrix.

$$
\begin{align} 
  &
  \left \{ 
  \frac{1}{n - 1} \; 
  \begin{pmatrix}
    \dot{f}_\bullet^\top \\
    \dot{s}_\bullet^\top 
  \end{pmatrix} \; 
  \begin{pmatrix}
    \dot{f}_\bullet & \dot{s}_\bullet
  \end{pmatrix}
  \right \} \; 
  \begin{pmatrix}
    \gamma_{\bullet, 1} & \gamma_{\bullet, 2}
  \end{pmatrix} \\ \\ 
  &= 
  \begin{pmatrix}
    \gamma_{\bullet, 1} & \gamma_{\bullet, 2}
  \end{pmatrix} \; 
  \begin{pmatrix}
    \sigma_1^2 & 0 \\
    0 & \sigma_2^2 
  \end{pmatrix} 
\end{align} 
$$ {#eq-fs-pca-2}

Each of the coefficient vectors, $(\gamma_{\bullet, 1}, \gamma_{\bullet, 2})$ is a unit vector, and the two vectors are orthogonal to one another.  That is, the matrix $(\gamma_{\bullet, 1}, \gamma_{\bullet, 2})$ is an orthonormal matrix.  Multiplying @eq-fs-pca-2 on the right by the transpose of $(\gamma_{\bullet, 1}, \gamma_{\bullet, 2})$, we obtain an eigen-decomposition (or polar decompositon) of the sample covariance matrix.

$$
\begin{align} 
  &
  \frac{1}{n - 1} \; 
  \begin{pmatrix}
    \dot{f}_\bullet^\top \\
    \dot{s}_\bullet^\top 
  \end{pmatrix} \; 
  \begin{pmatrix}
    \dot{f}_\bullet & \dot{s}_\bullet
  \end{pmatrix} \\ \\ 
  &= 
  \begin{pmatrix}
    \gamma_{\bullet, 1} & \gamma_{\bullet, 2}
  \end{pmatrix} \; 
  \begin{pmatrix}
    \sigma_1^2 & 0 \\
    0 & \sigma_2^2 
  \end{pmatrix} \; 
  \begin{pmatrix}
    \gamma_{\bullet, 1}^\top \\ 
    \gamma_{\bullet, 2}^\top
  \end{pmatrix}
\end{align} 
$$ {#eq-fs-pca-3}

The matrix of coefficients $(\gamma_{\bullet, 1}, \gamma_{\bullet, 2})$ takes the form of a 2D rotation matrix.

$$
\begin{align} 
  \begin{pmatrix}
    \gamma_{1, 1} & \gamma_{1, 2} \\
    \gamma_{2, 1} & \gamma_{2, 2} 
  \end{pmatrix} 
  &= 
  \begin{pmatrix}
    \cos{\theta} & - \sin{\theta} \\
    \sin{\theta} & \cos{\theta} 
  \end{pmatrix} 
\end{align} 
$$ {#eq-fs-rot-mat}

## Supervised Dimension Reduction {#sec-dim-r-supervised}

## Computational Considerations {#sec-dim-r-computation}

## Choosing and Evaluating Methods {#sec-dim-r-choose-method}

## Other Methods {#sec-dim-r-other-methods}

## Summary {#sec-dim-r-summary}

## Exercises {#sec-dim-r-exercises}


