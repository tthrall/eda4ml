<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Send comments to: Tony T (tthrall)">

<title>6&nbsp; Linear Regression – EDA for Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./reduce-dim.html" rel="next">
<link href="./study-design.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-364982630eef5352dd1537128a8ed5cb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./la-intro.html">Linear Algebra</a></li><li class="breadcrumb-item"><a href="./la-intro.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">EDA for Machine Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tthrall/eda4ml/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conditioning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Clustering: EDA in Higher Dimensions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Statistical Simulation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./study-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sampling and Study Design</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Linear Algebra</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./la-intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reduce-dim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Dimension Reduction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Text Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Text Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dirichlet-dstn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Dirichlet Distribution</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./latent-dirichlet-alloc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Latent Dirichlet Allocation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Time Series Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ts-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Time Series Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ts-forecast.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Time Series Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ts-fourier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Time Series Spectrum Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Graph Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./graph-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Graph Theory for Machine Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./em-algorithm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">EM: the Expectation-Maximization Algorithm</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#data-examples" id="toc-data-examples" class="nav-link active" data-scroll-target="#data-examples"><span class="header-section-number">6.1</span> Data Examples</a>
  <ul class="collapse">
  <li><a href="#heights-of-parents-and-oldest-child" id="toc-heights-of-parents-and-oldest-child" class="nav-link" data-scroll-target="#heights-of-parents-and-oldest-child"><span class="header-section-number">6.1.1</span> Heights of Parents and Oldest Child</a></li>
  <li><a href="#survey-data-better-life-index" id="toc-survey-data-better-life-index" class="nav-link" data-scroll-target="#survey-data-better-life-index"><span class="header-section-number">6.1.2</span> Survey Data: Better Life Index</a></li>
  <li><a href="#mnist-images-of-handwritten-digits" id="toc-mnist-images-of-handwritten-digits" class="nav-link" data-scroll-target="#mnist-images-of-handwritten-digits"><span class="header-section-number">6.1.3</span> MNIST: Images of Handwritten Digits</a></li>
  </ul></li>
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation"><span class="header-section-number">6.2</span> Notation</a></li>
  <li><a href="#geometry" id="toc-geometry" class="nav-link" data-scroll-target="#geometry"><span class="header-section-number">6.3</span> Geometry</a>
  <ul class="collapse">
  <li><a href="#distance-measures" id="toc-distance-measures" class="nav-link" data-scroll-target="#distance-measures"><span class="header-section-number">6.3.1</span> Distance Measures</a></li>
  <li><a href="#family-heights" id="toc-family-heights" class="nav-link" data-scroll-target="#family-heights"><span class="header-section-number">6.3.2</span> Family heights</a></li>
  <li><a href="#centering-data-vectors" id="toc-centering-data-vectors" class="nav-link" data-scroll-target="#centering-data-vectors"><span class="header-section-number">6.3.3</span> Centering Data Vectors</a></li>
  <li><a href="#least-squares-solutions" id="toc-least-squares-solutions" class="nav-link" data-scroll-target="#least-squares-solutions"><span class="header-section-number">6.3.4</span> Least Squares Solutions</a></li>
  <li><a href="#orthogonal-projections" id="toc-orthogonal-projections" class="nav-link" data-scroll-target="#orthogonal-projections"><span class="header-section-number">6.3.5</span> Orthogonal Projections</a></li>
  </ul></li>
  <li><a href="#column-versus-row-visualization" id="toc-column-versus-row-visualization" class="nav-link" data-scroll-target="#column-versus-row-visualization"><span class="header-section-number">6.4</span> Column versus Row Visualization</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">6.5</span> Summary</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">6.6</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#concepts" id="toc-concepts" class="nav-link" data-scroll-target="#concepts"><span class="header-section-number">6.6.1</span> Concepts</a></li>
  <li><a href="#calculations" id="toc-calculations" class="nav-link" data-scroll-target="#calculations"><span class="header-section-number">6.6.2</span> Calculations</a></li>
  <li><a href="#programming" id="toc-programming" class="nav-link" data-scroll-target="#programming"><span class="header-section-number">6.6.3</span> Programming</a></li>
  <li><a href="#advanced" id="toc-advanced" class="nav-link" data-scroll-target="#advanced"><span class="header-section-number">6.6.4</span> Advanced</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/tthrall/eda4ml/edit/main/la-intro.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/tthrall/eda4ml/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./la-intro.html">Linear Algebra</a></li><li class="breadcrumb-item"><a href="./la-intro.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Regression</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-la-intro" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Send comments to: Tony T (tthrall) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">11:36 Sun 2-Nov-2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<hr>
<p>Vectors and matrices are the central objects of linear algebra. And central to data science and machine learning is the notion of a <em>data matrix</em>, in which each row is composed of different types of values that represent a single case of data. For example, a single row of a data matrix might represent the recorded characteristics of an individual participant, item, or unit in some study. In contrast, each column (known as a <em>feature vector</em> or <em>data variable</em>) represents multiple instances of just one of these prescribed types of value. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>This chapter presents linear regression from the perspective of linear algebra. A response variable <span class="math inline">\(y\)</span> is modeled as some linear combination of feature vectors plus a residual error term. We begin with some examples.</p>
<section id="data-examples" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="data-examples"><span class="header-section-number">6.1</span> Data Examples</h2>
<section id="heights-of-parents-and-oldest-child" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="heights-of-parents-and-oldest-child"><span class="header-section-number">6.1.1</span> Heights of Parents and Oldest Child</h3>
<p>In 1885 Sir Francis Galton examined the heights (in inches) of parents and their adult children to determine the strength of evidence to support height as a hereditary trait. The corresponding <code>R</code> data set <code>HistData::GaltonFamilies</code> consists of 934 adult children from a total of 205 families. Restricting attention to the oldest child in each family, there were 26 daughters and 179 sons.</p>
<p>The table below shows a portion of this data matrix. Each row represents a family and consists of: a family identifier, the father’s height, the mother’s height, the oldest child’s height, and the oldest child’s gender.</p>
<div class="cell">
<div id="tbl-galton-3d" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-galton-3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Family heights in inches: father, mother, oldest child
</figcaption>
<div aria-describedby="tbl-galton-3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<caption>Family heights: father, mother, oldest child</caption>
<thead>
<tr class="header">
<th style="text-align: left;">family</th>
<th style="text-align: right;">father</th>
<th style="text-align: right;">mother</th>
<th style="text-align: right;">child</th>
<th style="text-align: left;">gender</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">001</td>
<td style="text-align: right;">78.5</td>
<td style="text-align: right;">67.0</td>
<td style="text-align: right;">73.2</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="even">
<td style="text-align: left;">002</td>
<td style="text-align: right;">75.5</td>
<td style="text-align: right;">66.5</td>
<td style="text-align: right;">73.5</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="odd">
<td style="text-align: left;">003</td>
<td style="text-align: right;">75.0</td>
<td style="text-align: right;">64.0</td>
<td style="text-align: right;">71.0</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="even">
<td style="text-align: left;">004</td>
<td style="text-align: right;">75.0</td>
<td style="text-align: right;">64.0</td>
<td style="text-align: right;">70.5</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="odd">
<td style="text-align: left;">005</td>
<td style="text-align: right;">75.0</td>
<td style="text-align: right;">58.5</td>
<td style="text-align: right;">72.0</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="even">
<td style="text-align: left;">006</td>
<td style="text-align: right;">74.0</td>
<td style="text-align: right;">68.0</td>
<td style="text-align: right;">69.5</td>
<td style="text-align: left;">female</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The figure below represents all the families, with the gender of the oldest child distinguished by color: red for daughters and blue for sons.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-mfc-scat3d" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mfc-scat3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="la-intro_files/figure-html/fig-mfc-scat3d-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mfc-scat3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Height of oldest child: daughters (red), sons (blue)
</figcaption>
</figure>
</div>
</div>
</div>
<p>In <a href="conditioning.html" class="quarto-xref"><span>Chapter 2</span></a> we regressed the son’s height on the father’s height. We obtained the regression line, which approximates the graph of averages: the average son’s height per father’s height. The linear regression can be interpreted as a linear prediction of the height of a son whose father is of some given height.</p>
<p>We can now expand on this idea by regressing the son’s height on the heights of both the mother and the father. This is a model in which the predicted son’s height, <span class="math inline">\(\hat{s}\)</span>, is some constant plus some linear combination of the parents’ heights.</p>
<p><span id="eq-s-per-mf-regression-plane"><span class="math display">\[
\begin{align}   
  \hat{s} &amp; = \mathcal{l}_{R}(m, f) \\   
  &amp;= \beta_0 \; + \; \beta_m \times m \; + \; \beta_f \times f
\end{align}
\qquad(6.1)\]</span></span></p>
<p>where</p>
<p><span id="eq-s-per-mf-2"><span class="math display">\[
\begin{align}
  \hat{s} &amp;= \text{predicted height of son} \\
  m &amp;= \text{height of mother}  \\
  f &amp;= \text{height of father}
\end{align}
\qquad(6.2)\]</span></span></p>
<p>Each set of coefficient values determines some plane in the 3-dimensional space of (mother, father, son) heights. The coefficients <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f)\)</span> obtained by linear regression determine the <em>regression plane</em> (<a href="#fig-mfs-scat3d" class="quarto-xref">Figure&nbsp;<span>6.2</span></a>) that gives the best linear approximation <span class="math inline">\((\hat{s})\)</span> to the son’s height for a given pair of parent heights <span class="math inline">\((m, f)\)</span>. <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-mfs-scat3d" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mfs-scat3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="la-intro_files/figure-html/fig-mfs-scat3d-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mfs-scat3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Son’s height given (mother, father) heights: predicted (plane) and observed (point)
</figcaption>
</figure>
</div>
</div>
</div>
<p>In vector-matrix notation we are seeking a vector <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f)\)</span> of coefficient values that yields the least-squares solution to the following linear approximation problem.</p>
<p><span id="eq-s-per-mf-3"><span class="math display">\[
\begin{align}
  s_\bullet &amp;\approx (1_\bullet, m_\bullet, f_\bullet) \times
    \begin{pmatrix}
      \hat{\beta}_0 \\ \hat{\beta}_m \\ \hat{\beta}_f
    \end{pmatrix}
\end{align}
\qquad(6.3)\]</span></span></p>
<p>where</p>
<p><span id="eq-s-per-mf-4"><span class="math display">\[
\begin{align}
  s_\bullet &amp;= \text{data column vector: heights of sons} \\
  1_\bullet &amp;= \text{column vector } (1, \ldots, 1) \\
  m_\bullet &amp;= \text{data column vector: heights of mothers}  \\
  f_\bullet &amp;= \text{data column vector: heights of fathers}
\end{align}
\qquad(6.4)\]</span></span></p>
<p>This is a statistical estimation problem that corresponds to the following linear algebra problem and notation.</p>
<p><span id="eq-b-Ax"><span class="math display">\[
\begin{align}
  b_\bullet &amp;\approx A_{\bullet, \bullet} \times x_\bullet
\end{align}
\qquad(6.5)\]</span></span></p>
<p>where</p>
<p><span id="eq-b-Ax-2"><span class="math display">\[
\begin{align}
  b_\bullet &amp;= s_\bullet \\
  A_{\bullet, \bullet} &amp;= (1_\bullet, m_\bullet, f_\bullet) \\
  x_\bullet &amp;= (\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f)
\end{align}
\qquad(6.6)\]</span></span></p>
<p>It turns out that the least squares solution <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f)\)</span> can be obtained as the vector of coefficients of an orthogonal projection of vector <span class="math inline">\(s_\bullet\)</span> onto the 3-dimensional subspace spanned by vectors <span class="math inline">\((1_\bullet, m_\bullet, f_\bullet)\)</span>. More on this later.</p>
</section>
<section id="survey-data-better-life-index" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="survey-data-better-life-index"><span class="header-section-number">6.1.2</span> Survey Data: Better Life Index</h3>
<p>We now turn to a data set having several data columns, namely the OECD’s Better Life Index (BLI). <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The following table shows a portion of the data.</p>
<div id="tbl-bli-wide" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bli-wide-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.2: Better Life Index (BLI)
</figcaption>
<div aria-describedby="tbl-bli-wide-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 42 × 26
  code  country     CG_SENG CG_VOTO EQ_AIRP EQ_WATER ES_EDUA ES_EDUEX ES_STCS
  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
1 AUS   Australia       2.7      92     6.7       92      84       20     499
2 AUT   Austria         1.3      76    12.2       92      86       17     491
3 BEL   Belgium         2        88    12.8       79      80       19     500
4 BRA   Brazil          2.2      80    11.7       70      57       16     400
5 CAN   Canada          2.9      68     7.1       90      92       17     517
6 CHE   Switzerland     2.3      45    10.1       96      89       17     498
# ℹ 36 more rows
# ℹ 17 more variables: HO_HISH &lt;dbl&gt;, HS_LEB &lt;dbl&gt;, HS_SFRH &lt;dbl&gt;,
#   IW_HADI &lt;dbl&gt;, IW_HNFW &lt;dbl&gt;, JE_EMPL &lt;dbl&gt;, JE_LMIS &lt;dbl&gt;, JE_LTUR &lt;dbl&gt;,
#   JE_PEARN &lt;dbl&gt;, PS_FSAFEN &lt;dbl&gt;, PS_REPH &lt;dbl&gt;, SC_SNTWS &lt;dbl&gt;,
#   SW_LIFS &lt;dbl&gt;, WL_EWLH &lt;dbl&gt;, WL_TNOW &lt;dbl&gt;, HO_BASE &lt;dbl&gt;, HO_NUMR &lt;dbl&gt;</code></pre>
</div>
</div>
</figure>
</div>
<p>Each row of this data matrix gives specified measurements of an identified country. The first two columns give, respectively, each country’s OECD code and name. The remaining 24 columns are measures pertaining to the well-being of the populace.</p>
<p>The column name of each measures consists of a two-letter prefix followed by a suffix. The prefix is associated with a broad indicator of social well-being. The suffix pertains to a particular component of this indicator. Here is an expansion of these prefixes.</p>
<div class="cell">
<div id="tbl-bli-comp-prefix" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bli-comp-prefix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.3: BLI Indicators and Sub-Components
</figcaption>
<div aria-describedby="tbl-bli-comp-prefix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<caption>BLI Indicators and Sub-Components</caption>
<colgroup>
<col style="width: 9%">
<col style="width: 30%">
<col style="width: 10%">
<col style="width: 49%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">prefix</th>
<th style="text-align: left;">name</th>
<th style="text-align: right;">n_comps</th>
<th style="text-align: left;">components</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CG</td>
<td style="text-align: left;">Civic Engagement</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">CG_SENG, CG_VOTO</td>
</tr>
<tr class="even">
<td style="text-align: left;">EQ</td>
<td style="text-align: left;">Environmental Quality</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">EQ_AIRP, EQ_WATER</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ES</td>
<td style="text-align: left;">Education System</td>
<td style="text-align: right;">3</td>
<td style="text-align: left;">ES_EDUA, ES_EDUEX, ES_STCS</td>
</tr>
<tr class="even">
<td style="text-align: left;">HO</td>
<td style="text-align: left;">Housing</td>
<td style="text-align: right;">3</td>
<td style="text-align: left;">HO_BASE, HO_HISH, HO_NUMR</td>
</tr>
<tr class="odd">
<td style="text-align: left;">HS</td>
<td style="text-align: left;">Health Status</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">HS_LEB, HS_SFRH</td>
</tr>
<tr class="even">
<td style="text-align: left;">IW</td>
<td style="text-align: left;">Income and Wealth</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">IW_HADI, IW_HNFW</td>
</tr>
<tr class="odd">
<td style="text-align: left;">JE</td>
<td style="text-align: left;">Jobs Employment</td>
<td style="text-align: right;">4</td>
<td style="text-align: left;">JE_EMPL, JE_LMIS, JE_LTUR, JE_PEARN</td>
</tr>
<tr class="even">
<td style="text-align: left;">PS</td>
<td style="text-align: left;">Personal Safety</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">PS_FSAFEN, PS_REPH</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SC</td>
<td style="text-align: left;">Social Connections</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">SC_SNTWS</td>
</tr>
<tr class="even">
<td style="text-align: left;">SW</td>
<td style="text-align: left;">Subjective Well-Being</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">SW_LIFS</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WL</td>
<td style="text-align: left;">Work Life Balance</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">WL_EWLH, WL_TNOW</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The component indicators (corresponding to the suffix of the column name) are elaborated in the following table.</p>
<div class="cell">
<div id="tbl-bli-components" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bli-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.4: BLI Component Indicators
</figcaption>
<div aria-describedby="tbl-bli-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<caption>BLI Component Indicators</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 30%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">prefix</th>
<th style="text-align: left;">suffix</th>
<th style="text-align: left;">unit</th>
<th style="text-align: left;">name</th>
<th style="text-align: left;">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CG</td>
<td style="text-align: left;">SENG</td>
<td style="text-align: left;">AVSCORE</td>
<td style="text-align: left;">Stakeholder Engagement</td>
<td style="text-align: left;">Extent to which people can engage with government in rule-making</td>
</tr>
<tr class="even">
<td style="text-align: left;">CG</td>
<td style="text-align: left;">VOTO</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Voter Turnout</td>
<td style="text-align: left;">Percent of registered voters who voted in recent elections</td>
</tr>
<tr class="odd">
<td style="text-align: left;">EQ</td>
<td style="text-align: left;">AIRP</td>
<td style="text-align: left;">MICRO_M3</td>
<td style="text-align: left;">Air Pollution</td>
<td style="text-align: left;">Concentration of PM2.5 particulate matter (micrograms per cubic meter)</td>
</tr>
<tr class="even">
<td style="text-align: left;">EQ</td>
<td style="text-align: left;">WATER</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Water Quality</td>
<td style="text-align: left;">Percent satisfied with water quality</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ES</td>
<td style="text-align: left;">EDUA</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Educational Attainment</td>
<td style="text-align: left;">Percent aged 25-64 with at least upper-secondary education</td>
</tr>
<tr class="even">
<td style="text-align: left;">ES</td>
<td style="text-align: left;">EDUEX</td>
<td style="text-align: left;">YR</td>
<td style="text-align: left;">Expected Years of Education</td>
<td style="text-align: left;">Expected years of schooling</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ES</td>
<td style="text-align: left;">STCS</td>
<td style="text-align: left;">AVSCORE</td>
<td style="text-align: left;">Student Cognitive Skills</td>
<td style="text-align: left;">PISA scores in reading, mathematics, and science</td>
</tr>
<tr class="even">
<td style="text-align: left;">HO</td>
<td style="text-align: left;">BASE</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Dwellings w/o Basic Facilities</td>
<td style="text-align: left;">Percentage of dwellings that lack basic sanitary facilities</td>
</tr>
<tr class="odd">
<td style="text-align: left;">HO</td>
<td style="text-align: left;">HISH</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Housing Expenditure</td>
<td style="text-align: left;">Percentage of household gross adjusted disposable income spent on housing</td>
</tr>
<tr class="even">
<td style="text-align: left;">HO</td>
<td style="text-align: left;">NUMR</td>
<td style="text-align: left;">RATIO</td>
<td style="text-align: left;">Rooms per Person</td>
<td style="text-align: left;">Number of rooms per person in dwelling</td>
</tr>
<tr class="odd">
<td style="text-align: left;">HS</td>
<td style="text-align: left;">LEB</td>
<td style="text-align: left;">YR</td>
<td style="text-align: left;">Life Expectancy at Birth</td>
<td style="text-align: left;">Average number of years a person can expect to live</td>
</tr>
<tr class="even">
<td style="text-align: left;">HS</td>
<td style="text-align: left;">SFRH</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Self-Reported Health</td>
<td style="text-align: left;">Percentage who report being in good or very good health</td>
</tr>
<tr class="odd">
<td style="text-align: left;">IW</td>
<td style="text-align: left;">HADI</td>
<td style="text-align: left;">USD</td>
<td style="text-align: left;">Household Adjusted Disposable Income</td>
<td style="text-align: left;">Average household income after taxes</td>
</tr>
<tr class="even">
<td style="text-align: left;">IW</td>
<td style="text-align: left;">HNFW</td>
<td style="text-align: left;">USD</td>
<td style="text-align: left;">Household Net Financial Wealth</td>
<td style="text-align: left;">Household net financial wealth (financial assets minus liabilities)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">JE</td>
<td style="text-align: left;">EMPL</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Employment Rate</td>
<td style="text-align: left;">Percentage of people aged 15-64 in paid employment</td>
</tr>
<tr class="even">
<td style="text-align: left;">JE</td>
<td style="text-align: left;">LMIS</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Labour Market Insecurity</td>
<td style="text-align: left;">Expected loss of earnings if someone becomes unemployed</td>
</tr>
<tr class="odd">
<td style="text-align: left;">JE</td>
<td style="text-align: left;">LTUR</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Long-Term Unemployment Rate</td>
<td style="text-align: left;">Percentage unemployed for 12+ months</td>
</tr>
<tr class="even">
<td style="text-align: left;">JE</td>
<td style="text-align: left;">PEARN</td>
<td style="text-align: left;">USD</td>
<td style="text-align: left;">Personal Earnings</td>
<td style="text-align: left;">Average annual earnings per full-time employee</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PS</td>
<td style="text-align: left;">FSAFEN</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Feeling Safe Walking Alone at Night</td>
<td style="text-align: left;">Percentage who feel safe</td>
</tr>
<tr class="even">
<td style="text-align: left;">PS</td>
<td style="text-align: left;">REPH</td>
<td style="text-align: left;">RATIO</td>
<td style="text-align: left;">Homicide Rate</td>
<td style="text-align: left;">Deaths per 100,000 people</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SC</td>
<td style="text-align: left;">SNTWS</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Support Network Quality</td>
<td style="text-align: left;">Percentage who believe they have someone to rely on in times of need</td>
</tr>
<tr class="even">
<td style="text-align: left;">SW</td>
<td style="text-align: left;">LIFS</td>
<td style="text-align: left;">AVSCORE</td>
<td style="text-align: left;">Life Satisfaction</td>
<td style="text-align: left;">Average self-evaluation on a scale from 0 to 10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WL</td>
<td style="text-align: left;">EWLH</td>
<td style="text-align: left;">PC</td>
<td style="text-align: left;">Employees Working Long Hours</td>
<td style="text-align: left;">Percentage of employees working 50+ hours per week</td>
</tr>
<tr class="even">
<td style="text-align: left;">WL</td>
<td style="text-align: left;">TNOW</td>
<td style="text-align: left;">HOUR</td>
<td style="text-align: left;">Time Devoted to Leisure and Personal Care</td>
<td style="text-align: left;">Hours per day spent on leisure, personal care, eating, and sleeping</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The <code>unit</code> column in the above table gives the unit of measure, with <code>PC</code> meaning percent, <code>YR</code> meaning number of years, and so on.</p>
<p>We now turn to a statistical and algebraic treatment of the BLI data matrix of <a href="#tbl-bli-wide" class="quarto-xref">Table&nbsp;<span>6.2</span></a>. Consider the indicator component <code>SW_LIFS</code> (Life Satisfaction) as a response variable, with the remaining 23 indicator components serving as explanatory variables. As with the previous data example, we want to approximate or predict the response variable by a constant <span class="math inline">\(\beta_0\)</span> plus a linear combination of the explantory variables, as follows.</p>
<p><span id="eq-L-per-C"><span class="math display">\[
\begin{align}
  L_\bullet &amp;\approx (1_\bullet, C_{\bullet, 1}, \ldots, C_{\bullet, d}) \times
    \begin{pmatrix}
      \hat{\beta}_0 \\ \hat{\beta}_1 \\ \vdots \\ \hat{\beta}_d
    \end{pmatrix}
\end{align}
\qquad(6.7)\]</span></span></p>
<p>where</p>
<p><span id="eq-L-per-C-2"><span class="math display">\[
\begin{align}
  L_\bullet &amp;= \text{life satisfaction indicator per country} \\
  1_\bullet &amp;= \text{column vector } (1, \ldots, 1) \\
  C_{\bullet, k} &amp;= k^{th} \text{ indicator component per country} \\
  d &amp;= \text{number of explanatory indicators}
\end{align}
\qquad(6.8)\]</span></span></p>
<p>We now have more explanatory variables than in the previous example, a fact that merits some comment.</p>
<p>On the one hand, the approach to determining least-squares regression coefficients <span class="math inline">\(\hat{\beta}_0, \ldots, \hat{\beta}_d\)</span> is unchanged. We project the response vector, now <span class="math inline">\(L_\bullet\)</span>, onto the space spanned by the constant vector <span class="math inline">\(1_\bullet\)</span> along with the explanatory variables, that is onto the space spanned by <span class="math inline">\((1_\bullet, C_{\bullet, 1}, \ldots, C_{\bullet, d})\)</span>. The fitted coefficients yield a function of the explanatory variables that forms a regression hyperplane of dimension 23 that passes through a cloud of data points, <span class="math inline">\((C_{\bullet, 1}, \ldots, C_{\bullet, d}, L_\bullet)\)</span>, in a space of dimension 24.</p>
<p>On the other hand, we are now estimating 24 regression coefficients based on observations from just 42 countries. From a statistical perspective, this paucity of observations relative to the number of estimates leads to large standard errors for the set of estimated coefficients. From the perspective of numerical linear algebra, the vector of fitted coefficients <span class="math inline">\((\hat{\beta}_0, \ldots, \hat{\beta}_d)\)</span> is less stable (more sensitive to error in the data) than it was in the previous example.</p>
</section>
<section id="mnist-images-of-handwritten-digits" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="mnist-images-of-handwritten-digits"><span class="header-section-number">6.1.3</span> MNIST: Images of Handwritten Digits</h3>
<p>The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten decimal digits consisting of 60,000 training images and 10,000 testing images. <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>The history of this database goes back to 1988, when the US Postal Service constructed images of digits appearing on handwritten zip codes. Around the same time the US Census Bureau requested NIST to evaluate optical character recognition (OCR) systems. In 1992, NIST and the Census Bureau sponsored a competition in which participating teams were given images of Handwriting Sample Forms (HSFs), including handwritten decimal digits. The initial version of MNIST was constructed sometime before summer 1994.</p>
<p>Here’s an example of each handwritten digit from the training set of images.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-xmpl-train" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-xmpl-train-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="la-intro_files/figure-html/fig-xmpl-train-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-xmpl-train-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Example images of handwritten digits from the MNIST dataset
</figcaption>
</figure>
</div>
</div>
</div>
<p>Each image is represented by a <span class="math inline">\(28 \times 28\)</span> matrix of pixels, with each pixel represented as a grayscale integer value from 0 through 255. That is, each image represents a single vector in a space of dimension 784 (since <span class="math inline">\(28 \times 28 = 784\)</span>).</p>
<p>The 1992 competition prompted the development of algorithms to determine the decimal digit represented by any such image. This is a classification problem: to label each case of data (image) as belonging to one of several possible categories (decimal digits).</p>
<p>One such method, multinomial logistic regression, assigns a probability that a given image represents a specified digit, resulting in a 10-element probability vector per image. <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<section id="multinomial-logistic-regression" class="level4" data-number="6.1.3.1">
<h4 data-number="6.1.3.1" class="anchored" data-anchor-id="multinomial-logistic-regression"><span class="header-section-number">6.1.3.1</span> Multinomial Logistic Regression</h4>
<p>To formulate the model, we convert the representation of an image from a <span class="math inline">\(28 \times 28\)</span> matrix of pixels into a vector of pixels of length 784. <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> We’ll denote such a vector as <span class="math inline">\((P_1, \ldots, P_d)\)</span>, where <span class="math inline">\(d = 784\)</span>.</p>
<p>Let <span class="math inline">\(D\)</span> denote the digit represented by the image. The ordering of the digits from 0 through 9 is not directly relevant to the image-recognition problem, so let us regard <span class="math inline">\(D\)</span> as a categorical variable having the set <span class="math inline">\(\{ 0, 1, \ldots, 9 \}\)</span> as possible values. An alternative representation is the set of indicator vectors <span class="math inline">\(e_0 = (1, 0, \ldots, 0)\)</span> through <span class="math inline">\(e_9 = (0, 0, \ldots, 1)\)</span>, called “one-hot encoding” in machine learning. <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Then the multinomial logistic regression model can be formulated as follows.</p>
<p><span id="eq-log-ratio-per-image"><span class="math display">\[
\begin{align}
  \log_e{ \frac{P(D = \nu)}{P(D = 0)} } &amp;= (1, P_1, \ldots, P_d) \times
    \begin{pmatrix}
      \beta_0^{(\nu)} \\ \beta_1^{(\nu)} \\ \vdots \\ \beta_d^{(\nu)}
    \end{pmatrix} &amp; \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align}
\qquad(6.9)\]</span></span></p>
<p>with</p>
<p><span id="eq-log-ratio-per-image-2"><span class="math display">\[
\begin{align}
  P(D = 0) &amp;= 1 - \sum_{\nu = 1}^9 P(D = \nu)
\end{align}
\qquad(6.10)\]</span></span></p>
<p>For a more compact notation let <span class="math inline">\(X_{\bullet} = (1, P_1, \ldots, P_d)\)</span> and let <span class="math inline">\(\beta_{\bullet}^{(\nu)} = (\beta_0^{(\nu)}, \beta_1^{(\nu)}, \ldots, \beta_d^{(\nu)})\)</span>, with the inner product <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> of these two vectors denoted as <span class="math inline">\(X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}\)</span>. Then we have</p>
<p><span id="eq-log-ratio-per-image-3"><span class="math display">\[
\begin{align}
  \log_e{ \frac{P(D = \nu)}{P(D = 0)} } &amp;= X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)} &amp; \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align}
\qquad(6.11)\]</span></span></p>
<p>Exponentiation of <a href="#eq-log-ratio-per-image-3" class="quarto-xref">Equation&nbsp;<span>6.11</span></a> gives:</p>
<p><span id="eq-log-ratio-per-image-4"><span class="math display">\[
\begin{align}
  \{ P(D = \nu) \} &amp;= \{ P(D = 0) \} \times e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}} &amp; \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align}
\qquad(6.12)\]</span></span></p>
<p>Taking the sum over <span class="math inline">\(\nu\)</span> we have:</p>
<p><span id="eq-log-ratio-per-image-5"><span class="math display">\[
\begin{align}
  \sum_{\nu = 1}^9 {P(D = \nu)} &amp;= \{ P(D = 0) \} \times \sum_{\nu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}}
\end{align}
\qquad(6.13)\]</span></span></p>
<p>Now applying <a href="#eq-log-ratio-per-image-2" class="quarto-xref">Equation&nbsp;<span>6.10</span></a> we have</p>
<p><span id="eq-log-ratio-per-image-6"><span class="math display">\[
\begin{align}
  \left \{ 1 - P(D = 0) \right \} &amp;= \{ P(D = 0) \} \times \sum_{\nu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}}
\end{align}
\qquad(6.14)\]</span></span></p>
<p>which yields:</p>
<p><span id="eq-log-ratio-per-image-7"><span class="math display">\[
\begin{align}
  P(D = 0) &amp;= \frac{1} { 1 + \sum_{\nu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}} }
\end{align}
\qquad(6.15)\]</span></span></p>
<p>Applying <a href="#eq-log-ratio-per-image-4" class="quarto-xref">Equation&nbsp;<span>6.12</span></a> gives:</p>
<p><span id="eq-log-ratio-per-image-8"><span class="math display">\[
\begin{align}
  P(D = \nu) &amp;= \frac{ e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}} } { 1 +  \sum_{\mu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\mu)}} } &amp; \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align}
\qquad(6.16)\]</span></span></p>
</section>
<section id="matrix-representation" class="level4" data-number="6.1.3.2">
<h4 data-number="6.1.3.2" class="anchored" data-anchor-id="matrix-representation"><span class="header-section-number">6.1.3.2</span> Matrix Representation</h4>
<p><a href="#eq-log-ratio-per-image-3" class="quarto-xref">Equation&nbsp;<span>6.11</span></a> pertains to the probability that a single image represents a single digit <span class="math inline">\(\nu \in \{1, \ldots, 9 \}\)</span>. Therefore, in a data set of <span class="math inline">\(n\)</span> images, with <span class="math inline">\(i\)</span> denoting the index of a particular image, we have:</p>
<p><span id="eq-log-ratio-per-image-9"><span class="math display">\[
\begin{align}
  \log_e{ \frac{P(D_i = \nu)}{P(D_i = 0)} } &amp;= X_{i, \bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}
\end{align}
\qquad(6.17)\]</span></span></p>
<p>Expanding the last equation to matrix notation, with <span class="math inline">\(i\)</span> as the row index and <span class="math inline">\(\nu\)</span> as a column index, we have</p>
<p><span id="eq-log-ratio-per-image-10"><span class="math display">\[
\begin{align}
&amp;
\begin{pmatrix}
  \log_e{ \frac{P(D_1 = 1)}{P(D_1 = 0)} }, &amp; \ldots, &amp; \log_e{ \frac{P(D_1 = 9)}{P(D_1 = 0)} } \\
  \vdots &amp; \vdots &amp; \vdots \\
  \log_e{ \frac{P(D_n = 1)}{P(D_n = 0)} }, &amp; \ldots, &amp; \log_e{ \frac{P(D_n = 9)}{P(D_n = 0)} }
\end{pmatrix}  \\ \\
&amp;=
\begin{pmatrix}
  X_{1, \bullet} \\
  \vdots \\
  X_{n, \bullet}
\end{pmatrix}
\begin{pmatrix}
  \beta_{\bullet}^{(1)}, &amp; \ldots, &amp; \beta_{\bullet}^{(9)}
\end{pmatrix}
\end{align}
\qquad(6.18)\]</span></span></p>
<p>The matrix on the left side of <a href="#eq-log-ratio-per-image-10" class="quarto-xref">Equation&nbsp;<span>6.18</span></a> has dimensions <span class="math inline">\(n \times 9\)</span>. On the right side, the first matrix factor has dimensions <span class="math inline">\(n \times 785\)</span>, and the second matrix factor has dimensions <span class="math inline">\(785 \times 9\)</span>.</p>
</section>
</section>
</section>
<section id="notation" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="notation"><span class="header-section-number">6.2</span> Notation</h2>
<p>The preceding section introduced example data sets along with corresponding linear regression models of the following form (the <em>generic linear model</em>). <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p><span id="eq-generic-lm"><span class="math display">\[
\begin{align}
  y &amp;= X \; \beta \; + \; \epsilon
\end{align}
\qquad(6.19)\]</span></span></p>
<p>Each of the elements of <a href="#eq-generic-lm" class="quarto-xref">Equation&nbsp;<span>6.19</span></a> has alternative names, including the following. <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p><span id="eq-generic-lm-terms"><span class="math display">\[
\begin{align}
  y &amp;= \text{a } \textit{response, target,} \text{ or } \textit{labeling}  \text{ variable} \\
  X &amp;= \text{feature matrix of } \textit{explanatory, predictor,} \text{ or } \textit{feature}  \text{ variables} \\
  \beta &amp;= \text{a vector of model } \textit{coefficients} \text{ or } \textit{parameters} \\
  \epsilon &amp;= \text{an } \textit{error} \text{ or } \textit{residual} \text{ term}
\end{align}
\qquad(6.20)\]</span></span></p>
<p>Here’s the linear regression model in matrix format for a given feature matrix of specified dimensions.</p>
<p><span id="eq-generic-lm-mat"><span class="math display">\[
\begin{align}
  y_\bullet &amp;= X_{\bullet, \bullet} \; \beta_\bullet \; + \; \epsilon_\bullet
\end{align}
\qquad(6.21)\]</span></span></p>
<p>Let the dimensions of <span class="math inline">\(X_{\bullet, \bullet}\)</span> be given as <span class="math inline">\(n\)</span> rows by <span class="math inline">\(d\)</span> columns. Then the column vectors <span class="math inline">\((y_\bullet, \epsilon_\bullet)\)</span> are each of length <span class="math inline">\(n\)</span>, and the number of rows in column-vector <span class="math inline">\(\beta_\bullet\)</span> is <span class="math inline">\(d\)</span>. Let us also delineate the columns of <span class="math inline">\(X_{\bullet, \bullet}\)</span>, the “feature vectors”, as follows.</p>
<p><span id="eq-generic-feature-cols"><span class="math display">\[
\begin{align}
  X_{\bullet, \bullet} &amp;= \left ( x_{\bullet, 1}, \ldots, x_{\bullet, d} \right )
\end{align}
\qquad(6.22)\]</span></span></p>
<p>The <span class="math inline">\(n-\)</span>dimensional feature vectors <span class="math inline">\(\{ x_{\bullet, k} \}_{k = 1}^d\)</span> span a subspace, “feature space”, within <span class="math inline">\(n-\)</span>space, denoted <span class="math inline">\(span(x_{\bullet, 1}, \ldots, x_{\bullet, d})\)</span>. Since this is the subspace generated by the columns of matrix <span class="math inline">\(X_{\bullet, \bullet}\)</span>, the subspace is also denoted as <span class="math inline">\(col(X_{\bullet, \bullet})\)</span>. <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>The mapping <span class="math inline">\(\beta_\bullet \mapsto X_{\bullet, \bullet} \; \beta_\bullet\)</span> sends a vector of coefficients <span class="math inline">\(\beta_\bullet\)</span> to the following linear combination within the feature subspace.</p>
<p><span id="eq-generic-feature-lin-combo"><span class="math display">\[
\begin{align}
  X_{\bullet, \bullet} \; \beta_\bullet &amp;=\beta_1 \; x_{\bullet, 1} \; + \; \cdots \; + \; \beta_d \; x_{\bullet, d}
\end{align}
\qquad(6.23)\]</span></span></p>
<p>The dimension of the feature subspace is called the <em>rank</em> of <span class="math inline">\(X_{\bullet, \bullet}\)</span>, denoted <span class="math inline">\(rank(X_{\bullet, \bullet})\)</span>. The rank can be no greater than <span class="math inline">\(n\)</span>, the dimension of the space containing each feature vector, nor can it be greater than <span class="math inline">\(d\)</span>, the number of feature vectors. If the feature vectors are linearly independent then this subspace is of dimension <span class="math inline">\(d\)</span>. If the feature vectors are not linearly independent (for example, if <span class="math inline">\(d &gt; n\)</span>) then the feature subspace is of some smaller dimension.</p>
<p>This linear regression format follows the more general mathematical notation <span class="math inline">\(y = f(x)\)</span>. In data science and machine learning, however, the response variable <span class="math inline">\(y\)</span> and the feature matrix <span class="math inline">\(X\)</span> have known values, whereas <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\epsilon\)</span> are <em>fit</em> (determined or evaluated based on <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span>) over the course of the modeling process.</p>
<p>In the data examples of the preceding section, the response variable took the following form.</p>
<ul>
<li>Family heights: <span class="math inline">\(y =\)</span> oldest child’s height</li>
<li>Better Life Index: <span class="math inline">\(y =\)</span> the Life Satisfaction indicator</li>
<li>MNIST: <span class="math inline">\(y =\)</span> a probability vector <span class="math inline">\(\{ P(D = \nu) \}_{\nu = 0}^9\)</span> assigned to each image</li>
</ul>
<p>The MNIST example illustrates a <em>vector-valued</em> rather than <em>scalar-valued</em> response variable.</p>
<p>If the data include a labeling or response variable, <span class="math inline">\(y\)</span>, then the problem is said to be <em>supervised</em>. In <em>unsupervised</em> problems (that lack a <span class="math inline">\(y\)</span> variable), we may need to find patterns in the given data. For example we may seek those feature variables (columns of the feature matrix <span class="math inline">\(X\)</span>), or linear combinations of feature variables, that account for most of the variability in the entire set of feature variables. Or we may need to find observations (rows of the feature matrix <span class="math inline">\(X\)</span>) that are similar and thereby form groups (or <em>clusters</em>) of observations. In these unsupervised situations we may model the feature matrix (or its covariance matrix) as the product of other matrices of special form (to be discussed later in this chapter).</p>
<p>In the remainder of this chapter we will focus on ideas and methods that help us to solve <a href="#eq-generic-lm" class="quarto-xref">Equation&nbsp;<span>6.19</span></a>, or rather, that help us to determine the value of <span class="math inline">\(\beta\)</span> that minimizes (in some sense) the residual term <span class="math inline">\(\epsilon\)</span>. We refer to this minimization as the <em>linear regression problem</em>, which is made precise once we specify the measure of <span class="math inline">\(\epsilon\)</span> to be minimized.</p>
</section>
<section id="geometry" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="geometry"><span class="header-section-number">6.3</span> Geometry</h2>
<p>We now consider the geometry of the <em>least-squares</em> solution of the linear regression problem, using the example of family heights. We begin by defining this measure of the residual term <span class="math inline">\(\epsilon\)</span>.</p>
<section id="distance-measures" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="distance-measures"><span class="header-section-number">6.3.1</span> Distance Measures</h3>
<p>The sum-of-squares measure of the residual vector <span class="math inline">\(\epsilon_\bullet = (\epsilon_1, \ldots, \epsilon_n)\)</span> is simply the sum of the squares of the components <span class="math inline">\(\epsilon_\nu\)</span>.</p>
<p><span id="eq-eps-ss"><span class="math display">\[
\begin{align}
  \sum_{\nu = 1}^n | \epsilon_\nu |^2
\end{align}
\qquad(6.24)\]</span></span></p>
<section id="vector-norms" class="level4" data-number="6.3.1.1">
<h4 data-number="6.3.1.1" class="anchored" data-anchor-id="vector-norms"><span class="header-section-number">6.3.1.1</span> Vector norms</h4>
<p><a href="#eq-eps-ss" class="quarto-xref">Equation&nbsp;<span>6.24</span></a> defines the following <em>norm</em> on <span class="math inline">\(n-\)</span>dimensional Euclidean space. For any vector <span class="math inline">\(v_\bullet = (v_1, \ldots, v_n) \in \mathbb{R}^n\)</span> we define <span class="math inline">\(\Vert v_\bullet \rVert_2\)</span> as follows.</p>
<p><span id="eq-l2-norm-n"><span class="math display">\[
\begin{align}
  \Vert v_\bullet \rVert_2
  &amp;= \left ( \sum_{\nu = 1}^n | v_\nu |^2 \right )^{\frac{1}{2}}
\end{align}
\qquad(6.25)\]</span></span></p>
<p>This norm can be derived from (or used to define) the <em>inner-product</em> of a pair of vectors <span class="math inline">\(v_\bullet, w_\bullet \in \mathbb{R}^n\)</span>, defined as follows with the following alternative notations.</p>
<p><span id="eq-inner-product-R-n"><span class="math display">\[
\begin{align}
  \left &lt;  v_\bullet, w_\bullet \right &gt;
  &amp;= v_\bullet \boldsymbol\cdot w_\bullet \\
  &amp;= v_\bullet^\top w_\bullet \\
  &amp;= \sum_{\nu = 1}^n v_\nu \; w_\nu
\end{align}
\qquad(6.26)\]</span></span></p>
<p>Then we have</p>
<p><span id="eq-l2-norm-inner-product"><span class="math display">\[
\begin{align}
  \Vert v_\bullet \rVert_2^2
  &amp;= \left &lt;  v_\bullet, v_\bullet \right &gt;
\end{align}
\qquad(6.27)\]</span></span></p>
<p>More generally, for any real number <span class="math inline">\(p \ge 1\)</span>, the so-called <span class="math inline">\(p-\)</span>norm (or Minkowski norm of order <span class="math inline">\(p\)</span>) is defined as</p>
<p><span id="eq-lp-norm-n"><span class="math display">\[
\begin{align}
  \Vert v_\bullet \rVert_p
  &amp;= \left ( \sum_{\nu = 1}^n | v_\nu |^p \right )^{\frac{1}{p}} &amp; \text{ for } 1 \le p &lt; \infty
\end{align}
\qquad(6.28)\]</span></span></p>
<p>This definition can be extended to the case <span class="math inline">\(p = \infty\)</span> as follows.</p>
<p><span id="eq-max-norm-n"><span class="math display">\[
\begin{align}
  \Vert v_\bullet \rVert_\infty
  &amp;= \max \left \{ |v_\nu | \right \}_{\nu = 1}^n
\end{align}
\qquad(6.29)\]</span></span></p>
<p>More generally, for <span class="math inline">\(v, w \in \mathcal{V}\)</span>, a real-valued or complex-valued vector space, a norm <span class="math inline">\(\Vert \cdot \rVert\)</span> is defined to have the following properties.</p>
<p><span id="eq-norm-properties"><span class="math display">\[
\begin{align}
  \Vert v \rVert &amp;\ge 0 \\
  \Vert v \rVert &amp;= 0 &amp; \text{ if and only if } v = 0 \\
  \Vert v + w \rVert &amp;\le \Vert v \rVert + \Vert w \rVert \\
  \Vert \lambda \; v \rVert &amp;= | \lambda | \; \Vert v \rVert &amp; \text{ for any scalar } \lambda
\end{align}
\qquad(6.30)\]</span></span></p>
</section>
<section id="matrix-norms" class="level4" data-number="6.3.1.2">
<h4 data-number="6.3.1.2" class="anchored" data-anchor-id="matrix-norms"><span class="header-section-number">6.3.1.2</span> Matrix norms</h4>
<p>If <span class="math inline">\(\lVert v_\bullet \rVert\)</span> denotes some defined norm for vectors <span class="math inline">\(v_\bullet \in \mathbb{R}^n\)</span> and if <span class="math inline">\(M_{\bullet, \bullet}\)</span> is an <span class="math inline">\(n \times n\)</span> numeric matrix, then the vector norm defines a corresponding matrix norm <span class="math inline">\(\lVert M_{\bullet, \bullet} \rVert\)</span> as follows.</p>
<p><span id="eq-m-norm-from-v-norm"><span class="math display">\[
\begin{align}
  \lVert M_{\bullet, \bullet} \rVert &amp;=
  \sup_{ \lVert v_\bullet \rVert = 1 }
  \left \{ \lVert  M_{\bullet, \bullet} \; v_\bullet \rVert \right \}
\end{align}
\qquad(6.31)\]</span></span></p>
<p>Note that some matrix norms are defined otherwise. For example, the Frobenius norm, <span class="math inline">\(\lVert M_{\bullet, \bullet} \rVert_F\)</span>, is defined as <span class="math inline">\(\lVert vec(M_{\bullet, \bullet}) \rVert_2\)</span>, where <span class="math inline">\(vec(M_{\bullet, \bullet})\)</span> converts an <span class="math inline">\(n \times n\)</span> matrix into a vector of length <span class="math inline">\(n^2\)</span> by concatenating matrix columns.</p>
</section>
<section id="metric-spaces" class="level4" data-number="6.3.1.3">
<h4 data-number="6.3.1.3" class="anchored" data-anchor-id="metric-spaces"><span class="header-section-number">6.3.1.3</span> Metric spaces</h4>
<p>A vector norm <span class="math inline">\(\Vert \cdot \rVert\)</span> defines a corresponding distance measure <span class="math inline">\(\delta_{\Vert \cdot \rVert}\)</span></p>
<p><span id="eq-norm-metric"><span class="math display">\[
\begin{align}
  \delta_{\Vert \cdot \rVert} (v_\bullet, w_\bullet)
  &amp;= \lVert v_\bullet - w_\bullet \rVert
\end{align}
\qquad(6.32)\]</span></span></p>
<p>A general distance measure or metric, <span class="math inline">\(\delta (\cdot, \cdot)\)</span>, together with the set of points <span class="math inline">\(\mathcal{M}\)</span> over which it is defined constitutes a <em>metric space</em> with the following properties, for any <span class="math inline">\(m_1, m_2, m_3 \in \mathcal{M}\)</span>.</p>
<p><span id="eq-metric-properties"><span class="math display">\[
\begin{align}
  \delta (m_1, m_1) &amp;= 0 \\
  \delta (m_1, m_2) &amp;&gt; 0 &amp; \text{ whenever } m_1 \ne m_2 \\
  \delta (m_1, m_2) &amp;= \delta (m_2, m_1) \\
  \delta (m_1, m_3) &amp;\le \delta (m_1, m_2) \; + \; \delta (m_2, m_3)
\end{align}
\qquad(6.33)\]</span></span></p>
<p>Hamming distance and Levenshtein distance are important examples of metrics used in natural language processing (NLP) that are not based on a vector norm. <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
</section>
</section>
<section id="family-heights" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="family-heights"><span class="header-section-number">6.3.2</span> Family heights</h3>
<p>Applying vector-matrix format of <a href="#eq-generic-lm-mat" class="quarto-xref">Equation&nbsp;<span>6.21</span></a> to the family heights data we have:</p>
<p><span id="eq-sfm-lm-mat"><span class="math display">\[
\begin{align}
  s_\bullet &amp;= (1_\bullet, m_\bullet, f_\bullet) \;
  \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2
  \end{pmatrix} \; + \; \epsilon_\bullet
\end{align}
\qquad(6.34)\]</span></span></p>
<p>where</p>
<p><span id="eq-sfm-lm-2"><span class="math display">\[
\begin{align}
  s_\bullet &amp;= \text{heights of sons } \\
  m_\bullet &amp;= \text{heights of mothers } \\
  f_\bullet &amp;= \text{heights of fathers } \\
  (1_\bullet, m_\bullet, f_\bullet) &amp;= \text{feature matrix} \\
  \beta_\bullet &amp;= \text{coefficient vector} = (\beta_0, \beta_1, \beta_2)^\top \\
  \epsilon_\bullet &amp;= \text{residual vector}
\end{align}
\qquad(6.35)\]</span></span></p>
<p>Consider the least-squares estimate <span class="math inline">\(\hat{\beta}_\bullet\)</span> and the consequent predicted height <span class="math inline">\(\hat{y}_\bullet = X_{_\bullet, _\bullet} \hat{\beta}_\bullet\)</span> of the son. <a href="#fig-smf-smpl" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> is based on a random sample of 20 families and shows the heights of sons on the vertical axis, along with their vertical displacement (residual) from the predicted value lying on the <em>regression plane</em>. <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-smf-smpl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-smf-smpl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="la-intro_files/figure-html/fig-smf-smpl-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-smf-smpl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: Sampled son heights: residual = observed - predicted
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-smf-smpl" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> represents individual rows of data, <span class="math inline">\(\{ (m_i, f_i, s_i) \}_{i = 1}^n\)</span> along with model predictions <span class="math inline">\((\hat{s} = \hat{\beta}_0 + \hat{\beta}_1 m + \hat{\beta}_2 f)\)</span> and residuals <span class="math inline">\((\hat{\epsilon} = s - \hat{s})\)</span> in three-dimensional <span class="math inline">\((m, f, s)\)</span> space.</p>
<p>To gain more insight into linear regression we’ll first reduce the regression problem to the simple case in which the response variable and the predictor variables have all been coerced to have an average value of zero, a process called <em>centering</em>. This will eliminate the need for the intercept coefficient, <span class="math inline">\(\beta_0\)</span>, and consequently eliminate the need to include the constant vector <span class="math inline">\(1_\bullet\)</span> in the feature matrix <span class="math inline">\(X_{_\bullet, _\bullet}\)</span>.</p>
</section>
<section id="centering-data-vectors" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="centering-data-vectors"><span class="header-section-number">6.3.3</span> Centering Data Vectors</h3>
<p>The regression plane that we glimpse in <a href="#fig-smf-smpl" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> actually spans all the <span class="math inline">\((m, f)\)</span> combinations that are mathematically possible. If we imagine infinitesimally short parents with <span class="math inline">\((m, f) = (0, 0)\)</span>, the predicted height of their son would be <span class="math inline">\(\hat{\beta}_0\)</span>, which is not zero. That is, the plane does not pass through the origin <span class="math inline">\((0, 0, 0)\)</span> and therefore does not qualify as a subspace of <span class="math inline">\((m, f, s)\)</span> space. <a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> But the regression plane determines a parallel subspace (that <em>does</em> pass through the origin).</p>
<p>The concept of a subspace is central to linear algebra. Therefore determining the subspace parallel to the regression plane will enable us to apply linear algebra methods to better understand linear regression.</p>
<p>One way to generate this subspace is to center each of the <span class="math inline">\((m_i, f_i, s_i)\)</span> data values, that is, to replace data value <span class="math inline">\(v_i\)</span> with its centered version <span class="math inline">\(\dot{v}_i = v_i - \bar{v}\)</span>, where <span class="math inline">\(\bar{v}\)</span> denotes the average value (arithmetic mean) of vector <span class="math inline">\(v_\bullet\)</span>.</p>
<p>In vector-matrix notation we have</p>
<p><span id="eq-avg-via-inner-product"><span class="math display">\[
\begin{align}
  \bar{v} &amp;= \frac{1}{n} \sum_{\nu = 1}^n v_\nu \\
  &amp;= \frac{1}{n} \;  1_\bullet^\top \; v_\bullet \\ \\
  &amp; \text{so that} \\ \\
  \dot{v}_\bullet &amp;= v_\bullet - ( \bar{v} \; 1_\bullet ) \\
  &amp;= v_\bullet - \frac{1}{n} \;  1_\bullet \;  1_\bullet^\top \; v_\bullet \\
  &amp;= \left ( I - \frac{1}{n} \;  1_\bullet \;  1_\bullet^\top \right ) \; v_\bullet
\end{align}
\qquad(6.36)\]</span></span></p>
<p>Let <span class="math inline">\(C_{\bullet, \bullet}\)</span> denote the matrix factor on the right side of the last equality, and define vector <span class="math inline">\(\tilde{1}_\bullet\)</span> as follows.</p>
<p><span id="eq-centering-matrix"><span class="math display">\[
\begin{align}
  \tilde{1}_\bullet &amp;= \frac{1}{\sqrt{n}} \;  1_\bullet \\ \\
  &amp; \text{so that} \\ \\
  \lVert \tilde{1}_\bullet \rVert &amp;= 1 \\ \\
  &amp; \text{and} \\ \\
  C_{\bullet, \bullet} &amp;= I \; - \; \tilde{1}_\bullet \; \tilde{1}_\bullet^\top
\end{align}
\qquad(6.37)\]</span></span></p>
<p>Then we have</p>
<p><span id="eq-centering-projection"><span class="math display">\[
\begin{align}
  \tilde{1}_\bullet \; \tilde{1}_\bullet^\top \;  v_\bullet
  &amp;= \bar{v} \; 1_\bullet \\ \\
  C_{\bullet, \bullet} \; v_\bullet &amp;= v_\bullet \; - \; \bar{v} \; 1_\bullet \\
  &amp;= \dot{v}_\bullet
\end{align}
\qquad(6.38)\]</span></span></p>
<p>Setting <span class="math inline">\(v_\bullet = 1_\bullet\)</span> gives</p>
<p><span id="eq-centering-eigenvector"><span class="math display">\[
\begin{align}
  \left ( \tilde{1}_\bullet \; \tilde{1}_\bullet^\top \right ) \;  1_\bullet
  &amp;= 1_\bullet \\ \\
  C_{\bullet, \bullet} \; 1_\bullet &amp;= 0_\bullet
\end{align}
\qquad(6.39)\]</span></span></p>
<p>We now multiply both sides of <a href="#eq-generic-lm-mat" class="quarto-xref">Equation&nbsp;<span>6.21</span></a>, the generic regression equation, by matrix <span class="math inline">\(C_{\bullet, \bullet}\)</span> to obtain</p>
<p><span id="eq-centering-generic-lm"><span class="math display">\[
\begin{align}
  C_{\bullet, \bullet} \; y_\bullet
  &amp;= C_{\bullet, \bullet} \; X_{\bullet, \bullet} \; \beta_\bullet
  \; + \; C_{\bullet, \bullet} \; \epsilon_\bullet
\end{align}
\qquad(6.40)\]</span></span></p>
<p>Now for the family heights data (<a href="#eq-sfm-lm-mat" class="quarto-xref">Equation&nbsp;<span>6.34</span></a>) we have</p>
<p><span id="eq-centering-sfm-lm"><span class="math display">\[
\begin{align}
  C_{\bullet, \bullet} \; X_{\bullet, \bullet} \; \beta_\bullet
  &amp;= C_{\bullet, \bullet} \; (1_\bullet, m_\bullet, f_\bullet) \;
  \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2
  \end{pmatrix} \\
  &amp;= (0_\bullet, \dot{m}_\bullet, \dot{f}_\bullet) \;
  \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2
  \end{pmatrix} \\
  &amp;= \beta_1 \; \dot{m}_\bullet \; + \; \beta_2 \; \dot{f}_\bullet \\
  &amp;= (\dot{m}_\bullet, \dot{f}_\bullet) \;
  \begin{pmatrix}
    \beta_1 \\
    \beta_2
  \end{pmatrix}
\end{align}
\qquad(6.41)\]</span></span></p>
<p>Then the centered version of <a href="#eq-sfm-lm-mat" class="quarto-xref">Equation&nbsp;<span>6.34</span></a> is</p>
<p><span id="eq-sfm-lm-ctr"><span class="math display">\[
\begin{align}
  \dot{s}_\bullet &amp;= (\dot{m}_\bullet, \dot{f}_\bullet) \;
  \begin{pmatrix}
    \beta_1 \\
    \beta_2
  \end{pmatrix} \; + \; \dot{\epsilon}_\bullet
\end{align}
\qquad(6.42)\]</span></span></p>
<p>where</p>
<p><span id="eq-sfm-lm-ctr-2"><span class="math display">\[
\begin{align}
  \dot{s}_\bullet &amp;= \text{centered heights of sons } \\
  \dot{m}_\bullet &amp;= \text{centered heights of mothers } \\
  \dot{f}_\bullet &amp;= \text{centered heights of fathers } \\
  (\dot{m}_\bullet, \dot{f}_\bullet) &amp;= \text{centered feature matrix} \\
  \beta_\bullet &amp;= \text{coefficient vector} = (\beta_1, \beta_2)^\top \\
  \dot{\epsilon}_\bullet &amp;= \text{centered residual vector}
\end{align}
\qquad(6.43)\]</span></span></p>
<p>That is, we can eliminate the intercept coefficient from the centered linear model, and we can also eliminate the constant vector <span class="math inline">\(1_\bullet\)</span> from the feature matrix <span class="math inline">\(X_{_\bullet, _\bullet}\)</span>. <a href="#fig-row-smf-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.5</span></a> is a version of <a href="#fig-smf-smpl" class="quarto-xref">Figure&nbsp;<span>6.4</span></a> corresponding to <a href="#eq-sfm-lm-ctr" class="quarto-xref">Equation&nbsp;<span>6.42</span></a>. Geometrically it’s the same figure, the difference being that each of the three axes has been shifted, now with 0 as the central value.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-row-smf-smpl-ctr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-row-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="la-intro_files/figure-html/fig-row-smf-smpl-ctr-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-row-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: Centered family heights
</figcaption>
</figure>
</div>
</div>
</div>
<p>The advantage of centering the family heights data is that <a href="#fig-row-smf-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.5</span></a> above represents all the dimensions of the centered linear model: the vector of responses <span class="math inline">\(\dot{s}_\bullet\)</span> along with the feature vectors <span class="math inline">\((\dot{m}_\bullet, \dot{f}_\bullet)\)</span>. The disadvantage is that we have replaced heights with deviations from average heights, and those average heights can no longer be discerned from the scatter diagram.</p>
<p>On the other hand, prior to data-centering our feature matrix included the constant vector <span class="math inline">\(1_\bullet\)</span> (a vector neither interesting nor visible in our scatter diagrams) in order for the model to include a constant coefficient <span class="math inline">\(\beta_0\)</span> that accounted for the distinct average heights of (mother, father, son).</p>
<p>These are two representations of essentially the same linear model, as can be seen by reconstructing the original variables from their centered versions:</p>
<p><span id="eq-sfm-lm-ctr-3"><span class="math display">\[
\begin{align}
  \dot{s}_\bullet &amp;= (\dot{m}_\bullet, \dot{f}_\bullet) \;
  \begin{pmatrix}
    \beta_1 \\
    \beta_2
  \end{pmatrix} \; + \; \dot{\epsilon}_\bullet \\
  &amp;= \beta_1 \; \dot{m}_\bullet \; + \;
     \beta_2 \; \dot{f}_\bullet \; + \;
     \dot{\epsilon}_\bullet \\ \\
     &amp;\text{ or equivalently } \\ \\
  s_\bullet - \bar{s} 1_\bullet
  &amp;= \beta_1 \; (m_\bullet - \bar{m} 1_\bullet) \; + \;
     \beta_2 \; (f_\bullet - \bar{f} 1_\bullet) \; + \;
     \dot{\epsilon}_\bullet \\ \\
     &amp;\text{ so that } \\ \\
  s_\bullet
  &amp;= (\bar{s} \; - \beta_1 \;\bar{m} \; - \beta_2 \;\bar{f}) \; 1_\bullet \; + \;
     \beta_1 \; m_\bullet \; + \;
     \beta_2 \; f_\bullet \; + \;
     \dot{\epsilon}_\bullet \\
  &amp;= \tilde{\beta}_0 \; 1_\bullet \; + \;
     \beta_1 \; m_\bullet \; + \;
     \beta_2 \; f_\bullet \; + \;
     \dot{\epsilon}_\bullet
\end{align}
\qquad(6.44)\]</span></span></p>
<p>In words, the centered model, having only two free coefficients <span class="math inline">\((\beta_1, \beta_2)\)</span>, is equivalent to an uncentered model subject to the following constraints:</p>
<ul>
<li>the constant coefficient <span class="math inline">\(\tilde{\beta}_0\)</span> is a certain linear combination of the average heights of (mother, father, son) that uses coefficients <span class="math inline">\((\beta_1, \beta_2)\)</span> and thereby forces the regression plane to pass through the (mathematical) <em>point of averages</em> <span class="math inline">\((\bar{m}, \bar{f}, \bar{s})\)</span>; and</li>
<li>the residual vector is constrained to have an average value of zero.</li>
</ul>
</section>
<section id="least-squares-solutions" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="least-squares-solutions"><span class="header-section-number">6.3.4</span> Least Squares Solutions</h3>
<p>We now discuss the <em>least-squares</em> solution to the generic linear regression problem (<a href="#eq-generic-lm-mat" class="quarto-xref">Equation&nbsp;<span>6.21</span></a>), which determines coefficient values <span class="math inline">\(\hat{\beta}_\bullet\)</span> that minimize the sum of squared residuals.</p>
<p><span id="eq-generic-lm-resid-ss"><span class="math display">\[
\begin{align}
  \sum_{i = 1}^n \epsilon_i^2 &amp;= \lVert \epsilon_\bullet \rVert^2 \\
  &amp;= \epsilon_\bullet^\top\epsilon_\bullet \\
  &amp;= (y_\bullet - X_{\bullet, \bullet} \beta_\bullet)^\top (y_\bullet - X_{\bullet, \bullet} \beta_\bullet)
\end{align}
\qquad(6.45)\]</span></span></p>
<p>To find coefficient values that minimize this sum of squares, one can take derivatives of the above expression with respect to <span class="math inline">\(\beta_\bullet\)</span> and set that result to zero, which yields the following <em>normal equations</em>:</p>
<p><span id="eq-generic-lm-nrml-eqs"><span class="math display">\[
\begin{align}
  X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet} \; \hat{\beta}_\bullet
  &amp;= X_{\bullet, \bullet}^\top \; y_\bullet
\end{align}
\qquad(6.46)\]</span></span></p>
<p>On the left side of the normal equations we have the matrix factor <span class="math inline">\(\left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )\)</span>. For the centered heights data this matrix factor is proportional to the (mother, father) covariance matrix, a <span class="math inline">\(2 \times 2\)</span> positive-definite matrix, and thus an invertible matrix. If this matrix factor is invertible, one can solve for <span class="math inline">\(\hat{\beta}_\bullet\)</span> as follows.</p>
<p><span id="eq-generic-lm-beta-hat"><span class="math display">\[
\begin{align}
  \hat{\beta}_\bullet
  &amp;= \left ( X_{\bullet, \bullet}^\top \; X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \; y_\bullet
\end{align}
\qquad(6.47)\]</span></span></p>
<p>The predicted vector <span class="math inline">\(\hat{y}_\bullet\)</span> is thus:</p>
<p><span id="eq-generic-lm-y-hat"><span class="math display">\[
\begin{align}
  \hat{y}_\bullet
  &amp;= X_{\bullet, \bullet} \hat{\beta}_\bullet \\
  &amp;= X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top y_\bullet
\end{align}
\qquad(6.48)\]</span></span></p>
</section>
<section id="orthogonal-projections" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="orthogonal-projections"><span class="header-section-number">6.3.5</span> Orthogonal Projections</h3>
<p>As previously noted (<a href="#eq-generic-feature-lin-combo" class="quarto-xref">Equation&nbsp;<span>6.23</span></a>), the mapping <span class="math inline">\(\beta_\bullet \mapsto X_{\bullet, \bullet} \beta_\bullet\)</span> sends coefficient vector <span class="math inline">\(\beta_\bullet\)</span> to a linear combination of the feature vectors, which is therefore a vector within the feature subspace. The particular coefficient vector <span class="math inline">\(\hat{\beta}_\bullet\)</span> obtained by least squares linear regression produces the linear mapping of <a href="#eq-generic-lm-y-hat" class="quarto-xref">Equation&nbsp;<span>6.48</span></a>:</p>
<p><span id="eq-lin-reg-projection"><span class="math display">\[
\begin{align}
  \hat{y}_\bullet &amp;= P \; y_\bullet
\end{align}
\qquad(6.49)\]</span></span></p>
<p>where <span class="math inline">\(P\)</span> is the following matrix.</p>
<p><span id="eq-lin-reg-projection-mat"><span class="math display">\[
\begin{align}
  P
  &amp;= X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top
\end{align}
\qquad(6.50)\]</span></span></p>
<p>Matrix <span class="math inline">\(P\)</span> is idempotent and symmetric, that is, both its square <span class="math inline">\(P^2\)</span> and its transpose <span class="math inline">\(P^\top\)</span> equal <span class="math inline">\(P\)</span> itself.</p>
<p><span id="eq-projection-properties"><span class="math display">\[
\begin{align}
  P^2
  &amp;= \left \{ X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \right \}
  \left \{ X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \right \} \\
  &amp;= X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \\
  &amp;= P \\ \\
  P^\top
  &amp;= \left \{ X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \right \}^\top \\
  &amp;=  X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \\
  &amp;= P
\end{align}
\qquad(6.51)\]</span></span></p>
<p>If a square matrix <span class="math inline">\(M\)</span> is idempotent, that is, if <span class="math inline">\(M^2 = M\)</span>, then <span class="math inline">\(M\)</span> represents a <em>projection</em>. Repeated applications of <span class="math inline">\(M\)</span> to vector <span class="math inline">\(v\)</span> return the initial application, i.e., <span class="math inline">\(M^k v = Mv\)</span> for any positive integer <span class="math inline">\(k\)</span>.</p>
<p>If in addition matrix <span class="math inline">\(M\)</span> is symmetric, that is, if <span class="math inline">\(M^\top = M\)</span>, then <span class="math inline">\(M\)</span> represents an <em>orthogonal projection</em>. In this case the complement of <span class="math inline">\(M\)</span>, <span class="math inline">\(I - M\)</span>, also qualifies as an orthogonal projection and the product of the two matrices is the zero matrix.</p>
<p>Consequently, any vector <span class="math inline">\(v\)</span> can be expressed as the sum of two vectors <span class="math inline">\(v = x + y\)</span>, with <span class="math inline">\(x = M v\)</span> and <span class="math inline">\(y = (I-M) v\)</span>. Vector <span class="math inline">\(x\)</span> belongs to the subspace spanned by the columns of <span class="math inline">\(M\)</span>, which is called the <em>image</em> or <em>column-space</em> of <span class="math inline">\(M\)</span>, denoted as <span class="math inline">\(col (M)\)</span>. Similarly <span class="math inline">\(y \in col (I - M)\)</span>. Moreover, these two vectors are orthogonal: <span class="math inline">\((x^\top y = y^\top x = 0)\)</span>. That is, subspace <span class="math inline">\(col (I - M)\)</span> is the orthogonal complement of subspace <span class="math inline">\(col (M)\)</span>.</p>
<p>Let’s apply these ideas to matrix <span class="math inline">\(P\)</span>. First, we have shown that matrix <span class="math inline">\(P\)</span> represents an orthogonal projection. On closer inspection, we can show that the subspace generated by <span class="math inline">\(P\)</span>, <span class="math inline">\(col (P)\)</span>, is the feature subspace. That is, for any vector <span class="math inline">\(v_\bullet\)</span> we have:</p>
<p><span id="eq-feature-projection"><span class="math display">\[
\begin{align}
  P \; v_\bullet &amp;= X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \; v_\bullet \\
  &amp;= X_{\bullet, \bullet} \left \{ \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \; v_\bullet \right \} \\
  &amp;= X_{\bullet, \bullet} \left \{ \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1}
  \begin{pmatrix}
    \dot{m}_{\bullet}^\top \; v_\bullet \\
    \dot{f}_{\bullet}^\top \; v_\bullet
  \end{pmatrix}
  \right \} \\
  &amp;= X_{\bullet, \bullet} \; \gamma_\bullet (v_\bullet)
\end{align}
\qquad(6.52)\]</span></span></p>
<p>In words, for any vector <span class="math inline">\(v_\bullet\)</span> in <span class="math inline">\(n-\)</span>space, <span class="math inline">\(P\)</span> sends <span class="math inline">\(v_\bullet\)</span> to an <span class="math inline">\(n-\)</span>vector of the form <span class="math inline">\(X_{\bullet, \bullet} \; \gamma_\bullet\)</span>, which belongs to the feature subspace, <span class="math inline">\(col(X_{\bullet, \bullet})\)</span>.</p>
<p>This means that the respective vectors of predicted response values <span class="math inline">\(\hat{y_\bullet}\)</span> and their residuals <span class="math inline">\(\hat{\epsilon}_\bullet\)</span> are orthogonal.</p>
<p><span id="eq-resid-predicted-ortho"><span class="math display">\[
\begin{align}
  \hat{y}_\bullet &amp;= P \; y_\bullet \\ \\
  \hat{\epsilon}_\bullet &amp;= y_\bullet - \hat{y}_\bullet \\
  &amp;= (I - P) \; y_\bullet \\ \\
  \hat{\epsilon}_\bullet^\top \; y_\bullet &amp;= y_\bullet^\top \; (I - P)^\top P \; y_\bullet \\
  &amp;= y_\bullet^\top \; (I - P) \; P \; y_\bullet \\
  &amp;= y_\bullet^\top \; (P - P^2) \; y_\bullet \\
  &amp;= y_\bullet^\top \; 0_{\bullet, \bullet} \; y_\bullet \\
  &amp;= 0
\end{align}
\qquad(6.53)\]</span></span></p>
<p>Now let <span class="math inline">\(\phi_\bullet\)</span> be any vector in feature space. Then <span class="math inline">\(\phi_\bullet\)</span> is some linear combination of the feature vectors and therefore can be represented as <span class="math inline">\(\phi_\bullet = X_{\bullet, \bullet} \gamma_\bullet\)</span> for some coefficient vector <span class="math inline">\(\gamma_\bullet\)</span>. It now follows the <span class="math inline">\(P \; \phi_\bullet = \phi_\bullet\)</span>:</p>
<p><span id="eq-subspace-invariance"><span class="math display">\[
\begin{align}
  P \; \phi_\bullet &amp;= P \; (X_{\bullet, \bullet} \gamma_\bullet) \\
  &amp;= X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \;  (X_{\bullet, \bullet} \gamma_\bullet) \\
  &amp;= X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} \left ( X_{\bullet, \bullet}^\top \;  X_{\bullet, \bullet} \right ) \gamma_\bullet \\
  &amp;= X_{\bullet, \bullet} \gamma_\bullet \\
  &amp;= \phi_\bullet
\end{align}
\qquad(6.54)\]</span></span></p>
<p>Consequently, the residual vector <span class="math inline">\(\hat{\epsilon}_\bullet\)</span> is orthogonal to any vector <span class="math inline">\(\phi_\bullet = X_{\bullet, \bullet} \gamma_\bullet\)</span> in the feature subspace:</p>
<p><span id="eq-resid-mf-ortho"><span class="math display">\[
\begin{align}
  \hat{\epsilon}_\bullet^\top \; \phi_\bullet &amp;= \dot{y}_\bullet^\top \; (I - P)^\top \phi_\bullet \\
  &amp;= \dot{y}_\bullet^\top \; (I - P) \; \phi_\bullet \\
  &amp;= \dot{y}_\bullet^\top \; 0_\bullet \\
  &amp;= 0
\end{align}
\qquad(6.55)\]</span></span></p>
<p>It now follows that of all vectors <span class="math inline">\(\phi_\bullet = X_{\bullet, \bullet} \gamma_\bullet\)</span> in the feature subspace, the predicted vector <span class="math inline">\(\hat{y}_\bullet\)</span> is closest to the given vector <span class="math inline">\(y_\bullet\)</span>:</p>
<p><span id="eq-lin-reg-min-distance"><span class="math display">\[
\begin{align}
  \lVert y_\bullet - \phi_\bullet \rVert^2
  &amp;= \lVert (y_\bullet - \hat{y}_\bullet) + (\hat{y}_\bullet - \phi_\bullet) \rVert^2  \\
  &amp;= \lVert \hat{\epsilon}_\bullet + (\hat{y}_\bullet - \phi_\bullet) \rVert^2 \\
  &amp;= \left ( \hat{\epsilon}_\bullet + (\hat{y}_\bullet - \phi_\bullet) \right )^\top
  \left ( \hat{\epsilon}_\bullet + (\hat{y}_\bullet - \phi_\bullet) \right ) \\
  &amp;= \hat{\epsilon}_\bullet^\top \hat{\epsilon}_\bullet \; + \; 0 \; + \; 0 \; + \; (\hat{y}_\bullet - \phi_\bullet)^\top (\hat{y}_\bullet - \phi_\bullet) \\
  &amp;= \lVert \hat{\epsilon}_\bullet \rVert^2 \; + \; \lVert \hat{y}_\bullet - \phi_\bullet \rVert^2 \\
  &amp;\ge \lVert \hat{\epsilon}_\bullet \rVert^2 \\
  &amp;= \lVert y_\bullet - \hat{y}_\bullet \rVert^2
\end{align}
\qquad(6.56)\]</span></span></p>
<p>There is one more point worth noting here. Suppose <span class="math inline">\(X_{\bullet, \bullet}\)</span> consisted of just a single column, say <span class="math inline">\(\dot{m}_\bullet\)</span>, the centered heights of mothers.</p>
<p><span id="eq-1D-cov-mat"><span class="math display">\[
\begin{align}
  X_{\bullet, \bullet} &amp;= \dot{m}_\bullet \\ \\
  X_{\bullet, \bullet}^\top X_{\bullet, \bullet}
  &amp;= \dot{m}_\bullet^\top \dot{m}_\bullet \\
  &amp;= \lVert \dot{m}_\bullet \rVert^2
\end{align}
\qquad(6.57)\]</span></span></p>
<p>Then we would have:</p>
<p><span id="eq-1D-projection-mat"><span class="math display">\[
\begin{align}
  P
  &amp;= X_{\bullet, \bullet} \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \\
  &amp;= \dot{m}_\bullet
    \frac{1}{\lVert \dot{m}_\bullet \rVert^2} \; \dot{m}_\bullet^\top \\
  &amp;= \left ( \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \right )
     \left ( \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \right )^\top \\
  &amp;= u_\bullet \; u_\bullet^\top \\ \\
  \text{where} \\ \\
  u_\bullet &amp;= \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \\ \\
  \text{so that} \\ \\
  \lVert u_\bullet \rVert &amp;= 1
\end{align}
\qquad(6.58)\]</span></span></p>
<p>That is, projection matrix <span class="math inline">\(P\)</span> can take the form of a 1-dimensional projection <span class="math inline">\(u \; u^\top\)</span> onto multiples of unit vector <span class="math inline">\(u\)</span>, and generalizes such 1-dimensional projections when the feature space is of a higher dimension.</p>
<p><a href="#fig-row-smf-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.5</span></a> above shows the result of projecting the centered sons’ heights to their predicted values in the parental plane (feature space). Each point in that figure represents an individual family, which corresponds to a single row of the centered feature matrix <span class="math inline">\((\dot{m}_\bullet, \dot{f}_\bullet)\)</span>. In the next section we introduce a different perspective on linear regression, namely a column-based view.</p>
</section>
</section>
<section id="column-versus-row-visualization" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="column-versus-row-visualization"><span class="header-section-number">6.4</span> Column versus Row Visualization</h2>
<p>Continuing with the example of centered heights, let’s now take a step back from two explanatory variables to just one, namely the mother’s centered height <span class="math inline">\(\dot{m}_\bullet\)</span> as a predictor of the son’s centered height <span class="math inline">\(\dot{s}_\bullet\)</span>. From <a href="#eq-1D-projection-mat" class="quarto-xref">Equation&nbsp;<span>6.58</span></a> we have</p>
<p><span id="eq-1D-ctr-lin-reg-soln"><span class="math display">\[
\begin{align}
  P
  &amp;= \left ( \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \right )
     \left ( \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \right )^\top \\ \\
  \hat{\beta}_\bullet &amp;= \left ( X_{\bullet, \bullet}^\top X_{\bullet, \bullet}  \right )^{-1} X_{\bullet, \bullet}^\top \; \dot{y}_\bullet \\
  &amp;= \frac{\dot{m}_\bullet^\top \; \dot{y}}{\lVert \dot{m}_\bullet \rVert^2} \\
  &amp;= \hat{\beta}_1 \\ \\
  \text{so that} \\ \\
  \hat{\dot{s}} &amp;= P \; \dot{s} \\
  &amp;= \left ( \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \right )
     \left ( \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \right )^\top \; \dot{s} \\
  &amp;= \frac{\dot{m}_\bullet^\top \; \dot{s}}{\lVert \dot{m}_\bullet \rVert^2} \; \dot{m}_\bullet \\
  &amp;= \hat{\beta}_1 \; \dot{m}_\bullet
\end{align}
\qquad(6.59)\]</span></span></p>
<p><a href="#fig-col-sm-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.6</span></a> shows this projection from <span class="math inline">\(\dot{s}_\bullet\)</span>) to the one-dimensional space spanned by <span class="math inline">\(\dot{m}_\bullet\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-col-sm-smpl-ctr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-col-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="la-intro_files/figure-html/fig-col-sm-smpl-ctr-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-col-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: Centered heights: s-vector (sons) projected to to m-axis (mothers)
</figcaption>
</figure>
</div>
</div>
</div>
<p>The coordinate system of this figure refers to the pair of basis vectors <span class="math inline">\(( \dot{m}_\bullet, \dot{s}_\bullet )\)</span>. If <span class="math inline">\(v\)</span> is a vector in this two-dimensional space then <span class="math inline">\(v\)</span> is some linear combination of the basis vectors.</p>
<p><span id="eq-v-in-ms-ctr-space"><span class="math display">\[
\begin{align}
  v &amp;= \sigma \; \dot{s}_\bullet \; + \; \mu \; \dot{m}_\bullet
\end{align}
\qquad(6.60)\]</span></span></p>
<p>Then <span class="math inline">\(v\)</span> has coordinates <span class="math inline">\((\sigma, \mu)\)</span> with respect to the <span class="math inline">\(( \dot{m}_\bullet, \dot{s}_\bullet )\)</span> basis.</p>
<p>Consequently the coordinates of vectors <span class="math inline">\(\dot{m}_\bullet\)</span>, <span class="math inline">\(\dot{s}_\bullet\)</span>, and <span class="math inline">\(\hat{\dot{s}}_\bullet\)</span> are respectively <span class="math inline">\((1, 0)\)</span>, <span class="math inline">\((0, 1)\)</span>, and <span class="math inline">\((\hat{\beta}_1, 0)\)</span>.</p>
<p>Note that the <span class="math inline">\(\dot{s}_\bullet\)</span> axis is not quite perpendicular to the <span class="math inline">\(\dot{m}_\bullet\)</span> axis. That is because the two vectors are not orthogonal:</p>
<p><span id="eq-m-s-inner-product"><span class="math display">\[
\begin{align}
  \left &lt; \dot{m}_\bullet, \; \dot{s}_\bullet \right &gt; &amp;= \dot{m}_\bullet^\top \; \dot{s}_\bullet \\
  &amp;\ne 0
\end{align}
\qquad(6.61)\]</span></span></p>
<p>Instead we have the following non-zero correlation coefficient, denoted here as <span class="math inline">\(r_{m, s}\)</span>. <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<p><span id="eq-m-s-cor"><span class="math display">\[
\begin{align}
  r_{m, s} &amp;= \left ( \frac{\dot{m}_\bullet}{\lVert \dot{m}_\bullet \rVert} \right )^\top \;
  \left ( \frac{\dot{s}_\bullet}{\lVert \dot{s}_\bullet \rVert} \right ) \\
  &amp;\approx 0.1
\end{align}
\qquad(6.62)\]</span></span></p>
<p>In linear algebra the expression for <span class="math inline">\(r_{m, s}\)</span> is defined to be the cosine of the angle between vectors <span class="math inline">\(\dot{m}_\bullet\)</span> and <span class="math inline">\(\dot{s}_\bullet\)</span>.</p>
<p>Therefore the two axes are shown with the angle, say <span class="math inline">\(\theta_{m, s}\)</span>, between the two <em>drawn</em> axes equal to the angle between the two <em>actual</em> vectors, <span class="math inline">\(\dot{m}_\bullet\)</span> and <span class="math inline">\(\dot{s}_\bullet\)</span>. Thus <span class="math inline">\(\cos(\theta_{m, s}) = r_{m, s}\)</span>. Since the correlation coefficient is positive rather than zero, the cosine is also positive, which implies that <span class="math inline">\(\theta_{m, s} &lt; \pi / 2\)</span>.</p>
<p><a href="#fig-col-sm-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.6</span></a> is a column-based view of the linear regression of the centered heights of the sons <span class="math inline">\((\dot{s}_\bullet)\)</span> on the centered heights of their mothers <span class="math inline">\((\dot{m}_\bullet)\)</span>. The figure illustrates the simplicity of linear least-squares regression as, in essence, an orthogonal projection.</p>
<p><a href="#fig-rc-sm-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.7</span></a> below compares the more commonly used (row-based) illustration of the same linear regression.</p>
<div id="fig-rc-sm-smpl-ctr" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rc-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rc-sm-smpl-ctr" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-Row-sm-smpl-ctr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-Row-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/g_row_sm_smpl_ctr.png" class="img-fluid figure-img" data-ref-parent="fig-rc-sm-smpl-ctr">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-Row-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Row
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rc-sm-smpl-ctr" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-Col-sm-smpl-ctr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-Col-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/g_col_sm_smpl_ctr.png" class="img-fluid figure-img" data-ref-parent="fig-rc-sm-smpl-ctr">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-Col-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Col
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rc-sm-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: Centered heights (son ~ mother): Row and Column Views
</figcaption>
</figure>
</div>
<p>The two perspectives on linear regression are complementary. <a href="#fig-Row-sm-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.7 (a)</span></a> portrays individual rows of data (families here). This is the most common means of visualizing data in two dimensions, and with good reason: it can be very thought-provoking and thus useful for refining models. This view of the regression problem shows model <em>results</em>. On the other hand, the column-based perspective shown in <a href="#fig-Col-sm-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.7 (b)</span></a> can help one to understand the model-fitting <em>process</em>.</p>
<p>Now let’s see how these ideas carry over from 2D to 3D: we now regress <span class="math inline">\(\dot{s}\)</span> on <span class="math inline">\((\dot{m}, \dot{f})\)</span>. <a href="#fig-col-smf-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.8</span></a> below shows this regression as an orthogonal projection of the <span class="math inline">\(\dot{s}_\bullet\)</span> basis vector to the plane defined by the <span class="math inline">\((\dot{m}_\bullet, \dot{f}_\bullet)\)</span> basis vectors. In this <span class="math inline">\((\dot{m}, \dot{f}, \dot{s})\)</span> coordinate system, the coordinates of the predicted (that is, projected) vector <span class="math inline">\(\hat{\dot{s}}_\bullet\)</span> are <span class="math inline">\((\hat{\beta}_1, \hat{\beta}_2, 0)\)</span>. The vector of residuals, <span class="math inline">\(\dot{s}_\bullet - \hat{\dot{s}}_\bullet\)</span>, is represented by the dotted line orthogonal to the <span class="math inline">\((\dot{m}, \dot{f})\)</span> plane.</p>
<p>Each of these vectors represents the 20 families in the sample, but those 20 vector elements are not visible from this column-based perspective. The details of those 20 families, or more generally of individual data cases, are shown in row-based perspectives, like <a href="#fig-row-smf-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.5</span></a>. Such details are important and of interest, of course. But the column-based perspective also merits our attention. It illustrates the geometry of the model-fitting process, and the angle <span class="math inline">\(\theta\)</span> between two axes corresponds to the correlation <span class="math inline">\(r\)</span> between the two variables <span class="math inline">\((\cos \theta = r)\)</span>. <a href="#fig-rc-smf-smpl-ctr" class="quarto-xref">Figure&nbsp;<span>6.9</span></a> compares the two perspectives.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-col-smf-smpl-ctr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-col-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="la-intro_files/figure-html/fig-col-smf-smpl-ctr-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-col-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: Projection of centered heights: son to (mother, father) plane
</figcaption>
</figure>
</div>
</div>
</div>
<div id="fig-rc-smf-smpl-ctr" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rc-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rc-smf-smpl-ctr" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-Row-smf-smpl-ctr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-Row-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/g_row_smf_smpl_ctr.png" class="img-fluid figure-img" data-ref-parent="fig-rc-smf-smpl-ctr">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-Row-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Row
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rc-smf-smpl-ctr" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-Col-smf-smpl-ctr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-Col-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/g_col_smf_smpl_ctr.png" class="img-fluid figure-img" data-ref-parent="fig-rc-smf-smpl-ctr">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-Col-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Col
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rc-smf-smpl-ctr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: Centered heights (son ~ mother + father): Row and Column Views
</figcaption>
</figure>
</div>
</section>
<section id="summary" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="summary"><span class="header-section-number">6.5</span> Summary</h2>
<p>This chapter treats linear regression from the perspective of linear algebra. The key points are:</p>
<ul>
<li><p><em>Row versus Column Views of the Data</em>: Viewing the data is good practice, and is usually done by representing individual cases of data, that is, rows of <span class="math inline">\((X, y)\)</span>, where <span class="math inline">\(X\)</span> is the feature matrix and <span class="math inline">\(y\)</span> is the response variable (target or labeling vector). A complementary perspective is to examine the relationship between <span class="math inline">\(y\)</span> and the columns of <span class="math inline">\(X\)</span>, the features, as <em>vectors</em>. Feature space is the subspace of <span class="math inline">\(n-\)</span>dimensional vector space that is spanned by the columns of <span class="math inline">\(X\)</span>, and is denoted <span class="math inline">\(col(X)\)</span>. Linear models are typically formulated from the column perspective, but the results of model-fitting are usually presented in a row perspective.</p></li>
<li><p><em>Least-Squares Linear Regression is an Orthogonal Projection</em>: The mapping of the response variable <span class="math inline">\(y\)</span> to its value <span class="math inline">\(\hat{y}\)</span> predicted by a fitted linear model is the orthogonal projection of vector <span class="math inline">\(y\)</span> to feature space, <span class="math inline">\(col(X)\)</span>.</p></li>
</ul>
</section>
<section id="exercises" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">6.6</span> Exercises</h2>
<section id="concepts" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="concepts"><span class="header-section-number">6.6.1</span> Concepts</h3>
<section id="row-vs-column-perspective" class="level4" data-number="6.6.1.1">
<h4 data-number="6.6.1.1" class="anchored" data-anchor-id="row-vs-column-perspective"><span class="header-section-number">6.6.1.1</span> Row vs Column Perspective</h4>
<p>Consider a dataset with <span class="math inline">\(n = 5\)</span> observations and <span class="math inline">\(d = 2\)</span> features plus a response variable.</p>
<ol type="a">
<li>Describe what “row space” means in the context of visualizing this data. What dimension is it?</li>
<li>Describe what “column space” or “feature space” means. What dimension is it?</li>
<li>In which space does the orthogonal projection for least-squares regression actually occur?</li>
</ol>
</section>
<section id="colx_bullet-bullet" class="level4" data-number="6.6.1.2">
<h4 data-number="6.6.1.2" class="anchored" data-anchor-id="colx_bullet-bullet"><span class="header-section-number">6.6.1.2</span> <span class="math inline">\(col(X_{\bullet, \bullet})\)</span></h4>
<p>Explain in your own words why the fitted values <span class="math inline">\(\hat{y}_\bullet\)</span> must lie in <span class="math inline">\(col(X_{\bullet, \bullet})\)</span>, the column space of the feature matrix.</p>
</section>
<section id="orthogonality" class="level4" data-number="6.6.1.3">
<h4 data-number="6.6.1.3" class="anchored" data-anchor-id="orthogonality"><span class="header-section-number">6.6.1.3</span> Orthogonality</h4>
<ol type="a">
<li>Explain why the residual vector <span class="math inline">\(\epsilon_\bullet = y_\bullet - \hat{y}_\bullet\)</span> is orthogonal to <span class="math inline">\(col(X_{\bullet, \bullet})\)</span>.</li>
<li>What does this orthogonality imply about the relationship between <span class="math inline">\(\epsilon_\bullet\)</span> and each column of <span class="math inline">\(X_{\bullet, \bullet}\)</span>?</li>
</ol>
</section>
<section id="centering-data" class="level4" data-number="6.6.1.4">
<h4 data-number="6.6.1.4" class="anchored" data-anchor-id="centering-data"><span class="header-section-number">6.6.1.4</span> Centering Data</h4>
<p>Explain why centering the feature vectors (subtracting the mean from each column of <span class="math inline">\(X_{\bullet, \bullet}\)</span>) eliminates the need for an intercept term in linear regression. What happens geometrically when data is centered?</p>
</section>
</section>
<section id="calculations" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="calculations"><span class="header-section-number">6.6.2</span> Calculations</h3>
<section id="d-projection" class="level4" data-number="6.6.2.1">
<h4 data-number="6.6.2.1" class="anchored" data-anchor-id="d-projection"><span class="header-section-number">6.6.2.1</span> 1D Projection</h4>
<p>Let: <span class="math inline">\(y_\bullet^\top = (3, 1, 2)\)</span> and <span class="math inline">\(X_{\bullet, \bullet}^\top = (x_{\bullet, 1})^\top = (1, 2, 2)\)</span>.</p>
<ol type="a">
<li><p>Find the orthogonal projection <span class="math inline">\(\hat{y}_\bullet\)</span> of <span class="math inline">\(y_\bullet\)</span> onto feature space <span class="math inline">\(col(X_{\bullet, \bullet})\)</span>.</p></li>
<li><p>Calculate the residual vector <span class="math inline">\(\epsilon_\bullet = y_\bullet - \hat{y}_\bullet\)</span>.</p></li>
<li><p>Verify that the residual vector is orthogonal to <span class="math inline">\(col(X_{\bullet, \bullet})\)</span>, that is, that <span class="math inline">\(\epsilon_\bullet \perp x_{\bullet, 1}\)</span>, by computing their inner product.</p></li>
<li><p>Sketch vectors <span class="math inline">\((y_\bullet, x_{\bullet, 1}, \hat{y}_\bullet, \epsilon_\bullet)\)</span> in 3D space.</p></li>
</ol>
</section>
<section id="distance-measures-1" class="level4" data-number="6.6.2.2">
<h4 data-number="6.6.2.2" class="anchored" data-anchor-id="distance-measures-1"><span class="header-section-number">6.6.2.2</span> Distance Measures</h4>
<p>For the vectors <span class="math inline">\(u_\bullet = (1,2,3)\)</span> and <span class="math inline">\(v_\bullet = (4, 1, 2)\)</span>:</p>
<ol type="a">
<li>Compute the Euclidean <span class="math inline">\((\mathcal{l}_2)\)</span> distance between <span class="math inline">\(u_\bullet\)</span> and <span class="math inline">\(v_\bullet\)</span>.</li>
<li>Compute the Manhattan <span class="math inline">\((\mathcal{l}_1)\)</span> distance.</li>
<li>Which distance measure is affected more by outliers? Why?</li>
</ol>
</section>
<section id="d-projection-1" class="level4" data-number="6.6.2.3">
<h4 data-number="6.6.2.3" class="anchored" data-anchor-id="d-projection-1"><span class="header-section-number">6.6.2.3</span> 2D Projection</h4>
<p>Let:</p>
<p><span id="eq-x-2D-projection"><span class="math display">\[
\begin{align}
y_\bullet &amp;=
  \begin{pmatrix}
    5 \\ 2 \\ 3 \\ 4
  \end{pmatrix} \\ \\
  &amp; \text{ and } \\ \\
X_{\bullet, \bullet} &amp;=
  \begin{pmatrix}
    1 &amp; 0 \\
    0 &amp; 1 \\
    1 &amp; 1 \\
    1 &amp; 0
  \end{pmatrix}
\end{align}
\qquad(6.63)\]</span></span></p>
<ol type="a">
<li><p>Find the linear least-squares vector <span class="math inline">\(\hat{\beta}_\bullet\)</span> of regression coefficients by solving the set of normal equations <span class="math inline">\(X_{\bullet, \bullet}^\top X_{\bullet, \bullet} \; \hat{\beta}_\bullet = X_{\bullet, \bullet}^\top \; y_\bullet\)</span>.</p></li>
<li><p>Calculate <span class="math inline">\(\hat{y} = X_{\bullet, \bullet} \; \hat{\beta}_\bullet\)</span>.</p></li>
<li><p>Verify that <span class="math inline">\(\hat{y}\)</span> is in <span class="math inline">\(col(X_{\bullet, \bullet})\)</span> by expressing it as a linear combination of the columns of <span class="math inline">\(X_{\bullet, \bullet}\)</span>.</p></li>
<li><p>Calculate <span class="math inline">\(\lVert \epsilon_\bullet \rVert^2\)</span>, where <span class="math inline">\(\epsilon_\bullet = y_\bullet - \hat{y}_\bullet\)</span>.</p></li>
</ol>
</section>
</section>
<section id="programming" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="programming"><span class="header-section-number">6.6.3</span> Programming</h3>
<section id="centering-heights" class="level4" data-number="6.6.3.1">
<h4 data-number="6.6.3.1" class="anchored" data-anchor-id="centering-heights"><span class="header-section-number">6.6.3.1</span> Centering Heights</h4>
<p>Using the Galton family heights data (<code>HistData::GaltonFamilies</code>):</p>
<ol type="a">
<li>Center the mother, father, and son height vectors by subtracting their means.</li>
<li>Fit a linear model predicting centered son height from centered mother and father heights (without an intercept term).</li>
<li>Compare the coefficients from part (b) to those from an uncentered model with intercept. What changed and what stayed the same?</li>
<li>Verify that the predictions from both models are identical.</li>
</ol>
</section>
<section id="visualizing-residuals" class="level4" data-number="6.6.3.2">
<h4 data-number="6.6.3.2" class="anchored" data-anchor-id="visualizing-residuals"><span class="header-section-number">6.6.3.2</span> Visualizing Residuals</h4>
<p>For the simple regression of son’s height on father’s height in the Galton data:</p>
<ol type="a">
<li>Create a scatter plot with the regression line.</li>
<li>Add vertical line segments showing the residuals for each observation.</li>
<li>Compute the sum of squared residuals.</li>
<li>Pick a different slope coefficient (not the least-squares value) and verify that it gives a larger sum of squared residuals.</li>
</ol>
</section>
<section id="visualizing-feature-space" class="level4" data-number="6.6.3.3">
<h4 data-number="6.6.3.3" class="anchored" data-anchor-id="visualizing-feature-space"><span class="header-section-number">6.6.3.3</span> Visualizing Feature Space</h4>
<p>Using centered Galton height data with two predictors (mother and father):</p>
<ol type="a">
<li><p>Create a 3D plot showing the mother, father, and son centered height vectors, <span class="math inline">\((\dot{m}_\bullet, \dot{f}_\bullet, \dot{s}_\bullet)\)</span>.</p></li>
<li><p>Add the vector of fitted values <span class="math inline">\(\hat{\dot{s}}_\bullet\)</span>.</p></li>
<li><p>Visually verify that the vector of fitted values lies in the plane spanned by the mother and father vectors.</p></li>
</ol>
</section>
</section>
<section id="advanced" class="level3" data-number="6.6.4">
<h3 data-number="6.6.4" class="anchored" data-anchor-id="advanced"><span class="header-section-number">6.6.4</span> Advanced</h3>
<section id="covariance-and-inner-product" class="level4" data-number="6.6.4.1">
<h4 data-number="6.6.4.1" class="anchored" data-anchor-id="covariance-and-inner-product"><span class="header-section-number">6.6.4.1</span> Covariance and Inner Product</h4>
<p>Consider the pair of <span class="math inline">\(n-\)</span>dimensional vectors <span class="math inline">\((v_\bullet, w_\bullet)\)</span> as a numeric data matrix of dimension <span class="math inline">\(n \times 2\)</span>. Recall the notation <span class="math inline">\(\bar{v}\)</span> for the arithmetic mean of <span class="math inline">\(v_\bullet\)</span>, and <span class="math inline">\(\dot{v}_\bullet = v_\bullet - \bar{v} \; 1_\bullet\)</span> for the centered version of <span class="math inline">\(v_\bullet\)</span>, so that <span class="math inline">\(\dot{v}_i = v_i - \bar{v}\)</span> for each element <span class="math inline">\(v_i\)</span> of <span class="math inline">\(v_\bullet\)</span>. Now the <em>sample covariance</em> <span class="math inline">\(cov(v_\bullet, w_\bullet)\)</span> is defined as follows:</p>
<p><span id="eq-sample-cov-defn"><span class="math display">\[
\begin{align}
  cov(v_\bullet, w_\bullet)
  &amp;= \frac{1}{n-1} \sum_{i = 1}^n (v_i - \bar{v}) \; (w_i - \bar{w})
\end{align}
\qquad(6.64)\]</span></span></p>
<ol type="a">
<li>Express the sample covariance using inner-product notation.</li>
<li>The <em>sample variance</em> is defined as <span class="math inline">\(var(v_\bullet) = cov(v_\bullet, v_\bullet)\)</span>. Express <span class="math inline">\(var(v_\bullet)\)</span> using norm notation.</li>
<li>The <em>sample standard</em> deviation is defined as <span class="math inline">\(sd(v_\bullet) = \sqrt{var(v_\bullet)}\)</span>. Express <span class="math inline">\(sd(v_\bullet)\)</span> using norm notation.</li>
<li>Use inner-product notation to express the <em>sample correlation</em> <span class="math inline">\(cor(v_\bullet, w_\bullet)\)</span>, defined as follows.</li>
</ol>
<p><span id="eq-sample-cor-defn"><span class="math display">\[
\begin{align}
  cor(v_\bullet, w_\bullet)
  &amp;= \frac{1}{n-1} \sum_{i = 1}^n \frac{v_i - \bar{v}}{sd(v_\bullet)} \; \frac{w_i - \bar{w}}{sd(w_\bullet)}
\end{align}
\qquad(6.65)\]</span></span></p>
</section>
<section id="correlation-geometry" class="level4" data-number="6.6.4.2">
<h4 data-number="6.6.4.2" class="anchored" data-anchor-id="correlation-geometry"><span class="header-section-number">6.6.4.2</span> Correlation Geometry</h4>
<p>When <span class="math inline">\(n-\)</span>dimensional vectors <span class="math inline">\(v_\bullet, w_\bullet\)</span> are centered <span class="math inline">\((v_\bullet \mapsto \dot{v}_\bullet)\)</span> and normalized to unit length <span class="math inline">\((\tilde{v}_\bullet = \dot{v}_\bullet / \Vert \dot{v}_\bullet \rVert)\)</span>, show that:</p>
<ol type="a">
<li>The inner product <span class="math inline">\(\tilde{v}_\bullet^\top \tilde{w}_\bullet\)</span> is equal to the correlation <span class="math inline">\(r\)</span> between features <span class="math inline">\(v_\bullet, w_\bullet\)</span> times <span class="math inline">\((n - 1)\)</span>.</li>
<li>The angle <span class="math inline">\(\theta\)</span> between <span class="math inline">\(v_\bullet, w_\bullet\)</span> satisfies <span class="math inline">\(\cos(\theta) = r\)</span>.</li>
<li>Apply this to the Galton height data: compute the angles between mother, father, and son height vectors.</li>
</ol>
</section>
<section id="correlated-features" class="level4" data-number="6.6.4.3">
<h4 data-number="6.6.4.3" class="anchored" data-anchor-id="correlated-features"><span class="header-section-number">6.6.4.3</span> Correlated Features</h4>
<ol type="a">
<li>Generate synthetic data <span class="math inline">\((y_\bullet, x_{\bullet, 1}, x_{\bullet, 2})\)</span> such that the two features <span class="math inline">\((x_{\bullet, 1}, x_{\bullet, 2})\)</span> are orthogonal (correlation = 0). Then fit a regression model.</li>
<li>Now generate similar data, but this time with the two features highly correlated (correlation &gt; 0.9). Once again, fit a regression model.</li>
<li>Compare the standard errors of the coefficient estimates. Why are they different?</li>
<li>Visualize the column spaces in both cases. How does the geometry explain the difference in precision?</li>
</ol>
</section>
<section id="sum-of-squares-decomposition" class="level4" data-number="6.6.4.4">
<h4 data-number="6.6.4.4" class="anchored" data-anchor-id="sum-of-squares-decomposition"><span class="header-section-number">6.6.4.4</span> Sum of Squares Decomposition</h4>
<p>For any linear regression:</p>
<ol type="a">
<li>Show algebraically that for centered data <span class="math inline">\(\lVert \dot{y}_\bullet \rVert^2 = \lVert \hat{\dot{y}}_\bullet \rVert^2 + \lVert \dot{\epsilon}_\bullet \rVert^2\)</span>.</li>
<li>Explain this geometrically using the Pythagorean theorem in <span class="math inline">\(n-\)</span>dimensional space.</li>
<li>Verify this numerically with the Galton heights data.</li>
<li>Comment on the following characterization: (total variation) = (explained variation) + (unexplained variation).</li>
</ol>
</section>
<section id="projection-matrix" class="level4" data-number="6.6.4.5">
<h4 data-number="6.6.4.5" class="anchored" data-anchor-id="projection-matrix"><span class="header-section-number">6.6.4.5</span> Projection Matrix</h4>
<p>The projection matrix <span class="math inline">\(P\)</span> is defined by <a href="#eq-lin-reg-projection-mat" class="quarto-xref">Equation&nbsp;<span>6.50</span></a>.</p>
<ol type="a">
<li>Show that <span class="math inline">\(P\)</span> is symmetric: <span class="math inline">\(P^\top = P\)</span>.</li>
<li>Show that <span class="math inline">\(P\)</span> is idempotent: <span class="math inline">\(P^2 = P\)</span>.</li>
<li>What does the idempotent property mean geometrically?</li>
<li>Compute <span class="math inline">\(P\)</span> for a small <span class="math inline">\((n = 5)\)</span> subset of the Galton heights data and verify these properties numerically.</li>
</ol>
</section>
<section id="feature-basis-vectors" class="level4" data-number="6.6.4.6">
<h4 data-number="6.6.4.6" class="anchored" data-anchor-id="feature-basis-vectors"><span class="header-section-number">6.6.4.6</span> Feature Basis Vectors</h4>
<p>Using the Galton heights centered data:</p>
<ol type="a">
<li>The regression coefficients <span class="math inline">\((\hat{\beta}_m, \hat{\beta}_f)\)</span> represent coordinates in the non-orthogonal basis defined by the mother and father vectors <span class="math inline">\((\dot{m}_\bullet, \dot{f}_\bullet)\)</span>. Express the fitted values <span class="math inline">\(\hat{\dot{s}}_\bullet\)</span> in this basis.</li>
<li>Transform to an orthogonal basis using the Gram-Schmidt process.</li>
<li>Express <span class="math inline">\(\hat{\dot{s}}_\bullet\)</span> in the orthogonal basis.</li>
<li>Verify that the projection is the same in both coordinate systems.</li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-wiki_categorical_variable" class="csl-entry" role="listitem">
<span>“Categorical Variable | Wikipedia.”</span> 2025. <em>Wikipedia</em>, October. <a href="https://en.wikipedia.org/wiki/Categorical_variable">https://en.wikipedia.org/wiki/Categorical_variable</a>.
</div>
<div id="ref-wiki_Hamming_distance" class="csl-entry" role="listitem">
<span>“Hamming Distance | Wikipedia.”</span> 2025. <em>Wikipedia</em>, October. <a href="https://en.wikipedia.org/wiki/Hamming_distance">https://en.wikipedia.org/wiki/Hamming_distance</a>.
</div>
<div id="ref-LeCun_Cortes_Burges_2005" class="csl-entry" role="listitem">
LeCun, Yann, Corinna Cortes, and Christopher J. C. Burges. 2005. <span>“The MNIST Database of Handwritten Digits.”</span> <a href="https://web.archive.org/web/20200430193701/http://yann.lecun.com/exdb/mnist/">https://web.archive.org/web/20200430193701/http://yann.lecun.com/exdb/mnist/</a>.
</div>
<div id="ref-wiki_Levenshtein_distance" class="csl-entry" role="listitem">
<span>“Levenshtein Distance | Wikipedia.”</span> 2025. <em>Wikipedia</em>, October. <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">https://en.wikipedia.org/wiki/Levenshtein_distance</a>.
</div>
<div id="ref-wiki_MNIST" class="csl-entry" role="listitem">
<span>“MNIST Database | Wikipedia.”</span> 2025. <em>Wikipedia</em>, October. <a href="https://en.wikipedia.org/wiki/MNIST_database">https://en.wikipedia.org/wiki/MNIST_database</a>.
</div>
<div id="ref-wiki_multinomial_regression" class="csl-entry" role="listitem">
<span>“Multinomial Logistic Regression | Wikipedia.”</span> 2025. <em>Wikipedia</em>, October. <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">https://en.wikipedia.org/wiki/Multinomial_logistic_regression</a>.
</div>
<div id="ref-wiki_one-hot" class="csl-entry" role="listitem">
<span>“One-Hot | Wikipedia.”</span> 2025. <em>Wikipedia</em>, October. <a href="https://en.wikipedia.org/wiki/One-hot">https://en.wikipedia.org/wiki/One-hot</a>.
</div>
<div id="ref-wiki_raster" class="csl-entry" role="listitem">
<span>“Raster Graphics | Wikipedia.”</span> 2025. <em>Wikipedia</em>, October. <a href="https://en.wikipedia.org/wiki/Raster_graphics">https://en.wikipedia.org/wiki/Raster_graphics</a>.
</div>
</div>
</section>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>To be more precise, every <em>data variable</em> qualifies as a <em>feature vector</em>, but a feature vector may also be some function of the data and other information.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>“Best” in the sense of minimizing the sum of squared residuals of actual minus predicted sons’ heights.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The OECD (Organisation for Economic Co-operation and Development) works with 100+ countries to collect and analyze data in order to promote public policy. The OECD’s 38 Member countries span the world, from North America and South America to Europe and Asia-Pacific.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See <span class="citation" data-cites="LeCun_Cortes_Burges_2005">LeCun, Cortes, and Burges (<a href="references.html#ref-LeCun_Cortes_Burges_2005" role="doc-biblioref">2005</a>)</span> and <span class="citation" data-cites="wiki_MNIST"><span>“MNIST Database | Wikipedia”</span> (<a href="references.html#ref-wiki_MNIST" role="doc-biblioref">2025</a>)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>See <span class="citation" data-cites="wiki_multinomial_regression"><span>“Multinomial Logistic Regression | Wikipedia”</span> (<a href="references.html#ref-wiki_multinomial_regression" role="doc-biblioref">2025</a>)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The conversion of a matrix of pixels to a vector of pixels is known as raster-to-vector (R2V) conversion, usually in row-major format, whereby the elements of the vector are taken from the top row and then from each succeeding row. See <span class="citation" data-cites="wiki_raster"><span>“Raster Graphics | Wikipedia”</span> (<a href="references.html#ref-wiki_raster" role="doc-biblioref">2025</a>)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>See <span class="citation" data-cites="wiki_one-hot"><span>“One-Hot | Wikipedia”</span> (<a href="references.html#ref-wiki_one-hot" role="doc-biblioref">2025</a>)</span> and <span class="citation" data-cites="wiki_categorical_variable"><span>“Categorical Variable | Wikipedia”</span> (<a href="references.html#ref-wiki_categorical_variable" role="doc-biblioref">2025</a>)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>The inner product of vectors <span class="math inline">\(x, y \in \mathcal{V}\)</span> has alternative notations, including <span class="math inline">\(x \boldsymbol\cdot y\)</span>, <span class="math inline">\(\left &lt; x, y \right &gt;\)</span>, and <span class="math inline">\(x^\top y\)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>We assume in this chapter that feature matrix <span class="math inline">\(X\)</span> has only numeric columns, possibly as the result of selecting from and transforming a larger collection of numeric and non-numeric features.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Matrix <span class="math inline">\(X\)</span> on the right side of <a href="#eq-generic-lm" class="quarto-xref">Equation&nbsp;<span>6.19</span></a> is called a <em>feature matrix</em> that may contain original data columns (other than the response or labeling variable) and may also contain columns that are functions of the data or of other information. A data matrix is model-agnostic, whereas a feature matrix is constructed to support a model of some form.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>There are various notations for the subspace spanned by the columns of matrix <span class="math inline">\(X_{\bullet, \bullet}\)</span> including <span class="math inline">\(\mathcal{C}(X_{\bullet, \bullet})\)</span>, <span class="math inline">\(\mathcal{Im}(X_{\bullet, \bullet})\)</span> (for the <em>image</em> of <span class="math inline">\(X_{\bullet, \bullet}\)</span>), and <span class="math inline">\(\mathcal{R}(X_{\bullet, \bullet})\)</span> (for the <em>range</em> of <span class="math inline">\(X_{\bullet, \bullet}\)</span>).<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>See <span class="citation" data-cites="wiki_Hamming_distance"><span>“Hamming Distance | Wikipedia”</span> (<a href="references.html#ref-wiki_Hamming_distance" role="doc-biblioref">2025</a>)</span> and <span class="citation" data-cites="wiki_Levenshtein_distance"><span>“Levenshtein Distance | Wikipedia”</span> (<a href="references.html#ref-wiki_Levenshtein_distance" role="doc-biblioref">2025</a>)</span>.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>The residual son’s height is the error term in the regression formula. In the figure, residuals are color-coded according to their sign: black if positive and red otherwise.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>The regression plane qualifies as a linear manifold, mathematically speaking.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>The cited value of the correlation coefficient <span class="math inline">\(r_{m, s}\)</span> pertains to the random sample of size 20 created to facilitate a detailed view of regression residuals. Among all 179 families whose oldest child was a son, we have <span class="math inline">\(r_{m, s} \approx 0.3\)</span>.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/github\.com\/tthrall\/eda4ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./study-design.html" class="pagination-link" aria-label="Sampling and Study Design">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sampling and Study Design</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./reduce-dim.html" class="pagination-link" aria-label="Dimension Reduction">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Dimension Reduction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/tthrall/eda4ml/edit/main/la-intro.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/tthrall/eda4ml/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>