[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDA for Machine Learning",
    "section": "",
    "text": "Welcome\nThis is the website for the Quarto book “EDA for ML” (eda4ml), which presents exploratory data analysis (EDA) and related topics in order to prepare you to learn more about machine learning (ML).\nThis website is free, licensed under the MIT License. You are invited to copy and change the content to create your own version of this course.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "16  Summary",
    "section": "",
    "text": "16.1 Review of Selected Topics",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "summary.html#review-of-selected-topics",
    "href": "summary.html#review-of-selected-topics",
    "title": "16  Summary",
    "section": "",
    "text": "16.1.1 Introduction\nWe first review the course material in order to prepare you (should you choose) to present this material going forward. The primary reference is eda_v3:\n\nNotes on Exploratory Data Analysis (EDA)\n\nMATH4350 version 3\nby Karen Trageser\n\n\nThe subsequent material for the course is based on the following ml reference:\n\nMachine Learning: a Concise Introduction\n\nby Steven W. Knox\nnext edition pending publication by Wiley\n(Course notes: ML Topics & Techniques, version 2.93)\n\n\nWe first compare the eda_v3 content to the selection of last week’s topics. We’ll then form small teams to select topics for discussion in the remainder of the session.\nThen we prepare to you to teach the course by discussing content and ideas to be emphasized, intended participants, desired learning outcomes, and instructor preparation.\n\n\n16.1.2 Overview of Topics\nHere are the chapter titles of eda_v3.\n\n\n\n\nTable 16.1: EDA v3 chapters\n\n\n\n\nEDA v3 chapters\n\n\nchpt\ntitle\n\n\n\n\n1\nIntroduction\n\n\n2\nExploratory Data Analysis (EDA)\n\n\n3\nUnsupervised Learning\n\n\n4\nSome Linear Algebra\n\n\n5\nDimension Reduction\n\n\n6\nTopic Modeling\n\n\n7\nSampling\n\n\n8\nTime Series\n\n\n9\nApp-A: Probability Review\n\n\n10\nApp-B: SVD Notes by Carla Martin\n\n\n\n\n\n\n\n\nAnd here are the topics we discussed last week.\n\n\n\n\nTable 16.2: Topics discussed Jan 13-17\n\n\n\n\nTopics discussed Jan 13-17\n\n\ndate\ntime\ntopic\n\n\n\n\n2025-01-13\nAM\nExploratory Data Analysis\n\n\n2025-01-13\nPM\nConditional Distributions\n\n\n2025-01-14\nAM\nClustering: EDA in Higher Dimensions\n\n\n2025-01-14\nPM\nText Analysis\n\n\n2025-01-15\nAM\nSampling and Study Design\n\n\n2025-01-15\nPM\nLinear Algebra\n\n\n2025-01-16\nAM\nDimension Reduction\n\n\n2025-01-17\nAM\nTime Series\n\n\n2025-01-17\nPM\nTime Series & Point Processes: Frequency Analysis\n\n\n\n\n\n\n\n\n\n\n16.1.3 Class Exercise: Select Topics for Further Discussion\nWhich topics presented last week merit further discussion? Which topics from eda_v3 would you like to know more about? Form a team of 2-4 classmates, and take 10 minutes to record the 1-2 topics of greatest interest to you. We’ll then use the remainder of the session to discuss them and follow up with write-ups.\n\n\n16.1.4 Topics Nominated for Further Discussion\nHere are the topics nominated for further discussion by one or more teams.\n\n\n\n\nTable 16.3: Topics Nominated by Teams\n\n\n\n\nTopics Nominated by Teams\n\n\ntopic\nn_teams\n\n\n\n\nTime Series Forecasting\n2\n\n\nLDA: Latent Dirichlet Allocation\n2\n\n\nSVD:Singular Value Decomposition\n1\n\n\nKDE: Kernel Density Estimation\n1\n\n\nClustering\n1\n\n\nPCA v MDS v SNE: when to use?\n1",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "summary.html#preparing-to-teach-part-1",
    "href": "summary.html#preparing-to-teach-part-1",
    "title": "16  Summary",
    "section": "16.2 Preparing to Teach Part 1",
    "text": "16.2 Preparing to Teach Part 1\nHere are the class exercises from our last session concerning the EDA content.\n\n\n\n\nTable 16.4: Part 1 Review Class Exercises\n\n\n\n\nPart 1 Review: Class Exercises\n\n\nheading\nprep (minutes)\n\n\n\n\nClarification Needed\n10\n\n\nSelection of Topics\n15\n\n\nIntended Participants\n10\n\n\nLearning Outcomes\n15\n\n\nTeaching Style\n10\n\n\n\n\n\n\n\n\n\n16.2.1 Primary References for the Course\nAs noted in the previous session, the primary reference for Part 1 of MATH4350 is eda_v3:\n\nNotes on Exploratory Data Analysis (EDA)\n\nMATH4350 version 3\nby Karen Trageser\n\n\nThe subsequent material for the course is based on the following ml reference:\n\nMachine Learning: a Concise Introduction\n\nby Steven W. Knox\nnext edition pending publication by Wiley\n(Course notes: ML Topics & Techniques, version 2.93)\n\n\n\n\n16.2.2 Topics\nThe chapter titles of eda_v3 are given in Table 16.1.\nThese chapters represent a broad range of topics to be covered in just a few days. Each chapter can easily take up a one-semester course at the undergraduate or graduate level. Consequently we must select and organize topics very carefully to equip participants with essential tools and prepare them for the remainder of the course\nThe topics we discussed last week are shown in Table 16.2.\n\n\n16.2.3 Class Exercises\nFor each of the following talking points, form a small group with one or two classmates, and take 10-15 minutes to prepare to report your thoughts to the class.\n\n\n16.2.4 Selection of Topics\nAs you prepare to teach MATH4350, which topics and key points from eda_v3 would you emphasize? Based on your experience with the material discussed last week, how would you organize classroom sessions? What guidance would you offer a friend preparing to teach this material? Take 15 minutes to prepare to report out to the class.\n\n16.2.4.1 Intended Participants\nLast week several of us noted that the EDA content was presented largely from a mathematical viewpoint, prompting us to consider whether and how to engage those potential participants whose expertise was more computational than mathematical. Take 10 minutes to report out to the class on the questions below, or on related questions of your own.\n\nThe US version of the course lays out pre-requisites in mathematics, statistics, and computation, citing established textbooks.\nFor the UK version of the course, should those pre-requisites be modified? How?\nHow should the readiness of a potential participant be assessed?\nWhen should a potential participant be advised to shore up needed pre-requisites on their own? What guidance should we give on courses or other learning resources? Are high-side refresher courses feasible?\n\n\n\n16.2.4.2 Learning Outcomes\nFrom last week’s discussions, we seem to be agreed that the goal of the EDA material should be to equip participants with tools they can (responsibly) use in their work, either immediately or eventually. Take 15 minutes to consider the questions below and then report to the class how you would express your goals for participant learning of the EDA content.\n\nShould this first week of content also be used to level-set, to ensure that participants share some common knowledge and vocabulary needed for the remainder of the course? If so, how should we define that baseline?\nAs an instructor, should you attempt to detect gaps in needed participant knowledge? How would you go about that? How would you address such gaps?\nResponsible use of data science methods requires knowledge of the capabilities and limitations of the methods, and of one’s current level of proficiency in a wide-ranging set of competencies. How would you help participants get a clear picture of both?\nHow would you summarize your goals as an instructor for participants with respect to the EDA content?\n\n\n\n16.2.4.3 Instructor Preparation\nEach of us goes about learning a new area in our own distinct manner. As instructors we naturally emphasize what we know or enjoy most, and tend to present content in a way that we would want that content presented to us. How should we take advantage of instructor strengths while maintaining some degree of consistency in content and delivery? Take 10 minutes to consider the following questions and then report out to the class.\n\nIn the US eda_v3 followed by mlci are the reference materials on which MATH4350 is based. Other instructors contribute content on specific topics, thereby enabling each instructor to assemble content as seems best. Is this a model for the UK version of the course? Where and how should the content be maintained?\nIn your experience as an instructor, have you ever encountered a course participant who seemed to learn differently from the way you learn?\nAs a community of instructors, how should we share our successes and challenges?\nHow, if at all, should we vet potential instructors?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "“Adjacency Matrix | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Adjacency_matrix.\n\n\nAnderson, T. W., and Ingram Olkin. 2002. “Maximum-Likelihood\nEstimation of the Parameters of a Multivariate Normal\nDistribution.” Linear Algebra and Its Applications, May.\nhttps://www.sciencedirect.com/science/article/pii/0024379585900497.\n\n\n“Animal Farm | Wikipedia.” 2025. Wikipedia, July.\nhttps://en.wikipedia.org/wiki/Animal_Farm.\n\n\nAntoniak, Maria. 2023. “Topic Modeling for the People.”\nMaria Antoniak. University of Colorado Boulder. https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html.\n\n\n“Betweenness Centrality | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Betweenness_centrality.\n\n\n“Brandes’ Algorithm | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Brandes%27_algorithm.\n\n\n“Categorical Variable | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Categorical_variable.\n\n\nClauset, Aaron, M. E. J. Newman, and Cristopher Moore. 2004.\n“Finding Community Structure in Very Large Networks.”\nPhysical Review E. https://doi.org/https://doi.org/10.1103/PhysRevE.70.066111.\n\n\n“Component (Graph Theory) | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Component_(graph_theory).\n\n\n“Degree Matrix | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Degree_matrix.\n\n\n“Dijkstra’s Algorithm | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm.\n\n\n“Dirichlet Distribution | Wikipedia.” 2025.\nWikipedia, July. https://en.wikipedia.org/wiki/Dirichlet_distribution.\n\n\n“Floyd–Warshall Algorithm | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm.\n\n\nFortunato, Santo. 2010. “Community Detection in Graphs.”\nPhysics Reports, February. https://doi.org/10.1016/j.physrep.2009.11.002.\n\n\n“Girvan–Newman Algorithm | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm.\n\n\n“Glossary of Graph Theory | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Glossary_of_graph_theory.\n\n\n“Graph Theory | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Graph_theory.\n\n\n“Laplacian Matrix | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Laplacian_matrix.\n\n\nLeCun, Yann, Corinna Cortes, and Christopher J. C. Burges. 2005.\n“The MNIST Database of Handwritten Digits.” https://web.archive.org/web/20200430193701/http://yann.lecun.com/exdb/mnist/.\n\n\n“Leiden Algorithm | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Leiden_algorithm.\n\n\n“Louvain Method | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Louvain_method.\n\n\n“Markov Chain | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Markov_chain.\n\n\n“MNIST Database | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/MNIST_database.\n\n\n“Modularity (Networks) | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Modularity_(networks).\n\n\n“Multinomial Logistic Regression | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Multinomial_logistic_regression.\n\n\n“One-Hot | Wikipedia.” 2025. Wikipedia, October.\nhttps://en.wikipedia.org/wiki/One-hot.\n\n\n“PageRank | Wikipedia.” 2025. Wikipedia, October.\nhttps://en.wikipedia.org/wiki/PageRank.\n\n\n“Raster Graphics | Wikipedia.” 2025. Wikipedia,\nOctober. https://en.wikipedia.org/wiki/Raster_graphics.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy\nApproach. O’Reilly Media. https://www.tidytextmining.com/.\n\n\n“Spectral Clustering | Wikipedia.” 2025.\nWikipedia, October. https://en.wikipedia.org/wiki/Spectral_clustering.\n\n\nSteyvers, Mark, and Tom Griffiths. 2007. Probabilistic Topic\nModels. https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf.\n\n\n“The Butter Battle Book | Wikipedia.” 2025.\nWikipedia, July. https://en.wikipedia.org/wiki/The_Butter_Battle_Book.\n\n\nTokdar, Surya T., and Robert E. Kass. 2010. “Importance Sampling:\nA Review.” WIREs Computational Statistics 2 (1): 54–60.\nhttps://doi.org/https://doi.org/10.1002/wics.56.\n\n\nVenables, W. N., and B. D. Ripley. 2002. “Modern Applied\nStatistics with s.” https://www.stats.ox.ac.uk/pub/MASS4/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "1  Exploratory Data Analysis",
    "section": "",
    "text": "1.1 Heights of Fathers and Sons\nIn 1885 Sir Francis Galton examined the heights of parents and their children to determine the strength of evidence to support height as a hereditary trait. Galton’s subsequent protege, Karl Pearson (who went on in 1911 to form the world’s first statistics department at University College London) followed up with an analysis of the heights of fathers and sons.\nFigure 1.1: Heights in inches of (father, son) pairs\nIn the scatter diagram above, each point represents the respective heights of a father-son pair. The density of points in the middle range of heights obscures individual points.\nHere is another view of the same data obtained by grouping father heights into successive intervals (each one labeled by its mid-point) and then constructing a box-plot of sons’ heights within each interval.\nFigure 1.2: Range of son’s height for given height of father\nTable 1.1: Heights of sons grouped by father’s height\n\n\n\n# A tibble: 9 × 6\n# Groups:   f_ivl [9]\n  f_ivl   f_mpt s_count s_min s_max s_avg\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 (58,60]    59       4  63.9  65.2  64.7\n2 (60,62]    61      16  60.8  69.1  65.5\n3 (62,64]    63      77  58.5  74.3  66.3\n4 (64,66]    65     208  59.8  74.7  67.5\n5 (66,68]    67     276  59.8  75.7  68.2\n6 (68,70]    69     275  62.2  78.4  69.5\n7 (70,72]    71     152  61.2  78.2  70.2\n8 (72,74]    73      63  66.7  77.2  71.4\n9 (74,76]    75       7  69.0  74.3  71.6\nGalton noted that in general sons were taller than fathers, with extremely tall or short fathers corresponding to not quite so extremely tall or short sons, respectively. This observation led Galton to coin the term “regression toward mediocrity”, or (more politely) “regression toward the mean”.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#class-exercise-diamond-data",
    "href": "eda.html#class-exercise-diamond-data",
    "title": "1  Exploratory Data Analysis",
    "section": "1.2 Class Exercise: Diamond Data",
    "text": "1.2 Class Exercise: Diamond Data\nTeam up with a classmate and load the diamond data provided by R package ggplot2. How many rows of data are there? How many columns? What questions occur to you about the data? How might you address those questions? Take 15 minutes to prepare to report out to the class.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#class-exercise-general-social-survey-data",
    "href": "eda.html#class-exercise-general-social-survey-data",
    "title": "1  Exploratory Data Analysis",
    "section": "1.3 Class Exercise: General Social Survey Data",
    "text": "1.3 Class Exercise: General Social Survey Data\nThe General Social Survey is a long-running US survey conducted by the independent research organization NORC at the University of Chicago. The survey consists of thousands of questions designed to monitor changes in social characteristics and attitudes in the US. In R package forcats, Hadley Wickham formed the data set gss_cat consisting of just a few columns to illustrate the challenges one encounters when working with categorical (non-numeric) variables (“factors”).\nTeam up with a classmate and load the gss_cat data-set provided by R package forcats. How many rows of data are there? How many columns? What questions occur to you about the data? How might you address those questions? Take 15 minutes to prepare to report out to the class.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#discussion-what-should-eda-mean",
    "href": "eda.html#discussion-what-should-eda-mean",
    "title": "1  Exploratory Data Analysis",
    "section": "1.4 Discussion: what should EDA mean?",
    "text": "1.4 Discussion: what should EDA mean?\nExploratory Data Analysis (EDA) is an approach to data analysis advocated by John Tukey, a leading American statistician of the 20th century. The approach contrasts with what Tukey called “confirmatory analysis”, that is, a focus on probability models of data-generation along with the estimation or testing of model parameters. The difference is one of emphasis: EDA includes models suggested by data, but with an emphasis on understanding current and potential data sets.\nThe exploration is led by one’s questions about the data. Relevant questions may or may not be obvious (or given). Variables may or may not be readily categorized as “response variables” versus “predictor variables”. The ability to develop and recognize relevant questions is an important skill largely gained through experience.\nImportant EDA outcomes include\n\nthe discovery of unanticipated data patterns, and\nproposals to examine tentative answers suggested by the current data, perhaps using a new data set designed for this purpose.\n\nEDA methods are used within the context of confirmatory analysis to examine the data for errors not encompassed by the models under study (e.g., errors in data transcription or transmission), or to search for other departures from model assumptions.\nEDA methods can be broadly understood as the methods of descriptive statistics: data summaries (graphical or tabular) intended to enhance our understanding of the data. EDA differs from descriptive statistics in a reliance on the questions of the data analyst and a readiness to examine various transformations of the data.\nAs an example, here are some ways we might address the question of how, if at all, the heights of fathers and sons differ in the data presented above.\n\n\n\n\n\n\n\n\nFigure 1.3: Overall, the sons are taller\n\n\n\n\n\n\n\n\nTable 1.2: Height distribution: fathers and sons\n\n\n\n# A tibble: 2 × 5\n  fs       min   mid   avg   max\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 father  59.0  67.8  67.7  75.4\n2 son     58.5  68.6  68.7  78.4\n\n\n\n\nThe box-plot and table above show that, on average, sons are about an inch taller than fathers. Here’s a histogram of the difference in heights (son minus father) across father-son pairs.\n\n\n\n\n\n\n\n\nFigure 1.4: Histogram of son’s height minus father’s height\n\n\n\n\n\n\n\n\nTable 1.3: Summary stats: son’s height minus father’s height\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   -9.0    -0.9     1.0     1.0     2.7    11.2 \n\n\n\n\nThe summary of individual differences in height (son minus father) strengthens the previous aggregate summaries: the distribution of son’s height minus father’s height is fairly symmetric around a difference of about one inch.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#team-exercises",
    "href": "eda.html#team-exercises",
    "title": "1  Exploratory Data Analysis",
    "section": "1.5 Team Exercises",
    "text": "1.5 Team Exercises\n\nResponse versus predictor variables: for each of the data sets presented above, propose one or more variables as response variables. How, if at all, might someone else argue for a different choice? What should we mean by “response” and “predictor” variables? Describe a situation in which this distinction would not be suitable.\nRegression to the mean: the discussion of father-son heights uses the phrase “extremely tall or short fathers corresponding to not quite so extremely tall or short sons, respectively”. How would you formulate the meaning of that phrase? Do the data demonstrate this phenomenon?\nDiamond data: propose a question for the diamond data, and then try to address that question. What (if anything) did you learn from this task?\nSurvey data: propose a question for the gss_cat data, and then try to address that question. What (if anything) did you learn from this task?",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#resources",
    "href": "eda.html#resources",
    "title": "1  Exploratory Data Analysis",
    "section": "1.6 Resources",
    "text": "1.6 Resources\nHands-On Programming in R, by Garrett Grolemund\nsimpleR Using R for Introductory Statistics., by John Verzani\nR for Data Science (2e), by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\nRPubs - Diamonds Dataset\nNORC’s General Social Survey (GSS)\nEDA: Exploratory data analysis - Wikipedia",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "conditioning.html",
    "href": "conditioning.html",
    "title": "2  Conditional Distributions",
    "section": "",
    "text": "2.1 Heights of Fathers and Sons\nThe box-plot below can be regarded as the (sample) conditional distribution of sons’ heights grouped by the height (rounded to the nearest odd inch) of each son’s father.\nFigure 2.1: Range of son heights for given height of father\nTable 2.1: Son stats per father’s height\n\n\n\n# A tibble: 9 × 7\n  f_ivl   f_mpt s_count s_min s_mid s_max s_avg\n  &lt;fct&gt;   &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 (58,60]    59       4  63.9  64.9  65.2  64.7\n2 (60,62]    61      16  60.8  65.6  69.1  65.5\n3 (62,64]    63      77  58.5  66.5  74.3  66.3\n4 (64,66]    65     208  59.8  67.4  74.7  67.5\n5 (66,68]    67     276  59.8  68.2  75.7  68.2\n6 (68,70]    69     275  62.2  69.2  78.4  69.5\n7 (70,72]    71     152  61.2  70.1  78.2  70.2\n8 (72,74]    73      63  66.7  71.3  77.2  71.4\n9 (74,76]    75       7  69.0  71.4  74.3  71.6\nThe last column in the table above is the sample average of the son’s height given the father’s height, which we take as an estimate of the population average of the son’s height given the father’s height, that is, the conditional expectation of son’s height given father’s height.\nThe figure below represents these sample conditional averages per father’s height as diamonds, whose area is roughly proportional to the number of sons in each group. The figure includes a reference line showing the father’s (midpoint) height plus 1 inch, corresponding to our previous calculation of an average son-minus father difference.\nFigure 2.2: Average of son heights per father’s height\nThe above graph of average son-height per father’s height forms an approximate straight line, although the slope of the line is less than 1 (which is the slope of the reference line).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#z-scores-transforming-data-values-to-standard-units",
    "href": "conditioning.html#z-scores-transforming-data-values-to-standard-units",
    "title": "2  Conditional Distributions",
    "section": "2.2 Z-Scores: transforming data values to standard units",
    "text": "2.2 Z-Scores: transforming data values to standard units\nImagine choosing a father-son pair at random from the entire population and measuring their respective heights. This would be an example of a pair of random variables \\((X, Y)\\). If we happened to know the average and standard deviation of (father, son) heights, respectively, from the entire population, we could convert the given heights to so-called standard units, or z-scores, as follows.\n\\[\n\\begin{align}\n  Z_x(X) = \\frac{X - \\mu_x}{\\sigma_x} \\\\\n  Z_y(Y) = \\frac{Y - \\mu_y}{\\sigma_y} \\\\\n\\end{align}\n\\qquad(2.1)\\]\nHere \\(\\mu\\) signifies the average (arithmetic mean) height across the entire population, and \\(\\sigma\\) denotes the population standard deviation. Thus \\(Z_x(X)\\) gives the number of standard deviations above or below the population average (expected value).\nOf course we seldom have precise values for these population parameters. In practice we then use sample estimates of the parameters, say \\(\\hat{\\mu}\\) for the sample average and \\(\\hat{\\sigma}\\) for the sample standard deviation.\n\\[\n\\begin{align}\n  \\hat{\\mu}_x &= \\frac{1}{n} \\sum_{k = 1}^{n} x_k \\\\\n  \\hat{\\sigma}_{x}^2 &= \\frac{1}{n-1} \\sum_{k = 1}^{n} (x_k - \\hat{\\mu}_x)^2 \\\\\n\\end{align}\n\\qquad(2.2)\\]\nSo the term “z-score” or “standard unit” is usually understood with respect to the sample distribution.\n\\[\n\\begin{align}\n  \\hat{Z}_x(x_k) = \\frac{x_k - \\hat{\\mu}_x}{\\hat{\\sigma}_x} \\\\\n  \\hat{Z}_y(y_k) = \\frac{y_k - \\hat{\\mu}_y}{\\hat{\\sigma}_y} \\\\\n\\end{align}\n\\qquad(2.3)\\]\n\n2.2.1 SD line\nThe line given by the equation \\(\\hat{Z}_y(y) = \\hat{Z}_x(x)\\) is called the “SD line”. Here’s an equivalent equation of this line.\n\\[\n\\begin{align}\n  \\text{SD line: } \\\\\n  y & = \\mathcal{l}_{SD}(x) \\\\\n  &= \\hat{\\mu}_y + \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\\n\\end{align}\n\\qquad(2.4)\\]\nOf all lines \\(y = \\mathcal{l}(x)\\) we might draw through the \\((x_k,y_k)\\) data points, the SD line \\(y = \\mathcal{l}_{SD}(x)\\) minimizes the sum of squared distances from each \\((x_k,y_k)\\) data point to its orthogonal projection to the line.\n\n\n2.2.2 Regression line\nConsider the father’s height as the predictor variable \\((x)\\) and the son’s height as the response variable \\((y)\\). We now seek a line that minimizes a different metric, namely the distance between the son’s height and its linear prediction based on the father’s height. In statistical parlance we are regressing the son’s height on the father’s height. (Because we have just one predictor variable this is called simple linear regression.) The minimizing line is called the regression line, and has the following equation.\n\\[\n\\begin{align}   \n  \\text{Regression line: } \\\\   \ny & = \\mathcal{l}_{R}(x) \\\\   \n&= \\hat{\\mu}_y + \\hat{r}  \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\ \\end{align}\n\\qquad(2.5)\\]\nAn equivalent equation is \\(\\hat{Z}_y(y) = \\hat{r} \\hat{Z}_x(x)\\), where \\(\\hat{r}\\) denotes the sample correlation coefficient.\n\\[\n  \\hat{r} = \\frac{1}{n-1} \\sum_{k = 1}^{n} \\hat{Z}_x(x_k) \\hat{Z}_y(y_k)\n\\qquad(2.6)\\]\nNote that \\(\\hat{r}\\) is restricted to the closed interval \\([-1, 1]\\).\nThe figure below shows the SD line and the Regression line for the father-son data.\n\n\n\n\n\n\n\n\nFigure 2.3: Father-son heights: regression line and SD line\n\n\n\n\n\nThe two lines intersect at the “point of averages”, that is, at \\((\\hat{\\mu}_x, \\hat{\\mu}_y)\\), which need not coincide with any data point.\nThe following figure and table summarize the regression “residuals”, that is the son’s height \\((y)\\) minus the height \\((\\hat{y})\\) predicted by the linear model.\n\n\n\nCall:\nlm(formula = son ~ father, data = father_son_ht)\n\nCoefficients:\n(Intercept)       father  \n    33.8866       0.5141  \n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Variation of son heights from regression line\n\n\n\n\n\n\n\n\nTable 2.2: Stats: son’s height minus predicted height\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -8.877  -1.514  -0.008   0.000   1.629   8.969 \n\n\n\n\nThere are many ways to examine how well a model represents the data. Here’s a scatter diagram of the value predicted (fitted) by the model versus the son’s actual height.\n\n\n\n\n\n\n\n\nFigure 2.5: Model-fitted value versus son’s height\n\n\n\n\n\nWe see that the sons who are extremely short or extremely tall are not represented well by the model, which is heavily influenced by mid-range father-son heights containing most of the data. The father’s height alone is a helpful but imperfect predictor of the son’s height.\n\n\n2.2.3 The Normal distribution\nThe father-son data set is well approximated by a bivariate normal distribution. The parameter estimates are as follows.\n\n\n\nTable 2.3: Father-son heights: average, sd, and correlation\n\n\n\nf_avg s_avg  f_sd  s_sd     r \n 67.7  68.7   2.7   2.8   0.5 \n\n\n\n\nSons are on average about an inch taller than fathers. Fathers and sons share similar standard deviations (2.7 versus 2.8). The sample correlation coefficient is about 0.5.\nAmong the mathematical properties of normal distributions is the fact that if the pair of random variables \\((X, Y)\\) has a bivariate normal distribution, then the conditional expectation \\(E(Y | X)\\) is indeed the linear regression function \\(\\mathcal{l}_R(X)\\) whose equation is that of the population regression line, \\(Z_y(Y) = r \\; Z_x(X)\\). The conditional distribution \\(\\mathcal{D}(Y | X)\\) of \\(Y\\) given \\(X\\) is normal with a mean of \\(\\mathcal{l}_R(X)\\) and a standard deviation of \\(\\sqrt{1 - r^2} \\; \\sigma_y\\). Conditioning on \\(X\\) thus shrinks the standard deviation of \\(Y\\) by a factor of \\(\\sqrt{1 - r^2}\\). For the father-son data, with \\(r\\) approximately equal to 0.5, this shrinkage factor is approximately 0.87, amounting to a 13% reduction in the standard deviation of \\(Y\\).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#class-exercise-diamond-data",
    "href": "conditioning.html#class-exercise-diamond-data",
    "title": "2  Conditional Distributions",
    "section": "2.3 Class Exercise: Diamond Data",
    "text": "2.3 Class Exercise: Diamond Data\nTeam up with a classmate and load the diamond data provided by R package ggplot2. Of the 10 variables (data columns) choose one of them as the response variable \\((y)\\), and another as a predictor variable \\((x)\\). Construct a scatter diagram of \\((x, y)\\) data points. Calculate the equation of the regression line. Is the predictor variable useful, or irrelevant? The R package stats includes potentially helpful functions including a linear regression function, stats::lm(), and a local polynomial regression function stats::loess(). Take 20 minutes to prepare to report out to the class.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#simulating-random-variables",
    "href": "conditioning.html#simulating-random-variables",
    "title": "2  Conditional Distributions",
    "section": "2.4 Simulating Random Variables",
    "text": "2.4 Simulating Random Variables\nThe R package stats contains functions that generate pseudo-random numbers following normal and other well-known statistical distributions. Here are some functions for simulating independent instances of a continuous or discrete random variable. In the table below, the “value” column distinguishes the function output as either continuous (dbl) or discrete (int).\n\n\n\n\nTable 2.4: Some random number generators in the R stats package\n\n\n\n\nSome random number generators in the R stats package\n\n\nfn\nvalue\ndistribution\n\n\n\n\nrbeta\ndbl\nBeta\n\n\nrcauchy\ndbl\nCauchy\n\n\nrchisq\ndbl\n(non-central) Chi-Squared\n\n\nrexp\ndbl\nExponential\n\n\nrf\ndbl\nF\n\n\nrgamma\ndbl\nGamma\n\n\nrlnorm\ndbl\nLog Normal\n\n\nrlogis\ndbl\nLogistic\n\n\nrnorm\ndbl\nNormal\n\n\nrt\ndbl\nStudent t\n\n\nrunif\ndbl\nUniform\n\n\nrweibull\ndbl\nWeibull\n\n\nrbinom\nint\nBinomial\n\n\nrgeom\nint\nGeometric\n\n\nrhyper\nint\nHypergeometric\n\n\nrnbinom\nint\nNegative Binomial\n\n\nrpois\nint\nPoisson\n\n\n\n\n\n\n\n\nThe functions listed above are designed to generate a user-prescribed number \\(n\\) of independent instances of a single random variable \\(X\\). For some purposes we may want to simulate a pair of random variables \\((X, Y)\\) or more generally a vector of random variables \\(X_{\\bullet} = (X_1, X_2, \\ldots, X_K)\\) such that the components of the vector are statistically dependent. Such vectors are said to follow a multivariate distribution. For example the stats package contains function rmultinom, a multivariate extension of rbinom. In general multivariate distributions are addressed by special-purpose R packages created by members of the R community.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#cautionary-remarks",
    "href": "conditioning.html#cautionary-remarks",
    "title": "2  Conditional Distributions",
    "section": "2.5 Cautionary Remarks",
    "text": "2.5 Cautionary Remarks\n\n2.5.1 Robust statistics\nThe sample average (arithmetic mean) is notoriously sensitive to outliers (data points far removed from most of the other data points). For this reason, the median is often used in place of the mean to describe central or typical values. For example, medians are commonly used to typify home prices in a neighborhood, and for other financial data.\nSimilarly, the interquartile range (IQR, the third minus the first quartile of the data) may be preferred to the standard deviation to measure how widely data points are spread around a central value (e.g., median).\nIn the present context this means that both the SD line and the Regression line are highly sensitive to outlying data points.\n\n\n2.5.2 Anscombe Quartet\nProfessor Frank Anscombe constructed the “Anscombe Quartet”: 4 data sets, each consisting of 11 observations of \\((x, y)\\) pairs of numeric values. Here are the statistics per group.\n\n\n\nTable 2.5: Identical (x, y) stats per group\n\n\n\n# A tibble: 4 × 6\n    grp x_avg y_avg  x_sd  y_sd     r\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     9  7.50  3.32  2.03 0.816\n2     2     9  7.50  3.32  2.03 0.816\n3     3     9  7.5   3.32  2.03 0.816\n4     4     9  7.50  3.32  2.03 0.817\n\n\n\n\nThe four groups share virtually identical averages, standard deviations, and \\((x, y)\\) correlation coefficients. Consequently the four data sets generate identical regression lines. Yet, as shown below, the pattern of \\((x, y)\\) values differs markedly among these data sets.\n\n\n\n\n\n\n\n\nFigure 2.6: Scatter diagram of (x, y) per group\n\n\n\n\n\nMoral: pay attention to the data! The graphical and tabular summaries we choose to present should be useful and informative. For example, it might be helpful to note that group 2 looks like the partial outline of a parabola. We should note that group 3 consists of 10 points falling on a line, with one outlier. In group 4 we should note that of the 11 data points, 10 have identical \\(x\\)-values. We want to minimize the chance of inadvertently conveying false impressions by merely reporting standard summary statistics.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#statistical-independence",
    "href": "conditioning.html#statistical-independence",
    "title": "2  Conditional Distributions",
    "section": "2.6 Statistical Independence",
    "text": "2.6 Statistical Independence\nThe father-son data is an example of a pair of statistically dependent variables, since the distribution of sons’ heights changes when conditioned on the father’s height. (The same can be said for fathers’ heights conditioned on the height of the son.) Short fathers tend to have short sons; tall fathers tend to have tall sons.\nPractical examples of statistically independent (unrelated) variables exist but are rare, since studies typically collect data on variables believed to be related. Nevertheless, the concept of statistical independence is very useful, as it gives rise to measures of departure from statistical independence (and thus measures of statistical association).\nThe correlation coefficient \\(r\\) is an example of such a measure for two continuous variables. If \\((X, Y)\\) is a pair of statistically independent variables, then \\(r = 0\\). Note, however, that \\((X, Y)\\) may be statistically dependent even if uncorrelated, that is, even if \\(r = 0\\).\n\n2.6.1 Definition\nThe pair of random variables \\((X, Y)\\) is defined to be statistically independent if\n\\[\n\\begin{align}\n  P(X \\in A, \\; Y \\in B) &= P(X \\in A) \\times P(Y \\in B) \\\\ \\\\\n  & \\text{for all possible sets } A, B \\\\\n\\end{align}\n\\qquad(2.7)\\]\nIf \\(P(X \\in A) &gt; 0\\) then statistical independence implies:\n\\[\n\\begin{align}\n  P(Y \\in B \\; | \\; X \\in A) &= \\frac{P(X \\in A, \\; Y \\in B)}{P(X \\in A)} \\\\\n  &= P(Y \\in B) \\\\ \\\\\n  & \\text{whenever } P(X \\in A) &gt; 0\n\\end{align}\n\\qquad(2.8)\\]\nThat is, the conditional probability of random variable \\(Y\\) belonging to set \\(B\\) given that \\(X\\) belongs to set \\(A\\) is equal to the unconditional probability that \\(Y\\) belongs to set \\(B\\). It follows that the conditional expectation \\(E(Y | X)\\) does not depend on \\(X\\), and thus equals the constant \\(E(Y)\\), the unconditional expected value of \\(Y\\).\n\n\n2.6.2 The case when X and Y are categorical variables\nAs previously noted, for a pair \\((X,Y)\\) of continuous variables, the correlation coefficient is a measure (though not a definitive measure) of statistical association or dependence. In the case when \\(X\\) and \\(Y\\) are each restricted to a finite set of values, the chi-square statistic is often a useful measure of statistical association.\nTo illustrate, we use data on handedness (right, left, or ambidextrous) of US adults aged 25-34. The data were collected by the US Health and Nutrition Examination Survey (HANES), as cited in FPP. The question we investigate is whether handedness is independent of sex (male, female). For each of the six possible combinations of handedness and sex, the table below counts the number of people in the sample having that combination.\n\n\n\n\nTable 2.6: XXX\n\n\n\n\nHandedness counts among sampled males and females\n\n\nhandedness\nmale\nfemale\n\n\n\n\nright\n934\n1070\n\n\nleft\n113\n92\n\n\nambi\n20\n8\n\n\n\n\n\n\n\n\nThe percentages of handedness among males and among females are as follows.\n\n\n\n\nTable 2.7: XXX\n\n\n\n\nPercentage handedness among males, and among females\n\n\nhandedness\nmale\nfemale\n\n\n\n\nright\n87.5\n91.5\n\n\nleft\n10.6\n7.9\n\n\nambi\n1.9\n0.7\n\n\n\n\n\n\n\n\nIf handedness and sex were independent, we should see similar percentages of males and females for each type of handedness. The above table indeed shows similar percentages, but are they close enough to conclude independence?\nIn the early 1900’s Karl Pearson developed the chi-squared test of independence of categorical variables. The reasoning is as follows. Suppose we accept as population estimates the data percentages of handedness (across males and females), and we also accept the somewhat different percentages of males and females in the sample. These so-called marginal distributions are not in dispute. What we’re investigating concerns the cell percentages, combinations of handedness and sex. Under the assumption of independence we would expect each cell percentage in the data to be close to the product of the handedness percentage and the male or female percentage. That product is an expected cell percentage (assuming independence). Multiplying the expected cell percentage by the by the sample size \\(n\\) (number of people in the sample) gives an expected cell count. Pearson’s test of independence is based on the following chi-squared statistic.\n\\[\n\\begin{align}\n  \\chi^2 &= \\sum_{j = 1}^J {\\sum_{k = 1}^K {\\frac{(O_{j,k} - E_{j,k})^2}{E_{j,k}}}} \\\\ \\\\\n  O_{j,k} &= \\text{observed count for cell } \\{j, k\\} \\\\\n  E_{j,k} &= \\text{expected count for cell } \\{j, k\\} \\\\\n\\end{align}\n\\qquad(2.9)\\]\nUnder the assumption of independence Pearson determined the probability distribution of the \\(\\chi^2\\) statistic mathematically based on the notion of “degrees of freedom”.\nThat is, for each row-index \\(j\\) the expected counts summed across \\(k\\) are constrained to match the corresponding sum of the observed values (the sum for row \\(j\\)). Similarly, for each column-index \\(k\\) the expected counts summed across \\(j\\) are constrained to match the corresponding sum of the observed values (the sum for column \\(k\\)). Given these fixed marginal sums, cell values can vary with \\((J-1) \\times (K-1)\\) degrees of freedom.\nUnder the assumption of independence and for a large sample size \\(n\\), the chi-squared statistic approximately follows the distribution of the sum of squared independent standard normal variables, the number of independent normal variables matching the degrees of freedom.\nFor the handedness data the degrees of freedom equals 2, and the value of the statistic is 11.8, which is beyond the 99% quantile of the corresponding chi-squared distribution (and thus yields a “p-value” of less than 1%). This would be regarded as strong evidence against the assumption of independence.\nThe chi-squared statistic is the sum of squared terms of the following form.\n\\[\n\\begin{align}\n  \\frac{O_{j,k} - E_{j,k}}{\\sqrt{E_{j,k}}} \\\\\n\\end{align}\n\\qquad(2.10)\\]\nThese terms are called “Pearson residuals”. For the handedness data, the Pearson residuals are as follows.\n\n\n\n\nTable 2.8: XXX\n\n\n\n\nPearson residuals for handedness data\n\n\nhandedness\nmale\nfemale\n\n\n\n\nright\n-0.7\n0.7\n\n\nleft\n1.5\n-1.5\n\n\nambi\n1.8\n-1.7\n\n\n\n\n\n\n\n\nRoughly speaking, under independence the magnitude of cell values should align with the scale of standard normal variables. For the handedness data, the large value of the chi-square statistic cannot be attributed to a single cell of the table, but rather to the left-handed and ambidextrous cells (handedness to which males are more prone than females).\n\n\n2.6.3 Simpson’s Paradox\nWe now turn to a different set of categorical data from a study of graduate admissions at UC Berkeley in 1973 available in R as datasets::UCBAdmissions. The study was prompted by a concern of bias against females. The table below summarizes admission percentages for males and for females across the six largest departments.\n\n\n\n\nTable 2.9: XXX\n\n\n\n\nAdmission percentages for males and for females\n\n\ndecision\nMale\nFemale\n\n\n\n\nAdmitted\n44.5\n30.4\n\n\nRejected\n55.5\n69.6\n\n\n\n\n\n\n\n\nThese percentages look damning, but the table below, showing admission rates per department, tells a different story.\n\n\n\n\nTable 2.10: Admission percentages per department\n\n\n\n\nAdmission percentages per department\n\n\ndept\namong_males\namong_females\noverall\n\n\n\n\nA\n62.1\n82.4\n64.4\n\n\nB\n63.0\n68.0\n63.2\n\n\nC\n36.9\n34.1\n35.1\n\n\nD\n33.1\n34.9\n34.0\n\n\nE\n27.7\n23.9\n25.2\n\n\nF\n5.9\n7.0\n6.4\n\n\n\n\n\n\n\n\nThe table shows that four of the six departments admitted a greater percentage of female applicants than male applicants. In the remaining two departments females did somewhat worse than males. Yet, summing over all six departments, women applicants fared decidedly worse than male applicants. How can this be?\nThe answer can be found by: (1) noting that the table above lists departments, labeled A through F, in descending order of overall admission rates; and (2) examining the following table that shows each department’s share of applicants: male, female, and overall.\n\n\n\n\nTable 2.11: Number of applications per department\n\n\n\n\nNumber of applications per department\n\n\ndept\nfrom_males\nfrom_females\noverall\n\n\n\n\nA\n825\n108\n933\n\n\nB\n560\n25\n585\n\n\nC\n325\n593\n918\n\n\nD\n417\n375\n792\n\n\nE\n191\n393\n584\n\n\nF\n373\n341\n714\n\n\n\n\n\n\n\n\nHere are the same counts but now converted into per-department percentage of applications from males, females, and overall, respectively.\n\n\n\n\nTable 2.12: Percent of applications per department\n\n\n\n\nPercent of applications per department\n\n\ndept\nfrom_males\nfrom_females\noverall\n\n\n\n\nA\n30.7\n5.9\n20.6\n\n\nB\n20.8\n1.4\n12.9\n\n\nC\n12.1\n32.3\n20.3\n\n\nD\n15.5\n20.4\n17.5\n\n\nE\n7.1\n21.4\n12.9\n\n\nF\n13.9\n18.6\n15.8\n\n\n\n\n\n\n\n\nWe see that relatively few females applied to departments A and B, which had the highest overall admission rates. Females tended more than males to apply to departments having overall low rates of admission. That is, departmental admission rate is an explanatory variable missing from the initial summary of male and female admission rates across all six departments.\nThis phenomenon, a pattern per group that is masked when summarized across groups, is known as Simpson’s paradox. More generally, we must be alelrt to the possibility that we have overlooked some variable (sometimes called a “confounding” variable) that could alter our conclusions.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#entropy",
    "href": "conditioning.html#entropy",
    "title": "2  Conditional Distributions",
    "section": "2.7 Entropy",
    "text": "2.7 Entropy\n\n2.7.1 Background\nThe term “entropy” was defined in the mid-19th century (with the emergence of Statistical Mechanics) as a measure of the disorder of a physical system. In 1948 (with the emergence of Information Theory) Claude Shannon introduced the same term and equivalent mathematical definition as a measure of uncertainty.\n\n\n2.7.2 Yes-No Questions\nLet’s illustrate the concept of entropy with a variant of the game “Twenty Questions”. The contestant is presented with a box of tickets, each ticket bearing a single capital letter of the English alphabet. The contestant is shown the box, and thus knows the number of tickets bearing each letter. (It may happen that only a few of the possible 26 letters actually appear in the box.) The game begins with the random drawing of a ticket not visible to the contestant. The contestant may ask yes-no questions about the ticket until the contestant determines with certainty the letter written on the ticket. (The contestant does not guess but rather comes to a firm conclusion.) The ticket is put back in the box, ending the first round of the game. Subsequent rounds of the game are exactly like the first, a new ticket is drawn at random; it’s letter must be deduced by the contestant through a sequence of yes-no questions. The contestant is evaluated on the average number of questions required to determine the letter on a randomly drawn ticket.\nWe suppose that contestant devises the most informative sequence of questions possible. Consequently, the average number of required questions is a measure of the difficulty presented by the set of tickets in the box, that is, of the uncertainty of the value of a randomly drawn ticket.\nHere are some different scenarios.\nBox 1: If the contents of the box were \\(\\{A, A , A, A \\}\\) then the contestant needn’t spend any questions to determine with certainty the value of a randomly drawn ticket. The average number of required questions would be zero.\nBox 2: If the contents of the box were \\(\\{A, A , B, B \\}\\) then the contestant would require one question to determine with certainty the value of a randomly drawn ticket. The average number of required questions would be one.\nBox 3: If the contents of the box were \\(\\{A, B , C, D \\}\\) then the contestant would require two questions to determine with certainty the value of a randomly drawn ticket. The average number of required questions would be two.\nBox 4: Now consider the box \\(\\{ A, A, B, C \\}\\). The contestant’s first question might be whether the ticket-value is \\(A\\), the most probable value. In half the rounds the answer would be a definitive yes, limiting the number of questions to 1. In the other half of the rounds, a single follow-up question would be required to identify the ticket-value with certainty. Averaged across rounds the required number of questions would be \\((\\frac{1}{2} \\times 1) + (\\frac{1}{2} \\times 2) = \\frac{3}{2}\\).\nIn general, consider a binary search strategy. Partition the set of all tickets into two subsets of distinct ticket-values, so that the two subsets contain a nearly equal number of tickets (to the extent possible). Devise the first question to determine to which of the two subsets the randomly drawn ticket belongs. Now partition the identified subset into two further subsets distinguished by ticket-values, again of equal or nearly equal size. Devise the second question to determine which of these two subsets is the origin of the randomly drawn ticket. Continue in this way until the randomly drawn ticket is identified.\nUnder the binary search strategy the maximum number, say \\(\\mu\\), of required questions is a function of the number, say \\(K\\), of distinct ticket-values, namely, \\(\\mu\\) is the smallest integer such that \\(\\mu \\ge \\log_2(K)\\). But that is a worst-case scenario: \\(\\mu\\) is generally greater than the average number of required questions, as illustrated by Box 4.\n\n\n2.7.3 Definition\nThe mathematical definition of entropy (usually denoted \\(H\\)) gives a lower bound on the average number of required questions that follow an optimal strategy. For a finite probability distribution \\(p_{\\bullet} = (p_1, p_2, \\ldots, p_K)\\) the mathematical definition is as follows.\n\\[\n\\begin{align}\n  H(p_1, p_2, \\ldots, p_K) \\\\\n  &= \\sum_{k = 1}^K { p_k \\times \\log_2(\\frac{1}{p_k}) } \\\\\n  &= - \\sum_{k = 1}^K { p_k \\times \\log_2(p_k) } \\\\\n  \\\\\n  & \\text{with } p_k = \\text{probability of drawing value } k \\\\\n  & \\text{and } K = \\text{number of distinct values}\n\\end{align}\n\\qquad(2.11)\\]\n(This definition of \\(H\\) uses a base-2 logarithm \\(\\log_2()\\) to match our yes-no question game. The units of this \\(H\\) are the required number of yes-no questions, that is, binary digits, or “bits”. \\(H\\) is sometimes defined using the natural logarithm \\(\\log_e()\\) yielding a unit called “nats”. Changing the base of the logarithm changes \\(H\\) by a multiplicative constant.)\nFor the first box in the game above, we have a single value \\(A\\), which is thus drawn with probability one, which yields \\(H = 0\\).\nFor the second box we have two values, each drawn with probability \\(\\frac{1}{2}\\), so \\(H = 1\\).\nFor the third box we have four values, each drawn with probability \\(\\frac{1}{4}\\), so \\(H = 2\\).\nFor the fourth box, we calculated that the average number of required questions is \\(\\frac{3}{2}\\), which is the value of \\(H\\).\n\n\n2.7.4 \\(H_{X, Y}\\) for independent \\((X, Y)\\)\nSo far we’ve discussed entropy with respect to a single random variable. We now extend the discussion to a pair of random variables. Let’s change the game so that each ticket now bears both a letter and a positive integer.\nLet our first example be \\(\\{ A_1, A_1, B_1, C_1, A_2, A_2, B_2, C_2 \\}\\), in which the set of letter-tickets \\(\\{ A, A, B, C \\}\\) is duplicated, initially with the subscript 1, and then with the subscript 2. Drawing a ticket at random from this box is equivalent to drawing a letter at random from \\(\\{ A, A, B, C \\}\\) and then independently drawing a number from \\(\\{ 1, 2 \\}\\). The contestant may as well first ascertain the letter and then ascertain the number, requiring on average \\(\\frac{3}{2} + 1\\) questions.\nMore generally, suppose we have a pair \\((X, Y)\\) of independent random variables that take on a finite set of values. Then\n\\[\n\\begin{align}\n  P(X = x_j, \\; Y = y_k) \\\\\n  &= P(X = x_j) \\times P(Y = y_k) \\\\\n\\end{align}\n\\]\nor more succinctly\n\\[\n\\begin{align}\n  p_{X, Y}(j, k) &= p_X(j) \\times p_Y(k)\n\\end{align}\n\\]\nThe entropy of the distribution of \\((X, Y)\\) is:\n\\[\n\\begin{align}\n  H_{X, Y} &= H(\\; \\{ p_{X, Y}(j, k) \\} \\;) \\\\\n  &= - \\sum_{j = 1}^J{\\sum_{k = 1}^K {p_{X, Y}(j,k) \\times log_2(\\; p_{X, Y} (j,k) \\;)}} \\\\\n  &= - \\sum_{j = 1}^J{\\sum_{k = 1}^K {p_X(j) \\times p_Y(k) \\times log_2(\\; p_X(j) \\times p_Y(k) \\;)}} \\\\\n  &= - \\sum_{j = 1}^J \\sum_{k = 1}^K p_X(j) \\times p_Y(k) \\times \\{ \\; log_2(p_X(j) + log_2(p_Y(k) \\; \\} \\\\\n  &= H_X \\times \\sum_{k = 1}^K p_Y(k) \\; + \\; \\sum_{j = 1}^J p_X(j) \\times H_Y \\\\\n  &= H_X + H_Y\n\\end{align}\n\\qquad(2.12)\\]\nIn words, when \\((X, Y)\\) are independent, their joint entropy equals the sum of their respective entropies.\n\n\n2.7.5 Mutual Information\nNow consider the case where \\((X, Y)\\) are dependent. Suppose, for example, that our box of tickets is \\(\\{ A_1, A_2, B_1, C_2 \\}\\). The marginal distribution of letters remains \\(\\{A, A, B, C \\}\\) and the marginal distribution of numbers is \\(\\{1, 2, 1, 2\\}\\) which is equivalent to the box \\(\\{1, 2\\}\\) of the previous example.\nIn the previous example, had the contestant first determined the subscript on the randomly drawn ticket, that information would not have affected the subsequent process of determining the letter. The average number of required questions would remain \\(1 + \\frac{3}{2}\\).\nNow, however, using the first question to determine the subscript reduces the letter possibilities to either \\(\\{A, B\\}\\) or else \\(\\{A, C\\}\\). A single additional question is required to determine the letter. Thus the total number of required questions is 2, which equals the entropy value \\(H\\).\nLet’s continue to suppose that \\((X, Y)\\) are dependent. An ill-informed contestant might adopt the strategy optimal for the independent case, attacking one variable at a time. But this strategy would no longer be optimal. There would now be instances in which knowledge of one variable would reduce the average number of additional questions required to determine the value of the other variable. Thus the joint entropy \\(H_{X, Y}\\) never exceeds the entropy \\(H_X + H_Y\\) of the independent case.\nThe reduction in entropy when the distribution of \\((X, Y)\\) is changed from independent to dependent (while retaining the original marginal distributions) goes by different names, including “information gain” and (less ambiguously) “mutual information”.\nWe define mutual information \\((MI)\\) as this reduction.\n\\[\n\\begin{align}\n  MI_{X, Y} &= H_X + H_Y - H_{X, Y} \\\\\n\\end{align}\n\\qquad(2.13)\\]\n\\(MI\\) is non-negative, and is zero when \\((X, Y)\\) are independent.\n\n\n2.7.6 KL Divergence\nContinuing from the discussion of mutual information, suppose that an ill-informed contestant has optimized their questioning strategy for the box \\(\\{ A_1, A_1, B_1, C_1, A_2, A_2, B_2, C_2 \\}\\), in which letters and numbers occur independently, when in fact the box is \\(\\{ A_1, A_2, B_1, C_2 \\}\\). The contestant’s strategy requires an average of \\(\\frac{5}{2}\\) questions to determine the randomly drawn letter-number combination with certainty, but for the actual box one requires just 2 questions. The misinformation about the box from which tickets are randomly drawn costs the contestant, on average, \\(\\frac{1}{2}\\) per question more than necessary.\nKullback-Liebler divergence is a mathematical representation of this phenomenon. It is defined as follows.\n\\[\n\\begin{align}\n  KL(P \\; || \\; Q) \\\\\n  &= E_P \\left( \\log_2 \\left( \\frac{ 1 }{ Q(X) } \\right) \\right) - E_P \\left( \\log_2 \\left( \\frac{ 1 }{ P(X) } \\right) \\right) \\\\  \n  &= E_P \\left( \\log_2 \\left( \\frac{ P(X) }{ Q(X) } \\right) \\right) \\\\  \n  &= \\sum_{ \\{x : P(x) &gt; 0 \\}} P(x) \\times \\log_2 \\left( \\frac{P(x)}{Q(x)} \\right) \\\\\n  \\\\\n  & \\text{with } P(x) = \\text{reference probability mass function} \\\\\n  & \\text{and } Q(x) = \\text{alternative probability mass function} \\\\\n\\end{align}\n\\qquad(2.14)\\]\nIn the example above, \\(P(\\cdot)\\) is the probability of drawing any given letter-number combination from \\(\\{ A_1, A_2, B_1, C_2 \\}\\), and \\(Q(\\cdot)\\) is the corresponding probability for \\(\\{ A_1, A_1, B_1, C_1, A_2, A_2, B_2, C_2 \\}\\). Here is the tally of the sum defining \\(KL(P \\; || \\; Q)\\).\n\n\n\n\nTable 2.13: XXX\n\n\n\n\nKL Divergence example\n\n\nx\nP(x)\nQ(x)\nlog2_ratio\nterm\n\n\n\n\nA_1\n0.25\n0.250\n0\n0.00\n\n\nB_1\n0.25\n0.125\n1\n0.25\n\n\nC_1\n0.00\n0.125\n-Inf\n0.00\n\n\nA_2\n0.25\n0.250\n0\n0.00\n\n\nB_2\n0.00\n0.125\n-Inf\n0.00\n\n\nC_2\n0.25\n0.125\n1\n0.25\n\n\n\n\n\n\n\n\nFor purposes of illustration the table above includes any value \\(x\\) assigned positive probability by either \\(P(\\cdot)\\) or \\(Q(\\cdot)\\), even those for which \\(P(x) = 0\\) and therefore do not contribute to the KL divergence, \\(KL(P \\; || \\; Q)\\). Also note that the sum of the terms, that is the KL divergence, is indeed \\(\\frac{1}{2}\\). We will return to KL divergence in subsequent discussions.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#team-exercises",
    "href": "conditioning.html#team-exercises",
    "title": "2  Conditional Distributions",
    "section": "2.8 Team Exercises",
    "text": "2.8 Team Exercises\n\nSimulate bivariate normal variables: Using stats::rnorm() or otherwise, generate independent instances of a standard normal variable \\(X\\) (that is, having mean zero and standard deviation 1). Next choose a value of \\(r\\) such that \\(-1 &lt; r &lt; 1\\). Now for each instance of \\(X\\) construct an instance of random variable \\(Y\\) so that the distribution \\(\\mathcal{D}(Y \\; | \\; X)\\) of \\(Y\\) given \\(X\\) is normal with expected value \\(r \\times X\\) and standard deviation \\(\\sqrt{1 - r^2}\\). (Hint: consider constructing \\(Y\\) by using \\(X\\) along with a new, independent standard normal variable \\(Z\\).) What are the unconditional mean and standard deviation of \\(Y\\)? What is the correlation coefficient of the pair \\((X, Y)\\)? How might you generalize your construction to accommodate other prescribed means \\((\\mu_x, \\mu_y)\\) and standard deviations \\((\\sigma_x, \\sigma_y)\\) of \\((X, Y)\\)?\nSimpson’s paradox: In the discussion above we illustrated Simpson’s paradox using the UCB Admissions data. Find or construct a different example.\nEntropy, Discrete Uniform: If Box 5 is \\(\\{ A, B, C, D, E \\}\\), how many questions are required, on average, to determine with certainty the ticket that has been randomly drawn? What is the entropy \\(H\\) of this box? More generally, calculate \\(H\\) for a box \\(\\{ x_1, x_2, \\ldots, x_K \\}\\) that contains \\(K\\) tickets, each having a unique value.\nEntropy, UCB Admissions: Consider a box of tickets that matches the UC Berkeley admissions data. The number of tickets in the box is the number of applications in the data. Each ticket has three markings: either “Male” or “Female” to denote the sex of the applicant; either “Admitted” or “Rejected” to denote the decision made on the application; and one of \\(\\{ A, B, C, D, E, F \\}\\) to denote the department that made the decision.\n\n\nRestricting attention to just the “Admitted” or “Rejected” marking, calculate \\(H_{\\text{decision}}\\).\nNow calculate \\(H_{\\text{decision, sex}}\\) and the mutual information \\(MI_{\\text{decision, sex}}\\).\nHow would you formulate the information gained in the analysis of sex bias by including departmental admission rates?",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "conditioning.html#resources",
    "href": "conditioning.html#resources",
    "title": "2  Conditional Distributions",
    "section": "2.9 Resources",
    "text": "2.9 Resources\nR Graphics Cookbook (2e) by Winston Chang\nStatistics (4e) by Freedman, Pisani, Purves | Goodreads\nIndependence (probability theory) - Wikipedia\nSex bias in graduate admissions: data from Berkeley, by Bickel, Hammel, and O’connell\nA Mathematical Theory of Communication, by C.E. Shannon\nShannon Entropy, Information Gain, and Picking Balls from Buckets | by Luis Serrano | Udacity Inc | Medium\nMutual information - Wikipedia\nDistances and Divergences for Probability Distributions by Andrew Nobel",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conditional Distributions</span>"
    ]
  },
  {
    "objectID": "graph-theory.html",
    "href": "graph-theory.html",
    "title": "14  Graph Theory for Machine Learning",
    "section": "",
    "text": "14.1 Types of Data in which Graphs Arise\nIn mathematics and computer science, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called arcs, links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically. 1",
    "crumbs": [
      "Graph Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Graph Theory for Machine Learning</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#types-of-data-in-which-graphs-arise",
    "href": "graph-theory.html#types-of-data-in-which-graphs-arise",
    "title": "14  Graph Theory for Machine Learning",
    "section": "",
    "text": "14.1.1 Transaction examples\nSuppose \\(A\\) contacts or pays \\(B\\), where the nodes \\(A\\) and \\(B\\) are people (or perhaps software functions). The edge connecting \\(A\\) and \\(B\\) might be labeled “contact” or “pay”, in which case we need to distinguish whose contacting or paying whom. That information is given by the direction of the edge.\nOn the other hand, suppose we have just two types of node: people and products. And we have just one type of edge, say, “purchase”. Thus person \\(A\\) purchases product \\(B\\). The subject-object relationship is evident from the distinct types of node. Then the “A-B” connection could just as well be read as: \\(B\\) is purchased by \\(A\\). Given the distinct types of node, the connection could be represented by an undirected edge. (This situation gives rise to a bipartite graph.)\n\n\n14.1.2 Relationship examples\nPerson \\(A\\) knows person \\(B\\).\nPerson \\(A\\) helps to create content \\(B\\) (article, book, movie, recording, …).\nPerson \\(A\\) attends event \\(B\\).\nLocations \\(A\\) and \\(B\\) are connected via public transport.\n\\(A\\) is distance \\(d\\) from \\(B\\). Then the \\(A-B\\) edge might be given weight \\(d\\).",
    "crumbs": [
      "Graph Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Graph Theory for Machine Learning</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#glossary-of-basic-terms",
    "href": "graph-theory.html#glossary-of-basic-terms",
    "title": "14  Graph Theory for Machine Learning",
    "section": "14.2 Glossary of Basic Terms",
    "text": "14.2 Glossary of Basic Terms\nA graph is an ordered pair \\(G = (V, E)\\) of a set of vertices \\(V\\) and a set of edges \\(E\\). Each edge consists of a pair of vertices \\((A, B)\\). The edge may be denoted as \\(e(A, B)\\) or as \\(A-B\\). In an undirected graph the edge \\(A-B\\) is identical to the edge \\(B-A\\), whereas in an directed graph \\(A-B\\) denotes the edge from \\(A\\) to \\(B\\) and is distinct from \\(B-A\\), the edge from \\(B\\) to \\(A\\). 2\n\n\n\n\nTable 14.1: Graph Theory: Basic Terms\n\n\n\n\nGraph Theory: Basic Terms\n\n\n\n\n\n\nterm\ndescription\n\n\n\n\nacyclic\na graph is acyclic if it has no cycles\n\n\nadjacency\nbinary indicator: given vertices are / are not endpoints of a common edge\n\n\nconnected component\na maximal connected subgraph\n\n\nconnected graph\na graph is connected if each pair of vertices is connected\n\n\nconnected vertices\na pair of vertices that co-occur in some path\n\n\ncycle\na finite path whose first and last vertex are the same\n\n\ndegree\nnumber of incident edges of a vertex\n\n\ndirected edge\nan ordered pair of vertices (called endpoints of the edge)\n\n\nedge\na specified pair of vertices (in a hypergraph, more than two vertices)\n\n\nedge weight\na numerical value assigned to an edge\n\n\ngraph\na system of vertices (nodes) and edges\n\n\nincidence\nif a vertex is an endpoint of an edge, the (vertex, edge) pair is said to be incident\n\n\nneighborhood, 1-hop\nthe subgraph of vertices adjacent to a referenced vertex\n\n\nneighborhood, 1.5-hop\nthe subgraph induced by a vertex, its adjacent vertices, and their adjacent vertices\n\n\nneighborhood, 2-hop\nthe subgraph of vertices of distance 2 from a referenced vertex\n\n\npath\na sequence of adjacent vertices\n\n\nsimple path\na path in which no vertex is repeated\n\n\nsubgraph\na subset of edges along with their endpoints and possibly additional vertices\n\n\nsubgraph, induced\na subset of vertices along with the edges having both endpoints in the subset\n\n\ntree, directed\na directed graph having a distinguished root vertex R such that there is exactly one path from R to any other vertex V\n\n\ntree, undirected\na connected, acyclic graph",
    "crumbs": [
      "Graph Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Graph Theory for Machine Learning</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#properties",
    "href": "graph-theory.html#properties",
    "title": "14  Graph Theory for Machine Learning",
    "section": "14.3 Properties",
    "text": "14.3 Properties\n\n14.3.1 Matrix Representations\nLet \\(G\\) be a graph having \\(n_V\\) nodes.\n\n14.3.1.1 Adjacency matrix\nThe \\(n_V \\times n_V\\) adjacency matrix identifies adjacent nodes, and is defined as follows. 3\n\\[\n\\begin{align}\n  A_{j, k} &=\n  \\begin{cases}\n    1 & \\text{ if } e(\\nu_j, \\nu_k) \\in E \\\\\n    0 & \\text{ otherwise}\n  \\end{cases}\n\\end{align}\n\\qquad(14.1)\\]\nThe same information can be represented by a matrix having just two columns that lists only those vertex pairs \\((\\nu_1, \\nu_2)\\) that define an edge of the graph. Such a sparse matrix representation may also be used to list only a subset of edges. The selected subset may be designed, for example, to achieve computational efficiency.\n\n\n14.3.1.2 Degree matrix\nIn an undirected graph, the degree matrix is an \\(n_V \\times n_V\\) diagonal matrix \\(D\\) in which diagonal element \\(D_{i, i}\\) counts the number of edges incident with node \\(\\nu_i\\). \\(D_{i, i}\\) is called the degree of node \\(\\nu_i\\), and is related to the adjacency matrix as follows. 4\n\\[\n\\begin{align}\n  D_{i, i} &= \\sum_{k = 1}^{n_V} A_{i, k}\n\\end{align}\n\\qquad(14.2)\\]\nIn a directed graph the adjacency matrix may not be symmetric. The edges incident with node \\(\\nu_i\\) of the form \\(e(\\nu_i, \\nu_k)\\) are “out-edges” of \\(\\nu_i\\), whereas those of the form \\(e(\\nu_j, \\nu_i)\\) are “in-edges”. The “out-degree” \\(D_{i, i}^{(out)}\\) and “in-degree” \\(D_{i, i}^{(in)}\\) are respectively the sum across columns of each row, and the sum across rows of each column.\n\\[\n\\begin{align}\n  D_{i, i}^{(out)} &= \\sum_{k = 1}^{n_V} A_{i, k} \\\\\n  D_{i, i}^{(in)}  &= \\sum_{j = 1}^{n_V} A_{j, i}\n\\end{align}\n\\qquad(14.3)\\]\n\n\n14.3.1.3 Laplacian matrix\nIn an undirected graph, the Laplacian (or Kirchhoff) matrix is an \\(n_V \\times n_V\\) matrix \\(L\\) defined as follows. 5\n\\[\n\\begin{align}\n  L &= D - A\n\\end{align}\n\\qquad(14.4)\\]\nConsequently\n\\[\n\\begin{align}\n  L_{j, k} &=\n  \\begin{cases}\n    D_{j, j} & \\text{ if } j = k \\\\\n    -1 & \\text{ if } j\\ne k \\text{ and } e(\\nu_j, \\nu_k) \\in E \\\\\n    0 & \\text{ otherwise}\n  \\end{cases}\n\\end{align}\n\\qquad(14.5)\\]\n\n\n\n14.3.2 Betweenness centrality\nQuestion: Which nodes and edges are “central”? 6\n\n14.3.2.1 Nodes\nThe notion of betweenness centrality is that a node \\(v\\) is central to a graph if it is often included in the shortest paths between any connected nodes of the graph. This notion was formally defined in the late 1970’s as the following measure \\(\\gamma_V(v)\\) of the betweenness centrality of node \\(v\\).\n\\[\n\\begin{align}\n  \\gamma_V(v) &= \\sum_{s, t \\ne v}\n    \\frac{\\sigma_{s, t} (v)}{\\sigma_{s, t}}\n\\end{align}\n\\qquad(14.6)\\]\nwhere\n\\[\n\\begin{align}\n  (s, t) &= \\text{ distinct connected nodes} \\\\\n  \\mathcal{P}_{s, t} &= \\text{ shortest paths from } s \\text{ to } t \\\\\n  \\mathcal{P}_{s, t} (v) &= \\text{ shortest paths from } s \\text{ to } t \\text{ that pass through } v \\\\\n  \\sigma_{s, t} &= \\text{ number of paths in } \\mathcal{P}_{s, t} \\\\\n  \\sigma_{s, t} (v) &= \\text{ number of paths in } \\mathcal{P}_{s, t} (v)\n\\end{align}\n\\qquad(14.7)\\]\nThe following conventions simplify the notation.\n\\[\n\\begin{align}\n  \\sigma_{s, t} &= 1 & \\text{ if } s = t \\\\\n  \\sigma_{s, t} (v) &= 0 & \\text{ if } v \\in \\{ s, t \\}\n\\end{align}\n\\qquad(14.8)\\]\nNote that each term in the sum within Equation 14.6 is a fraction known as the pair dependency of \\((s, t)\\) on \\(v\\), denoted as follows.\n\\[\n\\begin{align}\n  \\delta_{s, t}(v) &= \\frac{\\sigma_{s, t} (v)}{\\sigma_{s, t}}\n\\end{align}\n\\qquad(14.9)\\]\nThis is the proportion of shortest paths from node \\(s\\) to node \\(t\\) that include node \\(v\\). The proportion thus falls within the closed unit interval: \\(0 \\le \\delta_{s, t}(v) \\le 1\\).\nA related quantity is the single dependency of \\(s\\) on \\(v\\), defined as follows.\n\\[\n\\begin{align}\n  \\delta_s(v) &= \\sum_{t \\not \\in \\{s, v \\}} \\delta_{s, t}(v)\n\\end{align}\n\\qquad(14.10)\\]\nIn words, for a fixed starting point \\(s\\) and a fixed intermediate node \\(v\\) this is the sum over end-points \\(t\\) of the paired dependency of \\((s, t)\\) on \\(v\\). Then we have\n\\[\n\\begin{align}\n  \\gamma_V(v) &= \\sum_{s \\ne v} \\delta_s(v)\n\\end{align}\n\\qquad(14.11)\\]\n\n\n14.3.2.2 Edges\nThe betweenness centrality of an edge is defined similarly to that for a node.\n\\[\n\\begin{align}\n  \\gamma_E(e) &= \\sum_{s, t \\in V}\n    \\frac{\\sigma_{s, t} (e)}{\\sigma_{s, t}}\n\\end{align}\n\\qquad(14.12)\\]\nSummation is over all pairs \\((s, t)\\) of connected nodes. Each term of the summation is a fraction, namely the proportion of shortest paths from \\(s\\) to \\(t\\) that include edge \\(e\\).\n\n\n\n14.3.3 Connected components\nIn graph theory, a component of an undirected graph is a connected subgraph that is not part of any larger connected subgraph. The components of any graph partition its vertices into disjoint sets, and are the induced subgraphs of those sets. A graph that is itself connected has exactly one component, namely the entire graph. Components are sometimes called connected components. In R graph components can be determined by function igraph::components(). 7\n\n\n14.3.4 Communities\nIn graph theory, the notion of “community” (or cluster, or module) is similar to but less stringent than that of a connected component. A community is a subgraph, with many edges joining vertices of the same community and comparatively few edges joining vertices of different communities. 8\n\n\n14.3.5 Modularity\nThe strength of a given community structure of a graph can be measured as the modularity of the graph, which compares the actual number of edges between node pairs to the expected number in a randomly generated graph (constrained to preserve the degree of each node).\nSuppose that \\(c(\\nu_j)\\) is the assignment by classification function of \\(c(\\cdot)\\) of each node \\(\\nu_j\\) to one of the integers \\(\\{ 1, \\ldots, n_c \\}\\). Let \\(Q(c(\\cdot))\\) denote the modularity measure of \\(c(\\cdot)\\). Then \\(Q(\\cdot)\\) is defined compatably but distinctly for directed versus undirected graphs.\n\n14.3.5.1 Undirected graphs\nFor an undirected graph, modularity \\(Q(\\cdot)\\) is defined as follows.\n\\[\n\\begin{align}\n  Q &= \\frac{1}{2 n_E} \\sum_{j, k = 1}^{n_V} \\left (A_{j, k} - \\frac{D_{j, j} \\; D_{k, k}}{2 n_E} \\right ) \\; \\delta(c(\\nu_j), c(\\nu_k))\n\\end{align}\n\\qquad(14.13)\\]\nwhere\n\\[\n\\begin{align}\n  n_E &= \\left| E \\right| \\text{ = number of edges} \\\\\n  n_V &= \\left| V \\right| \\text{ = number of nodes} \\\\\n  A &= \\text{adjacency matrix} \\\\\n  D_{i, i} &= \\text{degree of node } \\nu_i \\\\\n  \\delta(\\cdot) &= \\text{Kronecker delta function}\n\\end{align}\n\\qquad(14.14)\\]\n\n\n14.3.5.2 Directed graphs\nFor a directed graph, modularity \\(Q(\\cdot)\\) is defined thus.\n\\[\n\\begin{align}\n  Q &= \\frac{1}{n_E} \\sum_{j, k = 1}^{n_V} \\left (A_{j, k} - \\frac{D_{j, j}^{(out)}D_{k, k}^{(in)}}{n_E} \\right ) \\; \\delta(c(\\nu_j), c(\\nu_k))\n\\end{align}\n\\qquad(14.15)\\]\n\n\n14.3.5.3 Agglomerative versus divisive algorithms\nConsider an algorithm designed to determine community structure based on modularity. A divisive algorithm might initially assign all nodes to a single community by setting \\(n_c = 1\\). An agglomerative algorithm might initially assign each node to its own community by setting \\(n_c = n_V\\).\nSee “Modularity (Networks) | Wikipedia” (2025) for further details. The modularity of a graph can be calculated in R by function igraph:modularity().",
    "crumbs": [
      "Graph Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Graph Theory for Machine Learning</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#algorithms-general",
    "href": "graph-theory.html#algorithms-general",
    "title": "14  Graph Theory for Machine Learning",
    "section": "14.4 Algorithms: General",
    "text": "14.4 Algorithms: General\n\n14.4.1 Page Rank\nQuestion: In a directed graph, which nodes have the most in-links?\nPageRank was the first algorithm used by Google to find relevant web pages in response to user queries. The algorithm determines the number and quality of links to a page under the assumption that more important websites are likely to receive more links from other websites. 9\nThis idea can be applied to any directed graph. Here’s an outline of a similar algorithm that assigns a score \\(s(\\nu)\\) to each node \\(\\nu\\) of directed graph \\(G = (V, E)\\) having \\(n_V\\) nodes. For each node \\(\\nu\\) the score is initially set to \\(s(\\nu) = \\frac{1}{n_V}\\). Then the algorithm iterates over the following steps: 10\n\nDistribute \\(s(\\nu)\\) equally among the out-neighbors of \\(\\nu\\).\nUpdate \\(s(\\nu)\\) to be the sum of all scores received from the in-neighbors of \\(\\nu\\).\nShrink toward uniformity: set \\(s(\\nu) \\leftarrow (1 - \\epsilon) s(\\nu) + \\epsilon \\frac{1}{n_V}\\), for some \\(\\epsilon \\approx 0.15\\).\n\nThe algorithm can also be expressed using the \\(n_V \\times n_V\\) matrix \\(T\\) to represent the distribution of each node’s score to the scores of its out-neighbors, so that\n\\[\n\\begin{align}\n  T_{j, k}  &=\n  \\begin{cases}\n    \\frac{1}{d_j} & \\text{if } \\nu_j \\text{ has a directed link to } \\nu_k \\\\\n    0 & \\text{ otherwise}\n  \\end{cases} \\\\ \\\\\n  &\\text{where} \\\\ \\\\\n  d_j &= \\text{number of out-neighbors of } \\nu_j\n\\end{align}\n\\qquad(14.16)\\]\nLet \\(s^{(i)}_\\bullet = (s^{(i)}_1, \\ldots, s^{(i)}_{n_V})\\) denote the vector of node-scores at the \\(i^{th}\\) iteration. Also let \\(1_\\bullet = (1, \\ldots, 1)\\) denote the vector of length \\(n_V\\) each of whose elements is equal to unity, so that \\(1_\\bullet \\otimes 1_\\bullet\\) denotes the \\(n_V \\times n_V\\) matrix each of whose elements is equal to unity. Then the algorithm can be represented as the following linear system, iterated over index \\(i\\).\n\\[\n\\begin{align}\n  s^{(0)}_\\bullet &= \\frac{1}{n_V} 1_\\bullet \\\\\n  s^{(i)}_\\bullet &= s^{(i-1)}_\\bullet \\; \\times \\; M \\\\\n  M &= (1 - \\epsilon) \\; T \\; + \\; \\frac{\\epsilon}{n_V} \\; 1_\\bullet \\otimes 1_\\bullet\n\\end{align}\n\\qquad(14.17)\\]\nThe matrix \\(T\\) is a Markov transition matrix: that is, the elements of each row are non-negative and sum to unity. Then there is at least one probability vector \\(\\pi_\\bullet\\) for which \\(\\pi_\\bullet \\times T = \\pi_\\bullet\\). Vector \\(\\pi_\\bullet\\) is thus a left eigenvector of \\(T\\) corresponding to an eigenvalue of 1, and is a stationary probability vector corresponding to \\(T\\). 11\nMatrix \\(M\\) is also a Markov transition matrix with its own stationary distribution, say \\(\\bar{s}_\\bullet\\). Moreover, \\(M\\) is irreducible, meaning that \\(M\\) assigns positive probability of reaching any node from any other node. Consequently \\(\\bar{s}_\\bullet\\) is the unique stationary distribution of \\(M\\). It also follows that iterating the above linear system gives a sequence of score vectors \\(s^{(i)}_\\bullet\\) that converge to \\(\\bar{s}_\\bullet\\).\n\\[\n\\begin{align}\n  \\lim_{i \\rightarrow \\infty}{s^{(i)}_\\bullet} &= \\bar{s}_\\bullet\n\\end{align}\n\\qquad(14.18)\\]\nIn 1999 Google noted that acceptable convergence of the PageRank algorithm typically required about 50 iterations for internet-sized graphs. 12\nThe algorithm is implemented in R as the function igraph::page_rank().\n\n\n14.4.2 Floyd-Warshall\nQuestion: In a directed weighted graph, what is the distance from one node to another: how long is the shortest path?\nSuppose we are given a directed weighted graph \\(G\\) having \\(n_V\\) nodes indexed as \\(\\{ \\nu_1, \\nu_2, \\ldots, \\nu_{n_V} \\}\\). For any path from node \\(\\nu_j\\) to node \\(\\nu_k\\), the length of the path is the sum of the weights of the edges making up the path.\nConsider the shortest of those paths from \\(\\nu_j\\) to \\(\\nu_k\\) that are restricted to intermediary nodes \\(N_m = \\{ \\nu_1, \\nu_2, \\ldots, \\nu_m \\}\\), that is, constrained to exclude intermediary nodes other than those in \\(N_m\\). If there is such a shortest path that moreover excludes node \\(\\nu_m\\), then it is also the shortest path restricted to intermediary nodes \\(N_{m - 1} = \\{ \\nu_1, \\nu_2, \\ldots, \\nu_{m - 1} \\}\\).\nOtherwise, if any shortest path includes node \\(\\nu_m\\), that path is the concatenation of: (1) a shortest path from \\(\\nu_j\\) to \\(\\nu_m\\) restricted to nodes \\(N_{m - 1}\\); and (2) a shortest path from \\(\\nu_m\\) to \\(\\nu_k\\) restricted to nodes \\(N_{m - 1}\\).\nIf node \\(\\nu_j\\) happens to be connected to \\(\\nu_k\\) by the edge \\(\\nu_j - \\nu_k\\), then that edge qualifies as the shortest path restricted to the empty set of intermediary nodes.\nWe now use these observations about intermediary nodes to construct an algorithm to determine the minimal distance from any node to any node. We will record these distances in the \\(n_v \\times n_V\\) matrix \\(D\\). 13\n\\[\n\\begin{align}\n  D_{j, k} &= \\text{distance from } \\nu_j \\text{ to } \\nu_k\n\\end{align}\n\\qquad(14.19)\\]\nWe set initial values to matrix \\(D\\) as follows.\n\\[\n\\begin{align}\n  D^{(0)}_{j, k} &=\n  \\begin{cases}\n    \\text{weight of edge } \\nu_j - \\nu_k & \\text{if that edge exists} \\\\\n    \\infty & \\text{otherwise}\n  \\end{cases}\n\\end{align}\n\\qquad(14.20)\\]\nWe now update these matrix values as follows.\n\\[\n\\begin{align}\n  D^{(i + 1)}_{j, k} &= \\min \\left \\{ D^{(i)}_{j, k} \\;,\\; \\{ D^{(i)}_{j, m} + D^{(i)}_{m, k} \\}_{m \\not \\in \\{ j, k \\}} \\right \\}\n\\end{align}\n\\qquad(14.21)\\]\nThe algorithm is implemented in R as the function Rfast::floyd().\n\n\n14.4.3 Dijkstra\nQuestion: In a weighted graph, how long is the shortest path between two given nodes, or between a given node and every other node?\n\\(G\\) is now an undirected, weighted graph, again having \\(n_V\\) nodes. A starting node \\(\\nu_1\\) is identified, but the remaining nodes will be later indexed as \\(\\{ \\nu_2, \\ldots, \\nu_{n_V} \\}\\) as part of the algorithm. The \\(n_v \\times n_V\\) distance matrix \\(D\\) is now symmetric. Our goal is to determine the values of the first row of \\(D\\) (or possibly an identified target cell in the first row).\nDijkstra’s algorithm starts with infinite distances and tries to improve them step by step: 14\n\nCreate a set \\(\\mathcal{U}\\) of unvisited nodes, with \\(\\mathcal{U}\\) initialized as \\(V\\), the set of all nodes.\nAssign an initial distance \\(D^{(0)}_{1, j}\\) from \\(\\nu_1\\) to every node \\(\\nu_j\\), with \\(D^{(0)}_{1, 1} = 0\\) and \\(D^{(0)}_{1, j} = \\infty\\) for \\(j \\ne 1\\).\nFrom the unvisited set, select the current node, \\(\\nu_i\\), to be the one with the smallest (finite) distance; initially (for \\(i = 1\\)), this is the starting node \\(\\nu_1\\) (having distance zero). If the unvisited set is empty, or contains only nodes with infinite distance (which are unreachable), then the algorithm terminates by skipping to step 6. If the only concern is the path to a target node, the algorithm terminates once the current node is the target node. Otherwise, the algorithm continues.\nNow examine nodes adjacent to \\(\\nu_i\\) that are still elements of \\(\\mathcal{U}\\). Calculate the distance of each adjacent node from \\(\\nu_1\\) through the current node \\(\\nu_i\\). Update the distance from \\(\\nu_1\\) to the adjacent node to be the minimum of the previously recorded distance and the new calculation.\nAfter considering all of the current node’s unvisited neighbors, the current node is removed from the unvisited set. Thus a visited node is never rechecked, which is correct because the distance recorded on the current node is minimal (as ensured in step 3), and thus final. Repeat from step 3.\nOnce the loop exits (steps 3–5), every visited node contains its shortest distance from the starting node.\n\nThe algorithm is implemented in R as the function igraph::distances().\n\n\n14.4.4 Brandes\nBrandes’ algorithm calculates the betweenness centrality of all nodes in a graph, with the aid of the following definitions. 15\nLet \\(\\mathcal{l}(s, v)\\) denote the length of a shortest path from node \\(s\\) to node \\(v\\). Note: Brandes’ algorithm assumes that all edge-weights are equal to unity (or share some other positive constant value).\nAlso let \\(\\mathcal{C}_s(v)\\) denote the neighbors of \\(v\\) that are closer to \\(s\\) than is \\(v\\).\n\\[\n\\begin{align}\n  \\mathcal{C}_s(v) &= \\left \\{ u \\in V :\n    (u-v) \\in E \\; \\wedge \\; \\mathcal{l}(s, u) &lt; \\mathcal{l}(s, v) \\right \\}\n\\end{align}\n\\qquad(14.22)\\]\nThe idea here is that for any given node \\(s\\), the remaining nodes can be partitioned based on their distance from \\(s\\). The partition is simplified by the assumption that all edges have the same weight (unity or some other positive constant). Node \\(v\\) belongs to the set of nodes that are the same distance from \\(s\\) as is \\(v\\). If node \\(u\\) is adjacent to \\(v\\) and is closer to \\(s\\) than is \\(v\\), then it must be just one unit closer and thus belong to that corresponding set of nodes.\nFor each vertex \\(s\\) the algorithm iterates over two stages: (1) shortest path determination ; and (2) back-propagation. The algorithm is implemented in R as the function igraph::betweenness().\n\n14.4.4.1 Single-source shortest path\nEquation 14.22 yields the following iterative formula for \\(\\sigma_{s, v}\\). 16\n\\[\n\\begin{align}\n  \\sigma_{s, v} &= \\sum_{u \\in \\mathcal{C}_s(v)} \\sigma_{s, u}\n\\end{align}\n\\qquad(14.23)\\]\nThat is, if node \\(v\\) is not adjacent to node \\(s\\) then any shortest path from \\(s\\) to \\(v\\) must have a member of \\(\\mathcal{C}_s(v)\\) as its penultimate node, and these penultimate nodes partition the shortest paths from \\(s\\) to \\(v\\). On the other hand, if \\((s-v)\\) is an edge then that edge is the unique shortest path from \\(s\\) to \\(v\\), in which case \\(\\sigma_{s, v} = 1\\).\n\n\n14.4.4.2 Backpropagation\nBrandes proved the following recursive formula for vertex dependencies:\n\\[\n\\begin{align}\n  \\delta_{s} (u) &= \\sum_{v : u \\in \\mathcal{C}_s(v)}\n    \\frac{\\sigma_{s, u}}{\\sigma_{s, v}} \\times (\\delta_s(v) + 1)\n\\end{align}\n\\qquad(14.24)\\]\nAccording to this formula the single dependency of \\(s\\) on a vertex \\(u\\) at distance \\(\\mathcal{l}(s, u)\\) is determined by the the set of nodes at the next greater distance. Furthermore, the order of summation is irrelevant, which enables calculations to start at the most distant set of nodes.",
    "crumbs": [
      "Graph Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Graph Theory for Machine Learning</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#algorithms-clustering",
    "href": "graph-theory.html#algorithms-clustering",
    "title": "14  Graph Theory for Machine Learning",
    "section": "14.5 Algorithms: Clustering",
    "text": "14.5 Algorithms: Clustering\n\n14.5.1 Girvan-Newman\nQuestion: How can one identify highly connected communities of nodes? 17\nIf a graph contains groups (communities) of nodes that are only loosely connected by a few inter-group edges, then all shortest paths between different communities must go along one of these few edges. Thus the edges connecting communities will have high edge betweenness. By removing these edges, the communities are separated from one another and so the underlying community structure of the graph is revealed.\nGirvan–Newman is a hierarchical, divisive algorithm that detects communities by iteratively removing a most central edge, and then recomputing the betweenness centrality of edges belonging to the component containing the removed edge. The connected components of the resulting graph are the communities.\nThe algorithm is as follows.\n\nCalculate the betweenness for all edges.\nRemove the edge with the highest betweenness.\nRecalculate betweennesses for all edges affected by the removal.\nRepeat from step 2 until no edges remain.\n\nThe algorithm is implemented in R as the function igraph::cluster_edge_betweenness().\n\n\n14.5.2 Spectral clustering\nSpectral clustering is another hierarchical, divisive method for community detection in an undirected graph. It is based on a spectral decomposition of the normalized Laplacian matrix. An outline of the iterative algorithm is as follows.\n\nCompute the adjacent \\((A)\\), degree \\((D)\\), and normalized Laplacian \\((\\tilde{L})\\) matrices, with\n\n\\[\n\\begin{align}\n  \\tilde{L} &= I - D^{-1/2} \\; A \\; D^{-1/2}\n\\end{align}\n\\qquad(14.25)\\]\nnoting that\n\\[\n\\begin{align}\n  \\tilde{L} \\times (D^{1/2} \\times 1_{\\bullet}) &= 0\n\\end{align}\n\\qquad(14.26)\\]\n\nCompute the eigenvector \\(x\\) corresponding to the second smallest eigenvalue of \\((\\tilde{L})\\).\nFor each node \\(\\nu_j\\) calculate the inner product \\(&lt;A_{j, \\bullet}, x&gt;\\) (the \\(x-\\)coordinate of \\(\\nu_j\\)).\nCluster nodes based on the sign (negative, or non-negative) of their \\(x-\\)coordinate.\n\nSee “Spectral Clustering | Wikipedia” (2025) for further details. The algorithm can be implemented in R as the function rSpectral::spectral_igraph_membership().\n\n\n14.5.3 Clauset-Newman-Moore\nThe Clauset-Newman-Moore (CNM) algorithm is a hierarchical agglomeration algorithm to find community structure in a graph. It does so by local (“greedy”) optimization. It greedily merges clusters to increase modularity, and halts when the increase falls under some threshold. It may be forced to track the hierarchy of clusters formed, and halt only when it has created a single cluster of the entire graph. See Clauset, Newman, and Moore (2004) for details. The algorithm is implemented in R as the function igraph::cluster_fast_greedy().\n\n\n14.5.4 Louvain\nThe Louvain method, like the Clauset-Newman-Moore (CNM) algorithm, uses local optimization of subgraph modularity to find community structure in a graph. Here’s an outline of the algorithm.\n\nPerform CNM until modularity is maximized.\nContract each cluster to a single new node.\nSet the edge-weight between new nodes to the sum of all old edge-weights between the old clusters.\nReturn to step 1 with the contracted graph.\n\nSee “Louvain Method | Wikipedia” (2025) for further details.\nLouvain produces non-overlapping communities: each node belongs to at most one community. This may be unrealistic in real-world applications. As an alternative, the Leiden algorithm can produce overlapping communities.\nThe Louvain method is implemented in R as the function igraph::cluster_louvain().\n\n\n14.5.5 Leiden\nThe Leiden method, developed in 2019 as a modification of the Louvain method, allows the identification of overlapping communities. See “Leiden Algorithm | Wikipedia” (2025) for details. The algorithm is implemented in R as the function igraph::cluster_leiden().\n\n\n\n\n“Adjacency Matrix | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Adjacency_matrix.\n\n\n“Betweenness Centrality | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Betweenness_centrality.\n\n\n“Brandes’ Algorithm | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Brandes%27_algorithm.\n\n\nClauset, Aaron, M. E. J. Newman, and Cristopher Moore. 2004. “Finding Community Structure in Very Large Networks.” Physical Review E. https://doi.org/https://doi.org/10.1103/PhysRevE.70.066111.\n\n\n“Component (Graph Theory) | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Component_(graph_theory).\n\n\n“Degree Matrix | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Degree_matrix.\n\n\n“Dijkstra’s Algorithm | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm.\n\n\n“Floyd–Warshall Algorithm | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm.\n\n\nFortunato, Santo. 2010. “Community Detection in Graphs.” Physics Reports, February. https://doi.org/10.1016/j.physrep.2009.11.002.\n\n\n“Girvan–Newman Algorithm | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm.\n\n\n“Glossary of Graph Theory | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Glossary_of_graph_theory.\n\n\n“Graph Theory | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Graph_theory.\n\n\n“Laplacian Matrix | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Laplacian_matrix.\n\n\n“Leiden Algorithm | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Leiden_algorithm.\n\n\n“Louvain Method | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Louvain_method.\n\n\n“Markov Chain | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Markov_chain.\n\n\n“Modularity (Networks) | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Modularity_(networks).\n\n\n“PageRank | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/PageRank.\n\n\n“Spectral Clustering | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Spectral_clustering.",
    "crumbs": [
      "Graph Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Graph Theory for Machine Learning</span>"
    ]
  },
  {
    "objectID": "graph-theory.html#footnotes",
    "href": "graph-theory.html#footnotes",
    "title": "14  Graph Theory for Machine Learning",
    "section": "",
    "text": "See “Graph Theory | Wikipedia” (2025).↩︎\nSee “Glossary of Graph Theory | Wikipedia” (2025).↩︎\nSee “Adjacency Matrix | Wikipedia” (2025).↩︎\nSee “Degree Matrix | Wikipedia” (2025).↩︎\nSee “Laplacian Matrix | Wikipedia” (2025).↩︎\nSee “Betweenness Centrality | Wikipedia” (2025).↩︎\nSee “Component (Graph Theory) | Wikipedia” (2025).↩︎\nSee Fortunato (2010).↩︎\nSee “PageRank | Wikipedia” (2025).↩︎\nWe assume each node has an edge to at least one other node. This can be enforced if needed. One possibility is to remove the set \\(\\mathcal{N}\\) of those nodes \\(\\mathcal{n}\\) having no out-link to any other node, and to also remove any edges of the form \\(\\nu - \\mathcal{n}\\) incident with nodes in \\(\\mathcal{N}\\). Another possibility is to change the set of out-nodes of node \\(\\mathcal{n} \\in \\mathcal{N}\\) from the empty set to the singleton set \\(\\{ \\mathcal{n} \\}\\).↩︎\nSee “Markov Chain | Wikipedia” (2025).↩︎\nSee “PageRank | Wikipedia” (2025).↩︎\nSee “Floyd–Warshall Algorithm | Wikipedia” (2025).↩︎\nSee “Dijkstra’s Algorithm | Wikipedia” (2025).↩︎\nSee “Brandes’ Algorithm | Wikipedia” (2025).↩︎\nThis formula assumes that if \\((u-v) \\in E\\), then the \\((u-v)\\) edge is among the shortest paths from \\(u\\) to \\(v\\). If all edges have the same weight, then the \\((u-v)\\) edge is the unique shortest path from \\(u\\) to \\(v\\).↩︎\nSee “Girvan–Newman Algorithm | Wikipedia” (2025).↩︎",
    "crumbs": [
      "Graph Theory",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Graph Theory for Machine Learning</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This online book is based on the preparatory part of a course on Machine Learning (ML) developed by former colleagues. The book aims to provide current and aspiring data scientists with a review of (or introduction to) key ideas and methods from statistics and linear algebra, along with certain types of data, namely text, time series, and network graphs. The book also aims to follow the content and spirit of exploratory data analysis (EDA).\nInstructors, especially those preparing students for a Masters degree in data science, are invited to copy and modify portions of the book for their own purposes.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "text-analysis.html",
    "href": "text-analysis.html",
    "title": "8  Text Analysis",
    "section": "",
    "text": "8.1 Introduction\nThis session highlights some basic ideas and methods that underpin a rapidly advancing field. We follow the online book by Silge and Robinson cited below and use their R package tidytext. The authors emphasize the “tidy” formatting of data (i.e., as key-value pairs) along with a set of R packages sharing this approach, collectively called the R tidyverse.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#text-example",
    "href": "text-analysis.html#text-example",
    "title": "8  Text Analysis",
    "section": "8.2 Text Example",
    "text": "8.2 Text Example\nToward the end of Shakespeare’s play “Macbeth”, the protagonist proclaims:\n\n\n[1] \"Life's but a walking shadow, a poor player \"    \n[2] \"That struts and frets his hour upon the stage, \"\n[3] \"And then is heard no more: it is a tale \"       \n[4] \"Told by an idiot, full of sound and fury, \"     \n[5] \"Signifying nothing.\"                            \n\n\n(Source: “Macbeth”, Act V, Scene V, lines 24-28.)\nFor purposes of technical analysis we break these flowing lines into a table of words. We begin as follows.\n\n\nShow the code\nsf_line_tbl &lt;- tibble::tibble(\n  l_idx = 24:28, \n  line  = sound_fury\n)\nsf_line_tbl\n\n\n# A tibble: 5 × 2\n  l_idx line                                             \n  &lt;int&gt; &lt;chr&gt;                                            \n1    24 \"Life's but a walking shadow, a poor player \"    \n2    25 \"That struts and frets his hour upon the stage, \"\n3    26 \"And then is heard no more: it is a tale \"       \n4    27 \"Told by an idiot, full of sound and fury, \"     \n5    28 \"Signifying nothing.\"                            \n\n\nThe table above merely identifies the original line number of each line. The next step is to break each line into a sequence of “tokens”, where a token is a meaningful unit of text (such as a word) to be used as the unit of analysis. (“Tokenization” is the process of splitting text into tokens.) Applying tidytext::unnest_tokens() to the data table above, we obtain the following table, with just one token (word) per row.\n\n\nShow the code\nsf_token_tbl &lt;- sf_line_tbl |&gt; \n  tidytext::unnest_tokens(\n    input  = \"line\", \n    output = \"word\"\n  )\nsf_token_tbl\n\n\n# A tibble: 38 × 2\n   l_idx word   \n   &lt;int&gt; &lt;chr&gt;  \n 1    24 life's \n 2    24 but    \n 3    24 a      \n 4    24 walking\n 5    24 shadow \n 6    24 a      \n 7    24 poor   \n 8    24 player \n 9    25 that   \n10    25 struts \n# ℹ 28 more rows\n\n\nThe next step is to remove so-called stop-words, that is, articles (“a”, “the”, …), connectors (“and”, “or”, …) and other words that provide structure to a sentence but otherwise carry little information. The tidytext package contains a data frame, stop_words, of such words, which enables us to remove them from the above table of tokens.\n\n\nShow the code\nsf_tokens_xsw &lt;- sf_token_tbl |&gt; \n  anti_join(\n    y  = tidytext::stop_words, \n    by = \"word\"\n  )\nsf_tokens_xsw\n\n\n# A tibble: 16 × 2\n   l_idx word      \n   &lt;int&gt; &lt;chr&gt;     \n 1    24 life's    \n 2    24 walking   \n 3    24 shadow    \n 4    24 poor      \n 5    24 player    \n 6    25 struts    \n 7    25 frets     \n 8    25 hour      \n 9    25 stage     \n10    26 heard     \n11    26 tale      \n12    27 told      \n13    27 idiot     \n14    27 sound     \n15    27 fury      \n16    28 signifying",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#larger-text-examples",
    "href": "text-analysis.html#larger-text-examples",
    "title": "8  Text Analysis",
    "section": "8.3 Larger Text Examples",
    "text": "8.3 Larger Text Examples\nWe’ll use larger bodies of text via the following R packages.\n\n8.3.1 Jane Austen’s novels\n\nPackage janeaustenr: Jane Austen (1775-1817) completed 6 novels, which the function austen_books() returns as a data frame with 2 columns: the text of the novels divided into strings (each approximating a line of printed text), and book, which gives the titles of the novels (in order of publication) as a factor.\n\nHere are the number of strings per book.\n\n\n# A tibble: 6 × 2\n  book                n_strings\n  &lt;fct&gt;                   &lt;int&gt;\n1 Sense & Sensibility     12624\n2 Pride & Prejudice       13030\n3 Mansfield Park          15349\n4 Emma                    16235\n5 Northanger Abbey         7856\n6 Persuasion               8328\n\n\nHere are the ten words used most frequently across these novels (excluding stop-words).\n\n\n\n\nTable 8.1: Jane Austen: frequent (non-stop) words\n\n\n\n\nJane Austen: 10 most common (non-stop) words\n\n\nword\ncount\n\n\n\n\nmiss\n1855\n\n\ntime\n1337\n\n\nfanny\n862\n\n\ndear\n822\n\n\nlady\n817\n\n\nsir\n806\n\n\nday\n797\n\n\nemma\n787\n\n\nsister\n727\n\n\nhouse\n699\n\n\n\n\n\n\n\n\n\n\n8.3.2 The Gutenberg Project\n\nPackage gutenbergr: Enables the user to download and process public domain works in the Project Gutenberg collection.\n\nThe collection boasts over 75000 free electronic books. The data frame gutenberg_subjects uses the Library of Congress Classifications (lcc) and Library of Congress Subject Headings (lcsh) to categorize topics included in the collection. The package offers this and other such metadata to facilitate searching for desired works.\nAs a contrast with Jane Austen, here are some well-known science fiction novels of H.G. Wells (1866-1946).\n\n\n# A tibble: 4 × 2\n     id title                      \n  &lt;int&gt; &lt;chr&gt;                      \n1    35 The Time Machine           \n2    36 The War of the Worlds      \n3  5230 The Invisible Man          \n4   159 The Island of Doctor Moreau\n\n\nAmong these novels, here are the most frequently used words (again excluding stop-words).\n\n\n\n\nTable 8.2: H.G. Wells: frequent (non-stop) words\n\n\n\n\nH.G. Wells: 10 most common (non-stop) words\n\n\nword\ncount\n\n\n\n\nNA\n3524\n\n\ntime\n454\n\n\npeople\n302\n\n\ndoor\n260\n\n\nheard\n249\n\n\nblack\n232\n\n\nstood\n229\n\n\nwhite\n222\n\n\nhand\n218\n\n\nkemp\n213",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#class-exercise",
    "href": "text-analysis.html#class-exercise",
    "title": "8  Text Analysis",
    "section": "8.4 Class Exercise",
    "text": "8.4 Class Exercise\nTeam up with a classmate and devise a way to compare word frequencies in the novels of Jane Austen and H.G. Wells, respectively. Share with the class your comparison of just the top 10 words used by each author. Propose a method for comparing all the words used by each author. Take 20 minutes to prepare to report to the class.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#tf-idf-term-frequency---inverse-doc-frequency",
    "href": "text-analysis.html#tf-idf-term-frequency---inverse-doc-frequency",
    "title": "8  Text Analysis",
    "section": "8.5 TF-IDF: Term Frequency - Inverse Doc Frequency",
    "text": "8.5 TF-IDF: Term Frequency - Inverse Doc Frequency\nCan the number of times each word appears in a document be used to indicate what the document is about? On the one hand, the number of occurrences of a given word in a given document might indicate the importance of the word within the document. On the other hand, words that commonly occur in most documents are unlikely to distinguish the key ideas in a given document.\nWe’ve already introduced one way to separate the wheat from the chaff: remove stop-words. Another approach, called tf-idf, is to multiply a term’s relative frequency (tf) in a selected document by its inverse document frequency (idf) with respect to a collection or corpus of documents. That is, the relative frequency of a term \\(t_0\\) in a given document \\(d_0\\) is the number of occurrences \\(\\mathcal{n}(t_0, d_0)\\) of the given term divided by the number of occurrences of all terms.\n\\[\n\\begin{align}\n  tf(t_0, d_0) &= \\frac{\\mathcal{n}(t_0, d_0)}{\\sum_{t \\in d_0}\\mathcal{n}(t, d_0)}\n\\end{align}\n\\qquad(8.1)\\]\nAs for inverse document frequency (idf), there are several alternative definitions. Here’s the definition we’ll use.\n\\[\n\\begin{align}\n  idf(t, \\mathcal{D}) &= \\log_e \\left( \\frac{| \\mathcal{D} |}{| \\mathcal{D}_t |} \\right) \\\\\n  \\\\\n  & t = \\text{term} \\\\\n  & \\mathcal{D} = \\text{corpus of documents } \\\\\n  & \\mathcal{D}_t = \\{ d \\in \\mathcal{D} : t \\in d  \\}\n\\end{align}\n\\qquad(8.2)\\]\nExample: let \\(\\mathcal{D}\\) denote the set of Jane Austen’s 6 novels, and let each novel take its turn as the document \\(d_0\\) of interest. For each book, the most distinctive word (that maximizes tf-idf) is as follows.\n\n\n\n\nTable 8.3: Jane Austen: max tf-idf word per book\n\n\n\n\nMax tf-idf word per book\n\n\nbook\nword\nn\ntf\nidf\ntf_idf\n\n\n\n\nSense & Sensibility\nelinor\n623\n0.005\n1.792\n0.009\n\n\nPride & Prejudice\ndarcy\n373\n0.003\n1.792\n0.005\n\n\nMansfield Park\ncrawford\n493\n0.003\n1.792\n0.006\n\n\nEmma\nemma\n786\n0.005\n1.099\n0.005\n\n\nNorthanger Abbey\ntilney\n196\n0.003\n1.792\n0.005\n\n\nPersuasion\nelliot\n254\n0.003\n1.792\n0.005",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#document-term-matrix-dtm",
    "href": "text-analysis.html#document-term-matrix-dtm",
    "title": "8  Text Analysis",
    "section": "8.6 Document-Term Matrix (DTM)",
    "text": "8.6 Document-Term Matrix (DTM)\nSo far, we’ve been analyzing text arranged in the tidy text format: a table in which each row pertains to a unique (document, token) pair. The tidytext::unnest_tokens() function counts the number of occurrences of each such pair. Tables in this format can be explored and visualized using the suite of tidy tools, including packages dplyr, tidyr, and ggplot2.\nAside from the tidytext package, most R tools for natural language processing aren’t compatible with this format. The CRAN Task View for Natural Language Processing lists packages that take other structures of input and provide non-tidy outputs. These packages are very useful in text mining applications, and many existing text datasets are structured according to these non-tidy formats.\nOne of the most common structures that text mining packages work with is the document-term matrix (or DTM). This is a matrix where:\n\neach row represents one document (such as a book or article),\neach column represents one term, and\neach value (typically) contains the number of appearances of that term in that document.\n\nSince most (document, term) pairings have zero occurrences, DTMs are usually implemented as sparse matrices. These objects can be treated as matrices (enabling one to access particular rows and columns), but are stored in a more efficient format.\nDTM objects cannot be used directly with tidy tools, and tidy data frames cannot be used as input for most text mining packages. Therefore, the tidytext package provides two functions that convert between the two formats.\n\ntidy() turns a document-term matrix into a tidy data frame. This function comes from the broom package, which provides similar tidying functions for many statistical models and objects.\ncast() turns a tidy one-term-per-row data frame into a matrix. Package tidytext provides three variations of this function, each converting to a different type of matrix:\n\ncast_sparse() (converting to a sparse matrix from the Matrix package);\ncast_dtm() (converting to a DocumentTermMatrix object from package tm); and\ncast_dfm() (converting to a dfm object from quanteda).\n\n\nA widely used implementation of DTMs in R is the DocumentTermMatrix class in the tm package. Many available text mining datasets are provided in this format.\n\n8.6.1 Example: Associated Press articles\nAs an example, here’s a description of Associated Press newspaper articles included as a DTM in the topicmodels package.\n\n\nShow the code\nif (topicmodels_loaded) {\n  data(\"AssociatedPress\", package = \"topicmodels\")\n}\n\n# AssociatedPress\n# &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt;\n# Non-/sparse entries: 302031/23220327\n# Sparsity           : 99%\n# Maximal term length: 18\n# Weighting          : term frequency (tf)\n\n\nThis Associated Press DTM consists of 2246 documents (rows) and 10473 terms (columns), with 99% of the potential (document, term) pairings having zero instances (and thus excluded from the sparse matrix).\n\n\n8.6.2 Example: inaugural addresses of US presidents\nThe inaugural addresses of US presidents, provided by package quanteda, is an interesting example of a document-features matrix (DFM), a variant of a DTM. Here are the identifying variables for the first 6 presidential addresses.\n\n\nShow the code\nutils::data(\"data_corpus_inaugural\", package = \"quanteda\")\nutils::head(docvars(data_corpus_inaugural), 6)\n\n\n  Year  President FirstName                 Party\n1 1789 Washington    George                  none\n2 1793 Washington    George                  none\n3 1797      Adams      John            Federalist\n4 1801  Jefferson    Thomas Democratic-Republican\n5 1805  Jefferson    Thomas Democratic-Republican\n6 1809    Madison     James Democratic-Republican\n\n\nHere is a list of tokens from the addresses given in 1861, 1933, and 1961.\n\n\nShow the code\nsome_presidential_tokens &lt;- data_corpus_inaugural |&gt; \n  corpus_subset(Year %in% c(1861L, 1933L, 1961L)) |&gt; \n  tokens()\nsome_presidential_tokens\n\n\nTokens consisting of 3 documents and 4 docvars.\n1861-Lincoln :\n [1] \"Fellow-Citizens\" \"of\"              \"the\"             \"United\"         \n [5] \"States\"          \":\"               \"In\"              \"compliance\"     \n [9] \"with\"            \"a\"               \"custom\"          \"as\"             \n[ ... and 3,987 more ]\n\n1933-Roosevelt :\n [1] \"I\"         \"am\"        \"certain\"   \"that\"      \"my\"        \"fellow\"   \n [7] \"Americans\" \"expect\"    \"that\"      \"on\"        \"my\"        \"induction\"\n[ ... and 2,045 more ]\n\n1961-Kennedy :\n [1] \"Vice\"      \"President\" \"Johnson\"   \",\"         \"Mr\"        \".\"        \n [7] \"Speaker\"   \",\"         \"Mr\"        \".\"         \"Chief\"     \"Justice\"  \n[ ... and 1,529 more ]\n\n\nHere is a rendering of this list of tokens as a document-feature matrix (DFM).\n\n\nShow the code\npresidential_dfm &lt;- some_presidential_tokens |&gt; \n  quanteda::dfm()\npresidential_dfm\n\n\nDocument-feature matrix of: 3 documents, 1,746 features (56.62% sparse) and 4 docvars.\n                features\ndocs             fellow-citizens  of the united states : in compliance with  a\n  1861-Lincoln                 1 146 256      5     19 5 77          1   20 56\n  1933-Roosevelt               0 109 130      2      3 0 44          0   13 38\n  1961-Kennedy                 0  65  86      2      2 4 26          0    5 29\n[ reached max_nfeat ... 1,736 more features ]\n\n\nWe now reconfigure the DFM as a tidy data frame (tibble).\n\n\nShow the code\npresidential_tbl &lt;- presidential_dfm |&gt; \n  tidytext::tidy()\npresidential_tbl\n\n\n# A tibble: 2,272 × 3\n   document       term            count\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1 1861-Lincoln   fellow-citizens     1\n 2 1861-Lincoln   of                146\n 3 1933-Roosevelt of                109\n 4 1961-Kennedy   of                 65\n 5 1861-Lincoln   the               256\n 6 1933-Roosevelt the               130\n 7 1961-Kennedy   the                86\n 8 1861-Lincoln   united              5\n 9 1933-Roosevelt united              2\n10 1961-Kennedy   united              2\n# ℹ 2,262 more rows\n\n\nWe can now use tidytext::bind_tf_idf() to determine the words that most distinguish the three presidential addresses.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#topic-models",
    "href": "text-analysis.html#topic-models",
    "title": "8  Text Analysis",
    "section": "8.7 Topic Models",
    "text": "8.7 Topic Models\nText analysis methods can be applied to a variety of document types, including books, speeches, blog posts, news articles, and so on. Sometimes we can divide a collection of documents into natural groups to be analyzed separately. We can also use topic modeling to construct such groups. Topic modeling is the unsupervised categorization of documents, similar to clustering of numeric data.\n\n8.7.1 Latent Dirichlet allocation (LDA)\nLatent Dirichlet allocation (LDA) is a popular method for fitting topic models, and is guided by two principles.\n\nEvery document is a mixture of topics. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”\nEvery topic is a mixture of words. For example, consider a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.\n\nThis approach allows the constructed groups to overlap, similar to the soft clustering of numeric data.\n\n\n8.7.2 Example: Associated Press articles\nTo illustrate, we’ll apply function LDA() to the data set (DTM) AssociatedPress, both provided by the topicmodels package. The DTM is a collection of 2246 news articles from an American news agency, mostly published around 1988. For purposes of illustration we’ll specify a two-topic model, as follows.1\n\n\nShow the code\nif (topicmodels_loaded) {\n  ap_lda &lt;- AssociatedPress |&gt; \n    LDA(\n      k = 2, \n      # set a seed so that the output of the model is predictable\n      control = list(seed = 1234)\n    )\n  ap_lda\n}\n\n\nA LDA_VEM topic model with 2 topics.\n\n\nWe now construct (topic, term) probabilities (called \\(\\beta\\) in the LDA literature).\n\n\n# A tibble: 20,946 × 3\n   topic term           beta\n   &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     1 aaron      1.69e-12\n 2     2 aaron      3.90e- 5\n 3     1 abandon    2.65e- 5\n 4     2 abandon    3.99e- 5\n 5     1 abandoned  1.39e- 4\n 6     2 abandoned  5.88e- 5\n 7     1 abandoning 2.45e-33\n 8     2 abandoning 2.34e- 5\n 9     1 abbott     2.13e- 6\n10     2 abbott     2.97e- 5\n# ℹ 20,936 more rows\n\n\nHere are the most probable terms for each of the two constructed topics, along with their probabilities (beta), shown as a bar chart.\n\n\n\n\n\n\n\n\nFigure 8.1: AP articles: most probable terms by topic\n\n\n\n\n\nTopics 1 and 2 seem to pertain to business and politics, respectively, although “new” and “people” are prominent terms for both topics.\nAnother way to compare topics 1 and 2 is to examine the terms shared by the two topics and then find the terms having the biggest disparity in (topic, term) probability (beta). Here’s a bar chart showing the more prominent differences, expressed as\n\\[\n\\begin{align}\n  \\log_2 \\left( \\frac{\\beta_2}{\\beta_1} \\right)\n\\end{align}\n\\qquad(8.3)\\]\nrestricting the set of terms to those assigned to both topics \\((\\min(\\beta_1, \\beta_2) &gt; 0)\\) with at least one of them exceeding a probability threshhold, say \\((\\max(\\beta_1, \\beta_2) &gt; 0.001)\\).\n\n\n\n\n\n\n\n\nFigure 8.2: Log2 ratio of beta in topic 2 / topic 1\n\n\n\n\n\nThis figure further supports the earlier conjecture that topics 1 and 2 pertain to business and politics, respectively.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#additional-aspects-of-text-analysis",
    "href": "text-analysis.html#additional-aspects-of-text-analysis",
    "title": "8  Text Analysis",
    "section": "8.8 Additional Aspects of Text Analysis",
    "text": "8.8 Additional Aspects of Text Analysis\nWe’ve only touched on a few basic ideas and methods underpinning text analysis. Additional topics (on both the analysis and generation of text) include the following (which vary in complexity).\n\nn-grams: phrases of \\(n\\) consecutive words\nword networks (as graphs)\nsentiment analysis\nclustering, categorization, and prediction\nword embedding (as real-valued vectors)\nspecialized tokenizers, stemming\nnon-English human languages (including machine translation)\nlarge language models (LLMs)",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#team-exercises",
    "href": "text-analysis.html#team-exercises",
    "title": "8  Text Analysis",
    "section": "8.9 Team Exercises",
    "text": "8.9 Team Exercises\n\nAs a follow-up to the class exercise, propose a way to compare the vocabularies of Austen and Wells. What words are shared most? Least? Should stop-words be excluded? Express your proposal as pseudo-code.\nWe presented a table showing the word whose tf-idf is maximum for each of Jane Austen’s novels. Extend this comparison to show the words having the topmost tf-idf values. How would you present this comparison as a table? As a figure?\nFollowing the example of a document-feature matrix (DFM), extract the inaugural addresses of 1861, 1933, and 1961 from quanteda::data_corpus_inaugural. For each token in each address, calculate its tf-idf to determine the tokens that most distinguish the three addresses.\nIn the preceding exercise, the meta-data give us the name of the speaker and the year of the address. How would you use that knowledge to evaluate a topic-modeling algorithm applied to the inaugural addresses? Time permitting, conduct such an evaluation of a topic-modeling method based on the topicmodels R package.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#resources",
    "href": "text-analysis.html#resources",
    "title": "8  Text Analysis",
    "section": "8.10 Resources",
    "text": "8.10 Resources\nText Mining with R: A Tidy Approach by Silge and Robinson\ntf–idf - Wikipedia\nCRAN Task View for Natural Language Processing\nIntroduction to the tm Package: Text Mining in R\nQuantitative Analysis of Textual Data • quanteda\nLatent Dirichlet Allocation by David Blei, Andrew Ng, and Michael Jordan. JMLR (2003)",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "text-analysis.html#footnotes",
    "href": "text-analysis.html#footnotes",
    "title": "8  Text Analysis",
    "section": "",
    "text": "Function LDA() in package topicmodels returns a topic model of class “LDA_VEM”. LDA denotes latent Dirichlet allocation. VEM denotes the Variational Expectation Maximization (EM) algorithm.↩︎",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Analysis</span>"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "",
    "text": "3.1 Initial Remarks\nIn the context of machine learning, “unsupervised learning” encompasses algorithms designed to seek patterns in data, with minimal required guidance from the user, and without the benefit of a response variable (or a set of observation “labels”) to be approximated by functions of predictor variables. Most but not all of these algorithms are designed for high-dimensional data, that is, data with many variables (columns).\nThis approach is in the spirit of exploratory data analysis (EDA), which is practically forced upon us when we come across a new data set (and can remain essential throughout the life-cycle of a project). On the one hand, EDA has no sharp measures of success that we are driven to optimize, so one might say that we’re strolling through the data, taking in the scenery. On the other hand, we are looking for unanticipated patterns in the data, and therefore it is useful to record just what patterns we do anticipate, conjecture, or wonder about.\nIn our previous sessions we have reviewed some ways of looking at one or two variables at a time. When we are presented with a new data set having many variables, it is useful to simplify the data in some way to help us form a first impression.\nOne way is to look for a few functions of the many variables that somehow carry much of the original information. The prime method for doing so is principal component analysis (PCA), where the functions are linear and capture much of the variation in the data. We’ll come back to PCA in subsequent sessions.\nIn this session we’ll discuss another way to simplify the data, namely, to group observations having similar profiles (patterns in the values of the variables). Such clustering methods are currently of great interest in both science and industry, giving rise to a number of new clustering methods.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#us-colleges",
    "href": "clustering.html#us-colleges",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "3.2 US Colleges",
    "text": "3.2 US Colleges\nTo illustrate ideas we’ll use data about US universities and colleges from 1995, described in the book, An Introduction to Statistical Learning with applications in R (ISLR). The data are recorded within the R package ISLR2 as ISLR2::College. From the R command help(\"College\") we see that the data consist of 777 observations (data rows), and obtain the following description of the variables (data columns).\n\n\n\n\nTable 3.1: Variables in the ISLR2::College data set\n\n\n\n\nUS college variables (1995 issue of USNWR)\n\n\nvariable\ndescription\n\n\n\n\ncollege_name\nName of the college or university\n\n\nPrivate\nNo or Yes indicating private or public\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applications accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nPct. new students from top 10% of H.S. class\n\n\nTop25perc\nPct. new students from top 25% of H.S. class\n\n\nF.Undergrad\nNumber of fulltime undergraduates\n\n\nP.Undergrad\nNumber of parttime undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPct. of faculty with PhD’s\n\n\nTerminal\nPct. of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPct. alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#class-exercise",
    "href": "clustering.html#class-exercise",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "3.3 Class Exercise",
    "text": "3.3 Class Exercise\nTeam up with a classmate and make your own copy of the ISLR2::College data. Record your questions and conjectures about the data. Which of these could be addressed by the set of data variables? Take 15 minutes to prepare to report out to the class.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#grouping-variables",
    "href": "clustering.html#grouping-variables",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "3.4 Grouping Variables",
    "text": "3.4 Grouping Variables\nClustering algorithms construct groups of observations based solely on statistical information obtained from a given data set, with no reliance on the meaning or importance of the data variables.\nA complementary way to form groups is to use so-called “grouping variables” that we select or construct from the data. Often, the grouping variables are evident from the questions we’re trying to answer, e.g., to compare spending patterns among different socio-economic groups.\nWhen we want to construct groups (clusters) of observations as a means of exploring the data, we should consider what would make one grouping (clustering) more or less effective than another for this purpose. We’ll return to this point.\n\n3.4.1 Example: college data grouping variables\nTo help us consider the goals and criteria of clustering (grouping), let’s return to the US college data and choose a few variables with which to group the data.\nConsider the distinction between more expensive versus less expensive schools as indicated by variable Expend (instructional expenditure per student). We might ask, “Are the more expensive schools able to attract more of the top students?” So let’s also use the variable Top10perc (percent of new students coming from the top 10% of their high school class). We’ll simply split each variable at its median value, creating two binary variables, which we’ll use as our grouping variables.\nBefore proceeding, let’s examine the distribution of these variables via density estimates and a scatter plot.\n\n\n\n\n\n\n\n\nFigure 3.1: Distribution of variables Top10perc and Expend\n\n\n\n\n\nWe see that Top10perc and Expend are positively correlated, and have relatively few departures from a monotonic relationship. Also note that Expend is measured (in US dollars) on a very different scale from Top10perc.\nNow let’s look at the two variables more closely, one at a time.\n\n\n\n\n\n\n\n\nFigure 3.2: Pct of new students per college from top 10% in their HS\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Instructional expenditure per student\n\n\n\n\n\nThe figures above show decent variation on either side of the median value. Therefore cutting each variable at its median value seems reasonable, as long as we bear in mind that each distribution has a right tail (a quite long tail for Expend).\nFor each of the variables shown we create a binary variable having two levels, “lwr” and “upr”, meaning an observation is either below the median value, or else is no less than the median value. We then form 4 groups, each a combination of levels of the 2 binary variables, which we label \\(g_{00}, g_{01}, g_{10}, g_{11}\\) to denote the binary factors based on Top10perc, and Expend (in that order), with {“lwr”, “upr”} coded as {0, 1}.\nTo check this grouping of the data let’s first select the variables from which we defined the 4 groups and examine the scatter diagram, now using the color and shape of each data point to distinguish the different groups.\n\n\n\n\n\n\n\n\nFigure 3.4: (Top10perc, Expend): binary grouping\n\n\n\n\n\nThe figure above confirms that we have constructed the binary grouping variables as we intended.\nThe table below shows the mean per group of the underlying grouping variables.\nNote: we’ve calculated the mean per group of all the numeric variables. We’ll return to those calculations in the next section.\n\n\n\n\nTable 3.2: (Top10perc, Expend): mean values per group\n\n\n\n\nMean values per group of underlying grouping variables\n\n\ngroup\n# obs\nTop10perc\nExpend\n\n\n\n\ng_00\n264\n14.0\n6371.7\n\n\ng_01\n102\n15.3\n10081.8\n\n\ng_10\n124\n31.8\n7078.1\n\n\ng_11\n287\n42.6\n13650.9\n\n\n\n\n\n\n\n\n\n\n3.4.2 Measures of clustering effectiveness\nIn the context of clustering algorithms, groups are not given but are rather to be calculated from the data. But under what criteria? Broadly speaking we want the points (each an observation of several numeric variables) to be close together within each cluster (group), and we want the clusters to be well separated from one another.\nSo how well do the groups constructed above perform against these criteria? We want points within each cluster (group) to be close to one another, but how should we measure distance between points? Euclidean distance springs to mind, but there is a catch: the numeric variables are recorded on differing scales.\n\n\n3.4.3 Standardizing variables\nThe following table lists the mean, standard deviation, and coefficient of variation \\((cv = \\frac{sd}{mean})\\) per variable. The variables having the largest mean value are listed first.\n\n\n\n\nTable 3.3: College variables: mean, sd, and cv = sd/mean\n\n\n\n\nMean, SD, and CV = SD/Mean\n\n\nvariable\nmean\nsd\ncv\n\n\n\n\nOutstate\n10440.7\n4023.0\n0.4\n\n\nExpend\n9660.2\n5221.8\n0.5\n\n\nRoom.Board\n4357.5\n1096.7\n0.3\n\n\nF.Undergrad\n3699.9\n4850.4\n1.3\n\n\nApps\n3001.6\n3870.2\n1.3\n\n\nAccept\n2018.8\n2451.1\n1.2\n\n\nPersonal\n1340.6\n677.1\n0.5\n\n\nP.Undergrad\n855.3\n1522.4\n1.8\n\n\nEnroll\n780.0\n929.2\n1.2\n\n\nBooks\n549.4\n165.1\n0.3\n\n\nTerminal\n79.7\n14.7\n0.2\n\n\nPhD\n72.7\n16.3\n0.2\n\n\nGrad.Rate\n65.5\n17.2\n0.3\n\n\nTop25perc\n55.8\n19.8\n0.4\n\n\nTop10perc\n27.6\n17.6\n0.6\n\n\nperc.alumni\n22.7\n12.4\n0.5\n\n\nS.F.Ratio\n14.1\n4.0\n0.3\n\n\n\n\n\n\n\n\nThe financial variables are measured in thousands of dollars, whereas the percentages merely range from 0 to 100. The percentages thus contribute little to the overall Euclidean distance from a point (vector of observed numeric values) to a mean vector. Consequently the percentages will have little influence on calculations of clustering effectiveness based on Euclidean distances.\nThis issue can be addressed in a few different ways. For now we standardize each numeric variable: subtract the overall mean of the variable (across all observations) and then divide by the corresponding standard deviation of the variable. This will make the respective scales of the variables more compatible.\n\n\n3.4.4 Within-group distances in standard units\nHaving standardized the college numeric variables we now calculate Euclidean distances. That is, within each group we calculate the distance of each point to the group mean vector. This vector of point-minus-mean is referred to as “error” (more precisely, “mean deviation”) in statistical parlance. We calculate the squared Euclidean length of this vector, and sum these squared distances across all points in the group. The result is called the (within-group) sum of squared errors (SSE). Here are the calculated SSE values (and related statistics) per group.\n\n\n\n\nTable 3.4: Point-center distances per group\n\n\n\n\nPoint-center distances per group\n\n\ngroup\ncount\ndf\nRMSE\nMSE\nSSE\n\n\n\n\ng_00\n264\n247\n3.2\n10.1\n2493.5\n\n\ng_01\n102\n85\n4.0\n16.3\n1386.6\n\n\ng_10\n124\n107\n3.8\n14.6\n1560.9\n\n\ng_11\n287\n270\n4.3\n18.5\n4992.3\n\n\n\n\n\n\n\n\nThe columns of the table are as follows.\n\ngroup: the label of each group\ncount: the number of observations (data rows)\ndf: count minus the number of numeric values (17)\nRMSE: \\(\\surd{\\text{MSE}}\\)\nMSE: SSE / df\nSSE: sum of squared point-center distances\n\nThe RMSE (root mean squared error) is a measure of the typical length of the point-center distances within each group\nThe sum of SSE across groups (the “total within-group SSE”) turns out to be 10433. This value serves as a reference point for other groupings (clusterings) of observations.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#k-means-clustering",
    "href": "clustering.html#k-means-clustering",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "3.5 K-means Clustering",
    "text": "3.5 K-means Clustering\nWe’ll start our discussion of clustering using a function in the R stats package, namely stats::kmeans(). The function requires us to specify either the desired number \\(K\\) of clusters (groups) of observations to be formed, or else to provide an initial set of \\(K\\) cluster centers. If we merely provide the desired number \\(K\\) of clusters, kmeans() randomly selects \\(K\\) data rows (observations) as the initial set of cluster centers.\nGiven a set of cluster centers the algorithm iteratively searches for a better set, meaning a set having a smaller “within-cluster sum of squares”. In broad terms, this is done in two steps.\n\nAssign: the algorithm assigns each data point to the nearest center (in Euclidean distance) thereby partitioning the data points into \\(K\\) clusters.\nUpdate: For each cluster of data points, the algorihtm calculates a new cluster center, namely, the mean vector of the data points within the cluster. This yields a new set of cluster centers.\n\nConvergence criteria: For each cluster the algorithm adds up the (point, center) squared distances. Those \\(K\\) sums of squares per cluster are summed acrosss clusters to form the “total within-cluster sum of squares (WCSS)” (previously referred to as the “total within-group SSE”). In addition the algorithm records the grand mean vector, the average across the entire data set and forms a weighted sum of the squared (cluster-center, grand-mean) Euclidean distances, which is called the “between-cluster sum of squares (BCSS)”.1 The algorithm concludes the search when cluster-membership no longer changes, or when the decrease in WCSS is sufficiently small.\n\n3.5.1 Results for specified inital cluster centers\nWe now provide the kmeans() function with cluster centers in the form of the group means calculated above.2\nThe resulting clusters are shown in the scatter diagrams below, where for purposes of comparison we have projected the clusters onto the grouping variables we previously adopted.\n\n\n\n\n\n\n\n\nFigure 3.5: (Top10perc, Expend): clusters identified by kmeans()\n\n\n\n\n\n\n\n3.5.2 Jaccard similarity\nThe figure above shows that the kmeans() clusters are distinct from but similar to the groups formed above whose mean values served as initial cluster centers. Here are the number of observations belonging to each combination of \\(g_{x,y}\\) group and kmeans() cluster.\n\n\n\nTable 3.5: Overlap: binary groups v kmeans clusters\n\n\n\n       grp_lbl\ncluster g_00 g_01 g_10 g_11\n      1  224   49   50    9\n      2   22   41   48  164\n      3   18   11   26   38\n      4    0    1    0   76\n\n\n\n\nThe table above provides in more detail the similarities and differences between the inital (grp_lbl) and final (cluster) clusters from kmeans(). The table can be represented and summarized by Jaccard’s measure of the similarity of two finite sets, \\((A, B)\\), namely:\n\\[\n\\begin{align}\n  J(A, B) &= \\frac{\\left| A \\cap B \\right|}{\\left| A \\cup B \\right|}\n\\end{align}\n\\qquad(3.1)\\]\nLetting \\((A, B)\\) denote the respective (row, column) of each cell in the above table, we obtain the following table of Jaccard similarity measures of each (cluster, grp_lbl) combination.\n\n\n\n\nTable 3.6: Jaccard similarity: binary groups v kmeans clusters\n\n\n\n\nJaccard similarity measures\n\n\ncluster\ng_00\ng_01\ng_10\ng_11\n\n\n\n\n1\n0.60\n0.13\n0.12\n0.01\n\n\n2\n0.04\n0.12\n0.14\n0.41\n\n\n3\n0.05\n0.06\n0.14\n0.11\n\n\n4\n0.00\n0.01\n0.00\n0.26\n\n\n\n\n\n\n\n\nThe weighted average (weighted by cell count) of these similarity values is 0.33.\n\n\n3.5.3 Distribution of point-center distances\nThe two figures below show the range of point-center distances for the original grouping and for the kmeans() clustering. In the latter case, points are closer to the mean of the cluster, as can be seen by comparing the scale of the two figures. The total within-cluster sum of squares is 7434 for kmeans() compared to 10433 for the original grouping.\n\n\n\n\n\n\n\n\nFigure 3.6: Initial binary grouping: point-center distances\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.7: K-means clusters: point-center distances\n\n\n\n\n\n\n\n3.5.4 Results for random initial cluster centers\nLet’s run kmeans() again, this time specifying only the desired number \\(K\\) of clusters. The function will then select \\(K\\) rows of the data at random as the initial set of cluster centers. In any case, no matter how the initial cluster centers may be selected, that selection influences the subsequent iterative formation of clusters.\nTo make the algorithm more robust we specify parameter nstart, the number times to run kmeans(), each using a distinct random selection of data rows as initial cluster centers. For each run the algorithm computes WCSS (the total within-cluster sum of squares). Once nstart runs have been completed, the run having the smallest value of WCSS is reported to the user. A rule of thumb is to set nstart to a value between 20 and 50.\nWe now run kmeans() again, setting \\(K = 4\\) and nstart = 25. For purposes of comparison with previous results, here is a projection of the resulting 4 clusters onto a (Top10perc, Expend) scatter diagram.\n\n\n\n\n\n\n\n\nFigure 3.8: Clusters identified from repeated random cluster centers\n\n\n\n\n\nInterestingly, we again see strong separation of clusters associated with the values of Top10perc and Expend.\nThe table below shows the number of observations in each combination of the original grouping and the new kmeans() clusters.\n\n\n       grp_lbl\ncluster g_00 g_01 g_10 g_11\n      1   21   40   47  161\n      2  225   50   51   11\n      3   18   11   26   39\n      4    0    1    0   76\n\n\nAs with the previous kmeans() clusters, we again see two of the clusters well aligned with groups \\(g_{00}\\) and \\(g_{11}\\), respectively.\nThe similarity of the current clusters with the previous clusters prompts us to examine the number of observations in each pair (old, new) clusters, distinguishing the two as (cl_group, cl_nstart).\n\n\n\nTable 3.7: kmeans() clusters formed with and without nstart\n\n\n\n        cl_nstart\ncl_group   1   2   3   4\n       1   0 332   0   0\n       2 269   5   1   0\n       3   0   0  93   0\n       4   0   0   0  77\n\n\n\n\nWe see that the new clusters are essentially a relabeling of the previous clusters. The figure below of new point-center distances is essentially the same as the previous figure with the first two clusters swapped. In fact the total within-cluster sum of squares is unchanged from the previous clustering.\n\n\n\n\n\n\n\n\nFigure 3.9: Revised point-center distances (nstart = 25)\n\n\n\n\n\n\n\n3.5.5 Which value of \\(K\\)?\nSo far in our application of kmeans() to the college data we have set \\(K = 4\\). How would the results change with different choices for the value of \\(K\\)? Breaking the observations into smaller but more numerous clusters should in general reduce the total within-cluster sum of squares (WCSS), but we can also expect to reach a point of diminishing returns.\nFor the college data, the figure below shows how WCSS decreases as \\(K\\) increases.\n\n\n\n\n\n\n\n\nFigure 3.10: Within-cluster sum of squares versus number of clusters\n\n\n\n\n\nIn general, smaller values of \\(K\\) are more interpretable. Based on the figure above we might set \\(K\\) to be 4 or 5. This is a judgement call that depends on the circumstances and goals of one’s project.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#other-clustering-algorithms",
    "href": "clustering.html#other-clustering-algorithms",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "3.6 Other clustering algorithms",
    "text": "3.6 Other clustering algorithms\nClustering methods are an active area of research. The K-means algorithm, first introduced in the 1930’s, has the longest history. More recent methods can be categorized along the following lines.\n\nSoft versus Hard: “Hard” clustering assigns each data point to a single cluster. “Soft” clustering assigns to each point a finite probability distribution of cluster membership.\nAgglomerative versus Divisive methods define the initial partition of the data into clusters in opposite ways. Agglomerative clustering initially defines each data point to be a cluster, and then merges clusters. Divisive clustering initially defines the entire data set to be a single cluster, and then breaks up this and subsequent clusters into smaller clusters.\nHierarchical or not: Instead of a partition, hierarchical methods create a hierarchy of parent (superset) and child (subset) clusters.\nCentroid-based versus Density-based: K-means and other centroid-based methods identify a cluster of points with the mean, median, or other central measure of the points in the cluster. Density-based methods find areas having a high density of points.\n\nSome methods of note are DBSCAN (Density Based Spatial Clustering of Applications with Noise) and HDBSCAN (a hierarchical version of DBSCAN).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#team-exercises",
    "href": "clustering.html#team-exercises",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "3.7 Team Exercises",
    "text": "3.7 Team Exercises\n\nIris data: Submit R command help(\"iris\") to learn about the measurements of iris flowers available in R as datasets::iris. Summarise these measurements in a graph or table, for example by applying the GGally::ggpairs() function.\nIris K-means: Apply the K-means algorithm to the iris measurements. How do the computed clusters compare to the grouping by variable Species? Count the number of observations corresponding to each combination of cluster and Species, for example by using R function stats::xtabs().",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#resources",
    "href": "clustering.html#resources",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "3.8 Resources",
    "text": "3.8 Resources\nAn Introduction to Statistical Learning by James, Witten, Hastie, & Tibshirani\nK-means clustering with tidy data principles – tidymodels\nk-means • tidyclust\nCluster analysis - Wikipedia\nHDBSCAN with the dbscan package",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "clustering.html#footnotes",
    "href": "clustering.html#footnotes",
    "title": "3  Clustering: EDA in Higher Dimensions",
    "section": "",
    "text": "The sum of squared (point, grand-mean) distances does not depend on any grouping (clustering) of the observations, and is called the “total sum of squares”. The between-cluster sum of squares (BCSS) equals the total sum of squares minus the within-cluster sum of squares. BCSS is also equal to a weighted sum of (cluster-center, grand-mean) squared distances.↩︎\nkmeans() expects the cluster centers to be expressed as a matrix, in which each row represents a single cluster center and each column represents a “coordinate” of the cluster centers corresponding to one of the numeric variables (columns) of the data matrix.↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering: EDA in Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "4  Statistical Simulation",
    "section": "",
    "text": "4.1 Introduction\nStatistical simulation generates random data from specified distributions in order to model the behavior of complex systems and analyze potential outcomes of various scenarios. Simulations augment analytic solutions, enabling researchers to explore different possibilities and assess alternative methods. Here are some of the uses of statistical simulation.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-introduction",
    "href": "simulation.html#sec-introduction",
    "title": "4  Statistical Simulation",
    "section": "",
    "text": "Comparing statistical methods\nAssessing the robustness of data analyses\nUnderstanding system behavior\nSupplementing real-world data",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-rng",
    "href": "simulation.html#sec-rng",
    "title": "4  Statistical Simulation",
    "section": "4.2 Generating Random Numbers from a Given Distribution",
    "text": "4.2 Generating Random Numbers from a Given Distribution\nAs mentioned in an earlier session, the R stats package provides random number generators for commonly used probability distributions. For each random number generator the package also provides the probability density function (for continuous distributions) or the probability mass function (for discrete distributions), along with cumulative versions (incomplete integrals or sums), and the inverse of the cumulative versions, the quantile functions. The naming convention is illustrated by the uniform distribution over a finite interval (unit interval by default).1\n\ndunif() density at a given point \\(x\\)\npunif() cumulative probability, that \\(X \\le x\\)\nqunif() quantile, return \\(x\\) having prescribed cumulative probability \\(p\\)\nrunif() generate indpendent instances \\((X_1, \\ldots, X_n)\\) of a uniform random variable\n\n\n4.2.1 Continuous Distributions\nThe stats package includes the following continuous distributions.\n\n\n\n\nTable 4.1: Continuous Distributions from stats\n\n\n\n\nContinuous Distributions from stats\n\n\nprefix\nsuffix\ndistribution\n\n\n\n\n[d, p, q, r]\nbeta\nBeta\n\n\n[d, p, q, r]\ncauchy\nCauchy\n\n\n[d, p, q, r]\nchisq\n(non-central) Chi-Squared\n\n\n[d, p, q, r]\nexp\nExponential\n\n\n[d, p, q, r]\nf\nF\n\n\n[d, p, q, r]\ngamma\nGamma\n\n\n[d, p, q, r]\nlnorm\nLog Normal\n\n\n[d, p, q, r]\nlogis\nLogistic\n\n\n[d, p, q, r]\nnorm\nNormal\n\n\n[d, p, q, r]\nt\nStudent t\n\n\n[d, p, q, r]\nunif\nUniform\n\n\n[d, p, q, r]\nweibull\nWeibull\n\n\n\n\n\n\n\n\nThese distributions have interesting histories and relationships.2 For example a beta variable \\(X\\) having shape parameters \\((\\alpha, \\beta)\\) can be expressed as the ratio \\(U/(U + V)\\), where \\((U, V)\\) are independent gamma variables having a common scale parameter, and respective shape parameters \\((\\alpha, \\beta)\\). See the resources listed below if interested.\n\n\n4.2.2 Discrete Distributions\nThe stats package also includes the following discrete distributions.\n\n\n\n\nTable 4.2: Discrete Distributions from stats\n\n\n\n\nDiscrete Distributions from stats\n\n\nprefix\nsuffix\ndistribution\n\n\n\n\n[d, p, q, r]\nbinom\nBinomial\n\n\n[d, p, q, r]\ngeom\nGeometric\n\n\n[d, p, q, r]\nhyper\nHypergeometric\n\n\n[d, p, q, r]\nnbinom\nNegative Binomial\n\n\n[d, p, q, r]\npois\nPoisson\n\n\n\n\n\n\n\n\nAgain, these distributions have interesting histories and relationships. For example the Poisson distribution can described as the limiting case of binomial distributions, \\(\\text{Binom}(n, p_n)\\) such that the respective expected values, \\(n \\times p_n\\), converge to \\(\\lambda &gt; 0\\) as \\(n \\rightarrow \\infty\\). Equivalently we can write \\(p_n \\approx \\lambda/n\\), so that \\(p_n\\), the probability of success on a single Bernoulli trial, becomes vanishingly small as the number \\(n\\) of Bernoulli trials increases. For this reason the Poisson distribution has been called “the law of rare events”.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-mean-v-median-example",
    "href": "simulation.html#sec-mean-v-median-example",
    "title": "4  Statistical Simulation",
    "section": "4.3 Example Application",
    "text": "4.3 Example Application\nStatistical simulation has been used to evaluate proposed statistical methods. Here’s an admittedly tame example (where the answer has already been determined mathematically).\nLet \\(\\hat{\\mathcal{m}}\\) and \\(\\hat{\\mu}\\) denote the median and mean, respectively, of a random sample from a continuous distribution. For a symmetric distribution (with a defined population mean value), the population median and mean coincide. But the sample mean is highly sensitive to a few outlying values in the sample, and is thus less robust than the sample median for long-tailed distributions.\nConsequently, in financial applications and other applications where long-tailed distributions occur, the median is the preferred descriptor of the central value of the sample and of the population.\nIn some cases the robustness of the sample median comes at a cost. If the population distribution is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\) then the sample mean has smaller variance than the sample median (although they estimate the same population value).\n\\[\n\\begin{align}\n  \\text{Var} \\left\\{ \\hat{\\mu} \\right\\} &= \\frac{\\sigma^2}{n} \\\\\n  \\\\\n  \\text{Var} \\left\\{ \\hat{\\mathcal{m}} \\right\\} &= \\frac{\\pi}{2} \\frac{\\sigma^2}{n + 1} \\\\\n  \\\\\n  \\frac{ \\text{Var} \\left\\{ \\hat{\\mathcal{m}} \\right\\} }{ \\text{Var} \\left\\{ \\hat{\\mu} \\right\\} } &\\approx \\frac{\\pi}{2} \\\\\n  &\\approx 1.57\n\\end{align}\n\\qquad(4.1)\\]\nThis ratio of variances in the normal case can be verified by simulation.\nIn this example, simulation would merely illustrate a property already determined mathematically. But in other situations, statistical simulation may be the best practical way to understand the properties of a proposed statistical procedure, or more generally, a system that entails random events.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-class-exr",
    "href": "simulation.html#sec-class-exr",
    "title": "4  Statistical Simulation",
    "section": "4.4 Class Exercise",
    "text": "4.4 Class Exercise\nWith a teammate, generate a sample of \\(n\\) pseudo-random numbers following the normal distribution, for example using stats::rnorm(). Calculate \\(\\hat{\\mathcal{m}}\\) and \\(\\hat{\\mu}\\), the sample median() and mean(). Now repeat that process a total of \\(R\\) times, recording the values of the sample mean and median in each run. This gives a sample of size \\(R\\) of the pair \\((\\hat{\\mathcal{m}}, \\hat{\\mu})\\). What is the sample variance of these two estimators? Take 20 minutes to prepare to report your progress to the class.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-mc-sim",
    "href": "simulation.html#sec-mc-sim",
    "title": "4  Statistical Simulation",
    "section": "4.5 Monte Carlo Simulation",
    "text": "4.5 Monte Carlo Simulation\nMonte Carlo simulation encompasses a broad set of algorithms that use random sampling to obtain numerical values. Mathematician Stanislav Ulam led the development of this approach (and coined the name) as part of the Manhattan Project during World War 2. The approach is used when other types of numerical calculation are not feasible.\n\n4.5.1 Example 1: estimate \\(\\pi\\)\nHere is a simple illustration. Recall that a circle of radius \\(r\\) has an area equal to \\(\\pi \\; r^2\\). Setting \\(r = 1\\) we see that the unit circle has an area equal to \\(\\pi\\). Centered at the origin of the plane, the area is divided equally among the four quadrants of the plane. Thus the area within the first quadrant equals \\(\\pi/4\\). We can use random sampling to estimate \\(\\pi\\) as follows.\n\nGenerate pairs of independent standard uniform random variables \\(\\{ (X_j, Y_j) \\}_{j = 1}^n\\). They constitute uniformly random points in the unit square.\nDefine \\(B_j\\) to be the Bernoulli indicator function, equal to 1 if \\(X_j^2 + Y_j^2 &lt; 1\\) and equal to zero otherwise. Note that \\(B_j\\) has expected value \\(\\pi/4\\).\nCalculate the sample average \\(A(B_{\\bullet})\\) as an estimate of \\(\\pi/4\\).\nMultiply the sample average by 4 as an estimate of \\(\\pi\\).\n\nThe precision of the estimate improves as the number \\(n\\) of randomly generated points increases.\n\n\n4.5.2 Example 2: estimate area of an arbitrary region\nThe figure below shows a simple closed curve within the unit square centered at the origin.\n\n\n\n\n\n\n\n\nFigure 4.1: Monte Carlo estimation of area\n\n\n\n\n\nThe closed curve defines an interior region \\(\\mathcal{R}\\) bounded by the square \\(\\mathcal{S} = [-0.5, 0.5] \\times [-0.5, 0.5]\\). We assume that for any point \\((x, y) \\in \\mathcal{S}\\) we can determine whether the point is inside or outside \\(\\mathcal{R}\\). The Monte Carlo estimate of the area of this region is the proportion of points randomly selected from \\(\\mathcal{S}\\) that fall within \\(\\mathcal{R}\\) multiplied by the area of \\(\\mathcal{S}\\) (which is 1 in this case).\nIn the current example, the closed curve shown above is a deformation of the unit circle in which the Euclidean norm at each point, \\(t\\), along the circle has been multiplied by a prescribed positive continuous function, \\(f(t)\\). Consequently a point \\((x, y)\\) is in the interior of the closed curve if \\(x^2 + y^2 &lt; f^2(t)\\), where \\(t \\in [0,1)\\) equals \\(\\arctan(y, x) \\bmod{2 \\pi}\\).\nThis is a common application of Mote Carlo simulation: estimating the volume of given region \\(\\mathcal{R}\\) in Euclidean space, under the assumption that, for any given point, we can determine whether the point is inside or outside the region.\n\n\n4.5.3 Importance Sampling 3\nThe above description of Monte Carlo simulation is also called rejection sampling, and can be generalized as follows. Let \\(\\mathcal{R}\\) denote a region of \\(\\mathbb{R}^d\\) having a finite volume \\(| \\mathcal{R} |\\). We want to estimate the average value, \\(\\mu(f, \\mathcal{R})\\) of any given integrable function \\(f: \\mathcal{R} \\rightarrow \\mathbb{R}\\).\n\\[\n\\begin{align}\n  \\mu(f, \\mathcal{R}) &= \\frac{1}{| \\mathcal{R} |} \\int_\\mathcal{R} f(x) \\; dx\n\\end{align}\n\\qquad(4.2)\\]\nThis corresponds to the expected value of \\(f(X)\\) when the random vector \\(X\\) has a uniform probability distribution over \\(\\mathcal{R}\\). More generally, suppose we also have some probability density function \\(p(\\cdot)\\) restricted to \\(\\mathcal{R}\\), and we want to calculate the expected value of \\(f(X)\\) when \\(X\\) has probability density \\(p(\\cdot)\\).\n\\[\n\\begin{align}\n  \\mu(f, p) &= \\int_\\mathcal{R} f(x) \\times p(x) \\; dx\n\\end{align}\n\\qquad(4.3)\\]\nNow. it may not be convenient or feasible to generate a random sample of vectors \\(\\{ X_1, \\ldots, X_n \\}\\) from the probability distribution having given density function \\(p(\\cdot)\\). But we may be able to generate a random sample of vectors \\(\\{ Y_1, \\ldots, Y_n \\}\\) from a density function \\(q(\\cdot)\\) defined on a region \\(\\mathcal{S}\\) containing \\(\\mathcal{R}\\). Then \\(\\mu(f, p)\\) can be expressed as the following weighted expectation of \\(f(Y)\\).\n\\[\n\\begin{align}\n  \\mu(f, p) &= \\int_\\mathcal{R} w(y) \\times f(y) \\times q(y) \\; dy  \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  w(y) &= \\frac{p(y)}{q(y)}\n\\end{align}\n\\qquad(4.4)\\]\nThe estimated value takes the form of the following weighted sum.\n\\[\n\\begin{align}\n  \\hat{\\mu}(f, p) &= \\frac{\\sum_{j = 1}^n w(Y_j) \\; f(Y_j)}\n  {\\sum_{k = 1}^n w(Y_k)}\n\\end{align}\n\\qquad(4.5)\\]\nThis approach is called importance sampling, and has many variations.\nExample: Suppose \\(Z\\) has a standard normal distribution with density function \\(\\phi(\\cdot)\\) and cumulative probability function \\(\\Phi(\\cdot)\\), and we want to determine the following conditional expectation.\n\\[\n\\begin{align}\n  \\mu(f, p) &= E \\left \\{ f(Z) \\; | \\; Z \\ge 6  \\right \\} \\\\\n  &= \\int_\\mathcal{R} f(z) \\times p(z) \\; dz \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  \\mathcal{R} &= \\{ z \\; : \\; z \\ge 6 \\} \\\\\n  p(z) &=\n  \\begin{cases}\n    0 & \\text{if } z &lt; 6 \\\\\n    \\frac{\\phi(z)}{1 - \\Phi(6)} & \\text{if } z \\ge 6\n  \\end{cases}\n\\end{align}\n\\qquad(4.6)\\]\nA naive approach to estimating \\(\\mu(f, p)\\) would be to generate standard normal variables \\(\\{ Z_1, \\ldots, Z_n \\}\\), rejecting those values less than 6, and then averaging \\(f(Z_\\nu)\\) across the remaining values. The problem is that we would expect only one in a billion values to pass this threshhold, making that algorithm highly inefficient.\nAlternatively, we could define \\(Y_\\nu = 12 | Z_\\nu |\\), and apply the weighted averaging of Equation 4.5.4\n\\[\n\\begin{align}\n  w(y) &= \\frac{p(y)}{q(y)} \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  p(y) &=\n  \\begin{cases}\n    0 & \\text{if } z &lt; 6 \\\\\n    \\frac{\\phi(y)}{1 - \\Phi(6)} & \\text{if } z \\ge 6\n  \\end{cases} \\\\\n  q(y) &= \\frac{2}{12} \\; \\phi \\left ( \\frac{y}{12} \\right )\n\\end{align}\n\\qquad(4.7)\\]\nThis weighting function \\(w(\\cdot)\\) rapidly decreases on the interval \\(y \\in [6, \\infty)\\) and otherwise assigns zero weight.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-mcmc",
    "href": "simulation.html#sec-mcmc",
    "title": "4  Statistical Simulation",
    "section": "4.6 Markov Chain Monte Carlo (MCMC) 5",
    "text": "4.6 Markov Chain Monte Carlo (MCMC) 5\n\n4.6.1 Overview\nMarkov Chain Monte Carlo (MCMC) simulation encompasses many variations. The basic problem to be addressed is the same as that of importance sampling:\n\nWe would like to generate a random sample from a distribution having given density, \\(p(x), x \\in \\mathbb{R}^d\\).\nWe cannot directly generate the desired random sample, although we can evaluate \\(p(x)\\) for any given \\(x \\in \\mathbb{R}^d\\).\n\nThe approach toward a solution can be framed as follows.\n\nConstruct a Markov chain \\(X: \\mathbb{Z} \\rightarrow \\mathbb{R}^d\\) (a discrete-time random process) having a stationary distribution (to which \\(X(\\cdot)\\) converges) given by density \\(p(\\cdot)\\).\nGenerate a sufficient number \\(n\\) of samples from \\(X(\\cdot)\\) so that \\(\\{ X(n), X(n+1), \\ldots \\}\\) approximates a sample from the desired stationary distribution.\n\n\n\n4.6.2 Markov chains\nRecall the definition of a Markov chain, \\(X(\\cdot)\\). The conditional distribution of \\(X(t)\\) given past values, \\(X(t-1), X(t-2), \\ldots\\), is just the conditional distribution of \\(X(t)\\) given its preceding value, \\(X(t-1)\\).\nTo illustrate, suppose that the values of \\(X(\\cdot)\\) are restricted to a finite set, say \\(\\{ 1, 2, 3 \\}\\). Then we define the transition probability \\(p_{a, b}\\) as the following conditional probability.\n\\[\n\\begin{align}\n  p_{a, b} &= P(X(t) = b \\; | \\; X(t-1) = a)\n\\end{align}\n\\qquad(4.8)\\]\nThese transition probabilities are the elements of the following transition probability matrix \\(T\\).\n\\[\n\\begin{align}\n  T &=\n  \\left (\n  \\begin{matrix}\n    p_{1, 1} & p_{1,2} & p_{1,3} \\\\\n    p_{2, 1} & p_{2,2} & p_{2,3} \\\\\n    p_{3, 1} & p_{3,2} & p_{3,3}\n  \\end{matrix}\n  \\right )\n\\end{align}\n\\qquad(4.9)\\]\nIf we denote the (marginal) probability distribution of \\(X(n)\\) as \\(\\pi_n\\)\n\\[\n\\begin{align}\n  \\pi_n &= \\left ( P(X(n) = 1), P(X(n) = 2), P(X(n) = 3) \\right )\n\\end{align}\n\\qquad(4.10)\\]\nthen we have\n\\[\n\\begin{align}\n  \\pi_{n+1} &= \\pi_n  \\; T\n\\end{align}\n\\qquad(4.11)\\]\nConsequently we have\n\\[\n\\begin{align}\n  \\pi_n &= \\pi_0  \\; T^n\n\\end{align}\n\\qquad(4.12)\\]\nUnder regularity conditions6 there is a unique stationary distribution, say \\(\\pi_\\infty\\), such that \\(\\pi_n \\rightarrow \\pi_\\infty\\). Convergence is accompanied by diminishing values of \\(\\lVert \\pi_{n+1} - \\pi_n \\rVert\\).\n\n\n4.6.3 Time Reversal\nRecall that \\(p(x)\\) is the prescribed stationary distribution of a Markov chain \\(X(\\cdot)\\) that we must construct. To do so, we will use the fact that if \\(X(\\cdot)\\) is time-reversible, then the marginal distribution of \\(X(\\nu)\\) is stationary. This can be seen as follows.\n\\(X(\\cdot)\\) is time-reversible if the following holds. For any given \\(n &gt; 0\\) define \\(Y(\\nu) = X(n - \\nu)\\) for \\(0 \\le \\nu \\le n\\). Then the joint probability distribution of the sequence \\(( X(0), \\ldots, X(n) )\\) is equal to that of the sequence \\(( Y(0), \\ldots, Y(n) ) = ( X(n), \\ldots, X(0) )\\).\nIt follows that the ordered pair of random variables \\((X(0), X(n))\\) has the same distribution as that of \\((X(n), X(0))\\), so that \\(X(0)\\) has the same distribution as \\(X(n)\\), and this holds for all \\(n &gt; 0\\). Thus if \\(X(\\cdot)\\) is time-reversible its marginal distribution must be stationary.\nNow assume for the moment that the values of \\(X(t)\\) form a discrete set, so that:\n\\[\n\\begin{align}\n  P\\left ( X(0) = a, X(1) = b \\right ) &= \\pi_\\infty(a) \\times p_{a, b} \\\\\n  P\\left ( X(1) = a, X(0) = b \\right ) &= \\pi_\\infty(b) \\times p_{b, a}\n\\end{align}\n\\qquad(4.13)\\]\nIf \\(X(\\cdot)\\) is time-reversible, it follows that:\n\\[\n\\begin{align}\n  \\pi_\\infty(a) \\times p_{a, b} &= \\pi_\\infty(b) \\times p_{b, a}\n\\end{align}\n\\qquad(4.14)\\]\nEquations of this form are called local balance equations. If they hold then marginal distribution \\(\\pi_\\infty\\) along with transition probabilities \\(p_{a, b}\\) define a stationary Markov chain.\n\n\n4.6.4 Metropolis-Hastings Algorithm7\nSuppose now that \\(q(Y \\; | \\; X )\\) is a transition density from which we can generate a random sequence \\(X(0), X(1), \\ldots\\), and let \\(p(x)\\) denote the prescribed stationary distribution of \\(X(\\cdot)\\). Also suppose that the current state of the generated sequence is \\(X(t) = x\\).\n\n4.6.4.1 Outline\nThe Metropolis-Hastings procedure is an iterative algorithm for generating the next value of the sequence as follows.\n\nGenerate a candidate value \\(y \\sim q(Y \\; | \\; x )\\).\nCalculate acceptance ratio \\(\\alpha(y \\; | \\; x)\\) shown below.\nGenerate \\(u \\sim unif(0, 1)\\). Set the next value to \\(y\\) if \\(u \\le \\alpha(y \\; | \\; x)\\). Otherwise set the next value to the current value, \\(x\\).\n\n\\[\n\\begin{align}\n  \\alpha(y \\; | \\; x) &= \\max \\left \\{1, \\frac{ p(y) \\; q(x \\; | \\; y) }{ p(x) \\; q(y \\; | \\; x ) } \\right \\}\n\\end{align}\n\\qquad(4.15)\\]\n\n\n4.6.4.2 Metropolis Algorithm\nThe original algorithm, known as the Metropolis algorithm, is a simpler procedure that generates a random walk. Let \\(g(\\cdot)\\) denote a probability density function that is symmetric with respect to \\(x = 0\\). Given the current state \\(X(t) = x\\), generate the candidate value \\(y\\) as:\n\\[\n\\begin{align}\n  y &= x + \\epsilon \\\\ \\\\\n  &\\text{where } \\\\ \\\\\n  \\epsilon &\\sim g(\\cdot)\n\\end{align}\n\\qquad(4.16)\\]\nThen the density \\(q(y \\; | \\; x)\\) of the transition from \\(x\\) to \\(y = x + \\epsilon\\) is:\n\\[\n\\begin{align}\n  q(y \\; | \\; x) &= g(y - x)\n\\end{align}\n\\qquad(4.17)\\]\nSince \\(g(\\cdot)\\) is symmetric about zero, \\(q(y \\; | \\; x )\\) is symmetric in its two arguments.\n\\[\n\\begin{align}\n  q(x \\; | \\; y) &= g(x - y) \\\\\n  &= g(y - x) \\\\\n  &= q(y \\; | \\; x)\n\\end{align}\n\\qquad(4.18)\\]\nConsequently the acceptance ratio simplifies as follows.\n\\[\n\\begin{align}\n  \\alpha(y \\; | \\; x) &= \\max \\left \\{1, \\frac{ p(y) \\; q(x \\; | \\; y) }{ p(x) \\; q(y \\; | \\; x ) } \\right \\} \\\\\n  &= \\max \\left \\{1, \\frac{ p(y) }{ p(x) } \\right \\}\n\\end{align}\n\\qquad(4.19)\\]\n\n\n4.6.4.3 Example\nFor some fixed \\(\\delta &gt; 0\\) let \\(g(\\cdot)\\) be the uniform density on the interval \\((-\\delta, \\delta)\\). Let the specified stationary distribution be given by the standard normal density function \\(\\phi(\\cdot)\\). Then the Metropolis algorithm outlined above will generate a random walk in discrete time having jump-magnitudes governed by \\(\\delta\\).\nThe steps to generate the next value \\(y\\) given the current value \\(x\\) are a simplification of the steps previously outlined.\n\nGeneratecandidate value \\(y = x + \\epsilon\\), where \\(\\epsilon \\sim uniform(-\\delta, \\delta)\\).\nCalculate acceptance ratio \\(\\alpha(y \\; | \\; x)\\) shown below.\nGenerate \\(u \\sim unif(0, 1)\\). Set the next value to \\(y\\) if \\(u \\le \\alpha(y \\; | \\; x)\\). Otherwise set the next value to the current value, \\(x\\).\n\n\\[\n\\begin{align}\n  \\alpha(y \\; | \\; x) &= \\max \\left \\{1, \\frac{ \\phi(y) }{ \\phi(x) } \\right \\}\n\\end{align}\n\\qquad(4.20)\\]\nHere are three simulations, each starting at \\(X(0) = 0\\), but with \\(\\delta \\in \\{ \\frac{1}{8}, \\frac{1}{2}, 2 \\}\\), respectively. Recall that \\(| X(t+1) - X(t) | \\le \\delta\\). Thus the trajectories are decreasingly smooth.\n\n\n\n\n\n\n\n\nFigure 4.2: X(t) with small to large jumps\n\n\n\n\n\nNext we see histograms of the generated values. For the first and smoothest series more time would have been needed to traverse a greater portion of the normal distribution values.\n\n\n\n\n\n\n\n\nFigure 4.3: Histograms of X(t) with small to large jumps\n\n\n\n\n\nComparing the sets of summary statistics below, we see that the first random walk was confined to a narrower range of values. The final column is the effective sample size estimated by R function astsa::ESS().\n\n\n\nTable 4.3: Summary statistics: normal random walks\n\n\n\n# A tibble: 3 × 9\n  d_idx   min    q_1    avg    q_3   max    sd     n    ess\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1 1     -2.07 -1.54  -0.995 -0.505 0.525 0.650  1024   4.07\n2 2     -3.06 -1.12  -0.303  0.601 2.53  1.10   1024  15.6 \n3 3     -3.37 -0.773 -0.151  0.555 3.23  0.985  1024 212.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-team-exr",
    "href": "simulation.html#sec-team-exr",
    "title": "4  Statistical Simulation",
    "section": "4.7 Team Exercises",
    "text": "4.7 Team Exercises\n\nAs in the class exercise, generate \\(R\\) samples, each a sample of size \\(n\\), but this time use the Cauchy distribution (rcauchy()) rather than normal distribution (rnorm()). Calculate the sample variance and construct a histogram of the \\(R\\) sample means:\n\n\\[\n\\left\\{ \\hat{\\mu}_r \\right\\}_{r = 1}^R\n\\qquad(4.21)\\]\n\nSuppose \\((U_1, \\ldots, U_n)\\) are independent and identically distributed random variables following the standard uniform distribution on the interval \\((0, 1)\\). Now set \\(X_j = - \\log_e(U_j)\\) for \\(j = 1, \\ldots, n\\). What is the distribution of \\((X_1, \\ldots, X_n)\\)?\nIn the Monte Carlo illustration of Section 4.5.1, what is the standard error of estimate of \\(\\pi\\)?",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#resources",
    "href": "simulation.html#resources",
    "title": "4  Statistical Simulation",
    "section": "4.8 Resources",
    "text": "4.8 Resources\nStatistical Distributions by Forbes, Evans, Hastings, and Peacock\nDistributions in Statistics by Johnson and Kotz\nAdvanced Statistical Computing by Roger Peng\nR: Random Number Generation\nMonte Carlo method - Wikipedia\n\n\n\n\nTokdar, Surya T., and Robert E. Kass. 2010. “Importance Sampling: A Review.” WIREs Computational Statistics 2 (1): 54–60. https://doi.org/https://doi.org/10.1002/wics.56.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "simulation.html#footnotes",
    "href": "simulation.html#footnotes",
    "title": "4  Statistical Simulation",
    "section": "",
    "text": "See (Rrng?).↩︎\nSee (forbes2010statistical?) and (johnson1985distributions?).↩︎\nSee Tokdar and Kass (2010).↩︎\nIf \\(Y_\\nu = 12 | Z_\\nu |\\), then the probability that \\(Y_\\nu \\ge 6\\) is about 0.62.↩︎\nSee (rdpeng2022advstatcomp?)↩︎\nA unique, stationary distribution exists if the chain is aperiodic, and if all states communicate and are positive recurrent. For further details see the Wikipedia article.↩︎\nSee (MH_algo_Wikipedia_2025?).↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical Simulation</span>"
    ]
  },
  {
    "objectID": "study-design.html",
    "href": "study-design.html",
    "title": "5  Sampling and Study Design",
    "section": "",
    "text": "5.1 Introduction\nThis session introduces some basic ideas and methods in statistical sampling theory and the design of experimental or observational studies. We follow the book “Statistics, 4/e” by Freedman, Pisani, and Purves (FPP).\nAs our world becomes evermore connected digitally, the available data of interest may seem either to encompass the entire population of interest (e.g., online behavior of consumers), or else to be highly focused (e.g., the purchasing history of a particular customer). At these extremes the notion of a sample from a population might seem irrelevant. But sampling theory becomes relevant as soon as we transition from describing the data on hand to asking how current phenomena might change under various scenarios.\nWe begin with examples of observational and experimental studies, and review some sampling strategies under practical constraints.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "study-design.html#observational-study-examples",
    "href": "study-design.html#observational-study-examples",
    "title": "5  Sampling and Study Design",
    "section": "5.2 Observational Study Examples",
    "text": "5.2 Observational Study Examples\n\n5.2.1 Literary Digest Poll, 1936\nIn 1936, President Franklin Delano Roosevelt (FDR) was completing his first term in office, and running for re-election against Alfred Landon, Governor of Kansas state. Most pundits predicted that FDR would win.\nThe Literary Digest magazine, however, predicted a decisive Landon victory based on a massive poll having 2.4 million respondents. The Digest predicted that Roosevelt would get only 43% of the vote.\nGeorge Gallup had just created his polling organization. Gallup took a sample of 3,000 voters to estimate what the Digest would predict, and took a sample of 50,000 voters to predict the election. Here are the results.\n\n\n\n\nTable 5.1: Predicted % for FDR in 1936\n\n\n\n\nPredicted % for FDR in 1936\n\n\nsource\nFDR predicted %\n\n\n\n\nDigest\n43\n\n\nGallup re Digest\n44\n\n\nGallup re election\n56\n\n\nelection result\n62\n\n\n\n\n\n\n\n\nAs it turned out, Roosevelt won by a landslide: 62% versus 38%. The Digest’s error of nearly 20 percentage points is the largest in history by a major poll. What went wrong?\nMoral: a large sample size alone does not guarantee accuracy. Ask yourself how the sample was chosen.\n\n\n5.2.2 Truman versus Dewey, 1948\nTruman was a colorful and effective US president, but he was the underdog in the 1948 elelction. Three major polling organizations covered the election: Crossley for the Hearst newspapers, Gallup (synicated in ~100 newspapers across the country), and Roper for Fortune magazine. By the fall of 1948, all three put Dewey ahead of Truman by 5 or more percentage points. Here are the predicted and actual percentages.\n\n\n\n\nTable 5.2: Predicted % per candidate in 1948\n\n\n\n\nPredicted % per candidate in 1948\n\n\nsource\nTruman\nDewey\nThrumond\nWallace\n\n\n\n\nCrossley\n45\n50\n2\n3\n\n\nGallup\n44\n50\n2\n4\n\n\nRoper\n38\n53\n5\n4\n\n\nelection result\n50\n45\n3\n2\n\n\n\n\n\n\n\n\nAll three polling organizations not only under-estimated the percentage vote for Truman, they were also over-confident in their predictions. Again we ask, what went wrong?\nThe three polling organizations used quota sampling. That is, in an effort to construct samples representative of all voters, the polling organizations ensured that sample percentages of women, men, employed, unemployed, and so on, exactly matched known population percentages.\nInterviewers were given prescribed numbers of voters to interview in each category, and were otherwise unconstrained in their choice of subjects to interview.\nThe problem is that there are many factors that influence voting, and many of them cannot be known in advance of the election. In addition, allowing the interviewers to select their subjects (constrained only by certain demographic quotas to be fulfilled) may introduce an unwitting bias, namely toward those whom the interviewers find easier to include. As it happened, quota sampling produced a bias toward those subjects who voted Republican.1\n\n\n5.2.3 Using Chance in Surveys\nSince 1948 polling organizations have adopted some form of probability sampling. The idea is that there are too many known and unknown factors that influence voting to control for them all. Therefore it is better to choose subjects at random, allowing chance to provide a sample that approximately represents the population (with better approximation for larger sample sizes).\nThe leading example of probability sampling is simple random sampling. Imagine a box of tickets, one ticket per registered voter. We shuffle the tickets, randomly select one to include in our sample, set that ticket aside, and then repeat this process.\nWhen little is known about a population, simple random sampling yields more accurate estimates of a population percentage or population average than other probability samples of the same size.2 The problem is that simple random sampling is impractical in many situations. Even if polling organizations had a list (sampling frame) of all registered voters in the country, it would be prohibitively costly to interview subjects scattered across the country.\nInstead, polling organizations often use multi-stage cluster sampling. As an example, from 1952 to 1984 the Gallup organization partitioned the 50 states into four regions: Northeast, South, Midwest, and West. In each region, towns are selected at random; within each town, wards are selected at random; within each ward, precincts are selected at random; and fnally within each precinct, households are selected at random. Then the eligible voters present in the household (cluster) are interviewed.3 At each stage, random selection may be weighted, e.g., by population size.\nProbability sampling of any type has two important features:\n\ninterviewers have no discretion in the choice of subjects;\nthere is a prescribed procedure for selecting subjects, involving the planned use of chance.\n\nThe hallmark of a probability sample is that the chance of including any given individual in the sample can be calculated in advance.\n\n\n5.2.4 More Recent Election Polls\nSince the 1952 election Gallup and other pollsters have used probability samples, and have, on the whole, reduced the magnitude of prediction error. Here are the prediction errors of the Gallup organization. (Ross Perot’s candidacy made 1992 an especially challenging year to predict.)\n\n\n\n\nTable 5.3: Gallup prediction accuracy: 1952-2004\n\n\n\n\nGallup prediction accuracy: 1952-2004\n\n\nyear\nn\nwinner\nGallup\nactual\nerror\n\n\n\n\n1952\n5385\nEisenhower\n51.0\n55.1\n-4.1\n\n\n1956\n8144\nEisenhower\n59.5\n57.4\n2.1\n\n\n1960\n8015\nKennedy\n51.0\n49.7\n1.3\n\n\n1964\n6625\nJohnson\n64.0\n61.1\n2.9\n\n\n1968\n4414\nNixon\n43.0\n43.4\n-0.4\n\n\n1972\n3689\nNixon\n62.0\n60.7\n1.3\n\n\n1976\n3439\nCarter\n48.0\n50.1\n-2.1\n\n\n1980\n3500\nReagan\n47.0\n50.7\n-3.7\n\n\n1984\n3456\nReagan\n59.0\n58.8\n0.2\n\n\n1988\n4089\nBush\n56.0\n53.4\n2.6\n\n\n1992\n2019\nClinton\n49.0\n43.0\n6.0\n\n\n1996\n2895\nClinton\n52.0\n49.2\n2.8\n\n\n2000\n3571\nBush\n48.0\n47.9\n0.1\n\n\n2004\n2014\nBush\n49.0\n50.6\n-1.6\n\n\n\n\n\n\n\n\n\n\n5.2.5 UC Berkeley Admissions\nIn a previous session we discussed a study from the 1970’s arising from a concern that admissions to graduate school at UC Berkeley might be biased against females. The study determined that the apparent bias was an instance of Simpon’s Paradox: females tended to apply to the departments having lower overall admission rates, resulting in an overall lower admission rate for females, even though most of the departments admitted a greater percentage of their female applicants than their male applicants.\nIn the context of sampling theory, we note that the data were limited to a single year, and included all applicants. One might ask whether the differences in admission rates were “statistically significant”, but the question presumes that the data are the result of a probability sample from a larger population, which is not the case here. The year of data (1973) was accepted as evidence that the apparent bias was an instance of Simpson’s Paradox. The concern was resolved under the assumption that the data and circumstances from that particular year were indicative of the admissions process.\nMoral: the expected accuracy of statistical estimates has been established for various types of probability sampling, but if the data are not the result of a probability sample from a defined population, these established formulas are not applicable.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "study-design.html#class-exercise",
    "href": "study-design.html#class-exercise",
    "title": "5  Sampling and Study Design",
    "section": "5.3 Class Exercise",
    "text": "5.3 Class Exercise\nBreak into teams of two or three to identify examples of (or opportunities for) observational studies in the context of work. Which of these would you regard as most valuable? Based on the examples above, or your experience, what are the potential pitfalls of such studies? Take 15 minutes to prepare to report to the class.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "study-design.html#experimental-study-examples",
    "href": "study-design.html#experimental-study-examples",
    "title": "5  Sampling and Study Design",
    "section": "5.4 Experimental Study Examples",
    "text": "5.4 Experimental Study Examples\n\n5.4.1 Salk Vaccine Field Trial\nPolio has been a devastating, sometimes fatal, disease. The first epidemic in the US was in 1916. In the following years the disease claimed several hundreds of thousands of victims, especially children. By the 1950’s several drugs against polio were in development. The drug developed by Jonas Salk had particularly promising lab results. By 1954 the US government determined the drug should be tested in the field.\nOne might ask why a promising drug shouldn’t simply be adopted, rather than field-tested. One problem was that polio incidence varied considerably from year to year. In 1952 there were about 60,000 cases. The following year there were half that number. Had the vaccine been adopted in 1954, any decrease in incidence could well have been ascribed to annual variation, rather than the effectiveness of the vaccine.\nThe decision to test rather than adopt a promising drug is a difficult question of medical ethics. For the polio vaccine, there were potential risks in taking the drug, and so the government decided a test was needed to establish whether the benefits of adopting the drug would outweigh the risks.\nThe National Foundation for Infantile Paralysis (NFIP) ran a controlled experiment among children in grades 1-3, an especially vulnerable age group. The field trial was carried out in selected school districts throughout the country, where the risk of polio was high. AS originally designed, the NFIP vaccinated all children in grade 2 whose parents consented to the vaccination, leaving unvaccinated those children in grade 2 whose parents did not consetn. Children in grades 1 and 3 were used as the controld group (were not vaccinated). The following table shows the results.\n\n\n\n\nTable 5.4: Polio rate per 100,000 subjects (NFIP study)\n\n\n\n\nPolio rate per 100,000 subjects\n\n\ngrade\nassignment\nsize\nrate\n\n\n\n\n2\ntreatment\n225000\n25\n\n\n1, 3\ncontrol\n725000\n54\n\n\n2\nno_consent\n125000\n44\n\n\n\n\n\n\n\n\nParticipating public health experts raised several concerns with the NFIP design. Polio can be transmitted by touch, so the incidence of polio infection may vary by grade. Also, parental consent is a potential confounding factor. More highly educated parents are more likely to consent, but their children are at greater risk. (Lower socio-economic status is associated with lower hygiene, and thus a greater chance of early exposure to polio when the mother’s antibodies would be activated, enabling the child to fight off a subsequent infection.) Consequently children of non-consenting parents should not be included in the control group, but rather analyzed separately.\nConsequently a second experiment was conducted in which both the treatment and control group came from the same population of children whose parents consented to vaccination. Those children would be assigned at random either to the treatment (vaccinated) or control (unvaccinated) group. Moreover, this was to be a “double-blind” experiment: the assignment of each participating child would be closely held among the administrators of the experiment, and not shared with participating parents, medical staff responsible for injections (of either the vaccine or a saline solution), or physicians determining whether a paricipating child had or had not contracted polio. Children whose parents did not consent to vaccination would be tracked separately from others. Here are the results of the double-blind experiment.\n\n\n\n\nTable 5.5: Polio rate per 100,000 subjects (double-blind study)\n\n\n\n\nDouble-Blind: polio rate per 100,000 subjects\n\n\nassignment\nsize\nrate\n\n\n\n\ntreatment\n200000\n28\n\n\ncontrol\n200000\n71\n\n\nno_consent\n350000\n46\n\n\n\n\n\n\n\n\nBoth experments provide strong evidence of the effectiveness of the vaccine. In addition, the second experiment bears out the concern that parental consent could be a confounding factor. Among the children of consenting parents who served as controls, and thus were not vaccinated, the polio rate is markedly higher than in the NFIP experiment where controls consisted of children in grades 1 and 3, some of whose parents would not have consented to vaccination had they been asked.\nKey points:\n\nenlisting experts to identify potential confounding factors is essential to good experimental design;\ndouble-blind experiments are often required to avoid potential unwitting bias in treatment-control assignments or the determination of experimental outcomes.\n\n\n\n5.4.2 Portacaval Shunt\nIn some cases of cirrhosis of the liver, the patient may start to hemorrhage and bleed to death. One treatment involves surgery to redirect the flow of blood through a portacaval shunt. The operation to create the shunt is long and hazardous. Do the benefits outweigh the risks? The effectiveness of this surgery has been evaluated in 51 studies, summarized in the following table.\n\n\n\n\nTable 5.6: Shunt evaluations: degree of enthusiasm per study design\n\n\n\n\nShunt evaluations: degree of enthusiasm per study design\n\n\ndesign\nmarked\nmoderate\nnone\n\n\n\n\nNo controls\n24\n7\n1\n\n\nControls, but not randomized\n10\n3\n2\n\n\nRandomized controlled\n0\n1\n3\n\n\n\n\n\n\n\n\nThere were 32 studies without controls (first line in the table): 24/32 of these studies, or 75%, were markedly enthusiastic about the shunt, concluding that the benefits definitely outweighed the risks. In 15 studies there were controls, but assignment to treatment or control was not randomized. Only 10/15, or 67%, were markedly enthusiastic about the shunt. But the 4 studies that were randomized and controlled showed the surgery to be of little or no value. The badly designed studies exaggerated the value of this risky surgery.\nA randomized, controlled experiment begins with a well-defined patient population. Some are eligible for the trial. Others are ineligible: they may be too sick to undergo the treatment, or they may have the wrong kind of disease, or they may not consent to participate. Eligibility is determined first; then the eligible patients are randomized to treatment or control. That way, the comparison is made only among patients who could have received the therapy. The bottom line: the control group is like the treatment group. By contrast, with poorly-controlled studies, ineligible patients may be used as controls. Moreover, even if controls are selected among those eligible for surgery, the surgeon may choose to operate only on the healthier patients while sicker patients are put in the control group.\nThis sort of bias seems to have been at work in the poorly-controlled studies of the portacaval shunt. In both the well-controlled and the poorly-controlled studies, about 60% of the surgery patients were still alive 3 years after the operation. In the randomized controlled experiments, the percentage of controls who survived the experiment by 3 years was also about 60%. But only 45% of the controls in the non-randomized experiments survived for 3 years.\n\n\n\n\nTable 5.7: Survival rate per (design, assignment)\n\n\n\n\n% patients alive after 3 years per (design, assignment)\n\n\ndesign\nsurgery\ncontrol\n\n\n\n\nrandomized\n60\n60\n\n\nnon_randomized\n60\n45\n\n\n\n\n\n\n\n\nIn both types of studies, the surgeons seem to have used similar criteria to select patients eligible for surgery. Indeed, the survival rates for the surgery group are about the same in both kinds of studies. So, what was the crucial difference? With the randomized controlled experiments, the controls were similar in general health to the surgery patients. With the poorly controlled studies, there was a tendency to exclude sicker patients from the surgery group and use them as controls. That explains the bias in favor of surgery.\n\n\n5.4.3 Repeated Weighing of NB10\nIn an ideal world, the same thing measured repeatedly would yield the same result. In practice, each result is thrown off by chance error that changes from measurement to measurement. One of the earliest scientists to deal with this problem was Tycho Brahe´ (1546–1601), the Danish astronomer. But it was probably noticed first in the market place, as merchants weighed out spices and measured off lengths of silk. There are several questions about chance errors. Where do they come from? How much is likely to cancel out in the average? The first question has a short answer: in most cases, nobody knows. We’ll address the second question via an example of precision weighing done at the National Bureau of Standards (NBS), now the National Institute of Science and Technology (NIST).\nFirst, a brief explanation of standard weights. Stores weigh merchandise on scales. The scales are checked periodically by local weights-and-measures officials, using local standard weights. The local standards too must be periodically checked against external standards, and so on. Eventually weights are calibrated against national standards. In the US that is done by NBS, now NIST.\nThis chain of comparisons ends at the International Prototype Kilogram (for short, The Kilogram), a platinum-iridium weight held at the International Bureau of Weights and Measures near Paris. By international treaty (The Treaty of the Meter, 1875) “one kilogram” was defined to be the weight of this object under standard conditions. All other weights are determined relative to The Kilogram. For instance, The Pound = 0.4539237 of The Kilogram.\nEach country that signed the Treaty of the Meter got a national prototype kilogram, whose exact weight had been determined as accurately as possible relative to The Kilogram. These prototypes were distributed by lot, and the United States got Kilogram #20. The values of all the U.S. national standards are determined relative to K20\nIn the US, accuracy in weighing at the supermarket ultimately depends on the accuracy of the calibration work by NBS/NIST. One basic issue is reproducibility: if a measurement is repeated, how much will it change? We will discuss the results for one such weight, called NB 10, whose nominal value is 10 grams (about the weight of two nickels). In fact, the people who manufactured NB 10 tried to make it weigh 10 grams and missed by a little. NB 10 was acquired by NBS around 1940, and they’ve weighed it many times since then. We’ll look at 100 of these measurements, which were made in the same room, on the same apparatus, by the same technicians. Every effort was made to follow the same procedure each time. All the factors known to affect the results, like air pressure or temperature, were kept as constant as possible.\nHere are the first 5 measurements of NB10 in grams, along with the deficit (shortfall) from 10 grams expressed in micrograms.\n\n\n\n\nTable 5.8: NB10: measurement (grams) and deficit (micrograms)\n\n\n\n\nNB10: measurement (grams) and deficit (micrograms)\n\n\nmeasurement\ndeficit\n\n\n\n\n9.999591\n409\n\n\n9.999600\n400\n\n\n9.999594\n406\n\n\n9.999601\n399\n\n\n9.999598\n402\n\n\n\n\n\n\n\n\nHere are all 100 measurements expressed as deficits in micrograms.\n\n\n\n\nTable 5.9: NB10 (index, deficit)\n\n\n\n\nNB10 (index, deficit)\n\n\nindex\ndeficit\nindex\ndeficit\nindex\ndeficit\nindex\ndeficit\n\n\n\n\n1\n409\n26\n397\n51\n404\n76\n404\n\n\n2\n400\n27\n407\n52\n406\n77\n401\n\n\n3\n406\n28\n401\n53\n407\n78\n404\n\n\n4\n399\n29\n399\n54\n405\n79\n408\n\n\n5\n402\n30\n401\n55\n411\n80\n406\n\n\n6\n406\n31\n403\n56\n410\n81\n408\n\n\n7\n401\n32\n400\n57\n410\n82\n406\n\n\n8\n403\n33\n410\n58\n410\n83\n401\n\n\n9\n401\n34\n401\n59\n401\n84\n412\n\n\n10\n403\n35\n407\n60\n402\n85\n393\n\n\n11\n398\n36\n423\n61\n404\n86\n437\n\n\n12\n403\n37\n406\n62\n405\n87\n418\n\n\n13\n407\n38\n406\n63\n392\n88\n415\n\n\n14\n402\n39\n402\n64\n407\n89\n404\n\n\n15\n401\n40\n405\n65\n406\n90\n401\n\n\n16\n399\n41\n405\n66\n404\n91\n401\n\n\n17\n400\n42\n409\n67\n403\n92\n407\n\n\n18\n401\n43\n399\n68\n408\n93\n412\n\n\n19\n405\n44\n402\n69\n404\n94\n375\n\n\n20\n402\n45\n407\n70\n407\n95\n409\n\n\n21\n408\n46\n406\n71\n412\n96\n406\n\n\n22\n399\n47\n413\n72\n406\n97\n398\n\n\n23\n399\n48\n409\n73\n409\n98\n406\n\n\n24\n402\n49\n404\n74\n400\n99\n403\n\n\n25\n399\n50\n402\n75\n408\n100\n404\n\n\n\n\n\n\n\n\nHere is the distribution of the NB10 deficits expressed as a histogram.\n\n\n\n\n\n\n\n\nFigure 5.1: NB10: micrograms below 10 grams\n\n\n\n\n\nThe average deficit is 405 micrograms, and the standard deviation is about 6.5 micrograms.\nThe next figure shows the deficits expressed in standard units. The histogram has been smoothed into an approximate density function, with a standard normal curve superimposed.\n\n\n\n\n\n\n\n\nFigure 5.2: NB10: z-scores and standard normal curve\n\n\n\n\n\nThe first of the two figures above shows that the average deficit of NB10 is around 405 micrograms. Thus the actual weight of NB10 is likely to be about 10 grams minus 405 micrograms. Knowing this, one could calibrate other weights against NB10, adjusting for the fact that NB10 weighs this much less than the nominal 10 grams. Without such knowledge, taking NB10 at face value as weighing 10 grams would amount to a biased measurement system.\nThe second of the two figures above reveals measurements whose standard units (z-scores) are near \\(\\pm 5\\). (The indices of the outlying measurements are #86 and #94.) NBS could find no reason to disqualify these values. Now, a sample of size 100 from a normal distribution would be highly unlikely to include such large deviations from the average value. We conclude that this very carefully controlled measurement process produces values that do not follow the normal distribution.\nKey points:\n\nHigh-precision measurement systems typically reveal chance errors in the measurement process. Before relying on such systems one should determine the likely size of such chance errors by generating repeated measurements and calculating their standard deviation (or some other measure of the spread in values).\nA small proportion of outlying values are possible even from the most careful measurement process. These outliers strongly influence the calculated mean and standard deviation of the data.\nOne should consider the possibility that a measurement system is biased. In this case, each measurement can be represented as the sum of three terms: the actual (unknown) value + bias + chance error.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "study-design.html#accuracy-of-the-sample-average",
    "href": "study-design.html#accuracy-of-the-sample-average",
    "title": "5  Sampling and Study Design",
    "section": "5.5 Accuracy of the Sample Average",
    "text": "5.5 Accuracy of the Sample Average\nThe last example showed that NB10, nominally weighing 10 grams, actually weighs a bit less, namely, about 405 micrograms less than 10 grams. Of course a new batch of measurements of NB10 might yield an average deficit differing a bit from 405 micrograms. Just how different might that new average be? Here is a quick review of the mathematical answer to that question.\nWe model a sequence of measured deficits\n\\[\n\\begin{align}\n  D_{\\bullet} &= (D_1, \\ldots, D_n)\n\\end{align}\n\\qquad(5.1)\\]\nas independent, identically distributed random variables (whose common distribution is something like the histogram shown above). Each measured deficit \\(D_j\\) is represented as the sum of \\(\\mu_d\\), the actual (but unknown) deficit, plus \\(\\epsilon_j\\), a chance error.\n\\[\n\\begin{align}\n  D_j &= \\mu_D + \\epsilon_j\n\\end{align}\n\\qquad(5.2)\\]\nWe assume the population distribution of the chance errors has the same shape and scale as that of measured deficits, but with a population mean value of zero. Thus the chance errors have the same population standard deviation, \\(\\sigma_D\\), as the measured deficits. Based on the 100 NB10 measurements this unknown value is not too far from 6.5 micrograms.\nThen \\(S(D_{\\bullet})\\), the sum of \\(n\\) measured deficits, has the following expected value (population mean) and population variance.\n\\[\n\\begin{align}\n  S(D_{\\bullet}) &= \\sum_{j = 1}^n D_j \\\\\n  &= \\sum_{j = 1}^n ( \\mu_D + \\epsilon_j ) \\\\\n  &= n \\; \\mu_D \\; + \\; \\sum_{j = 1}^n \\epsilon_j \\\\\n  \\\\\n  E \\left\\{ S(D_{\\bullet}) \\right\\} &= n \\; \\mu_D \\\\\n  \\\\\n  Var \\left\\{ S(D_{\\bullet}) \\right\\} &= n \\; Var \\left\\{ \\epsilon_j \\right\\} \\\\\n  &= n \\; \\sigma_D^2 \\\\\n\\end{align}\n\\qquad(5.3)\\]\nDenoting the sample average as \\(A(\\cdot)\\), we have\n\\[\n\\begin{align}\n  A(D_{\\bullet}) &= \\frac{1}{n} S(D_{\\bullet}) \\\\\n  &= \\mu_D \\; + \\; A(\\epsilon_{\\bullet}) \\\\\n  \\\\\n  E \\left\\{ A(D_{\\bullet}) \\right\\} &= \\mu_D \\\\\n  \\\\\n  Var \\left\\{ A(D_{\\bullet}) \\right\\} &= \\frac{1}{n^2} Var \\left\\{ S(D_{\\bullet}) \\right\\} \\\\\n  &= \\frac{1}{n^2}  \\; n \\; \\sigma_D^2 \\\\\n  &= \\frac{1}{n}  \\; \\sigma_D^2 \\\\\n  \\\\\n  SD \\left\\{ A(D_{\\bullet}) \\right\\} &= \\sqrt(Var \\left\\{ A(D_{\\bullet}) \\right\\}) \\\\\n  &= \\frac{1}{\\sqrt{n}}  \\; \\sigma_D \\\\\n\\end{align}\n\\qquad(5.4)\\]\nThus the population standard deviation of the sample average \\(A(D_{\\bullet})\\) can be estimated from the sample standard deviation:\n\\[\n\\begin{align}\n  \\hat{SD} \\left\\{ A(D_{\\bullet}) \\right\\} &= \\frac{1}{\\sqrt{n}}  \\; \\hat{\\sigma}_D \\\\\n\\end{align}\n\\qquad(5.5)\\]\nReturning to our sample of 100 NB10 measurements, with a sample average of about 405 micrograms and a sample standard deviation of about 6.5 micrograms, we have\n\\[\n\\begin{align}\n  \\hat{SD} \\left\\{ A(D_{\\bullet}) \\right\\} &= \\frac{1}{\\sqrt{100}}  \\; 6.5 \\\\\n  &= 0.65\n\\end{align}\n\\qquad(5.6)\\]\nOur sample average of 405 micrograms probably deviates from the actual deficit of NB10 by no more 2 micrograms or so.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "study-design.html#team-exercises",
    "href": "study-design.html#team-exercises",
    "title": "5  Sampling and Study Design",
    "section": "5.6 Team Exercises",
    "text": "5.6 Team Exercises\n\nA teaching assistant gives a quiz to his section. There are 10 questions on the quiz and no part credit is given. After grading the papers, the TA writes down for each student the number of questions the student got right and the number wrong. The average number of right answers is 6.4 with an SD of 2.0. The average number of wrong answers is [?] with an SD of [?]. Fill in the blanks, or do you need the data? Explain briefly.\nA large, representative sample of Americans was studied by the Public Health Service, in the Health and Nutrition Examination Survey (HANES2). The percentage of respondents who were left-handed decreased steadily with age, from 10% at 20 years to 4% at 70. “The data show that many people change from left-handed to right-handed as they get older.” True or false? Why? If false, how do you explain the pattern in the data?\nFor a certain group of women, the 25th percentile of height is 62.2 inches and the 75th percentile is 65.8 inches. The histogram follows the normal curve. Find the 90th percentile of the height distribution.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "study-design.html#resources",
    "href": "study-design.html#resources",
    "title": "5  Sampling and Study Design",
    "section": "5.7 Resources",
    "text": "5.7 Resources\n“Statistics, 4/e” by Freedman, Pisani, and Purves ([FPP](https://www.goodreads.com/book/show/147358.Statistics))\n[Sampling Techniques, 3/e](https://www.google.co.uk/books/edition/Sampling_Techniques/8Y4QAQAAIAAJ?hl=en) by William G. Cochran",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "study-design.html#footnotes",
    "href": "study-design.html#footnotes",
    "title": "5  Sampling and Study Design",
    "section": "",
    "text": "Gallup and others used quota sampling in pre-election surveys from 1936 though 1948. Interviewers consistently chose a disproportionate number of people who wound up voting Republican, but until 1948, a close election, the Democratic lead was larger than this oversampling of Republicans.↩︎\nOn the other hand, if a percentage or average is known to vary across identifiable subsets (strata) of the population, then stratified sampling can yield more accurate estimates than simple random sampling.↩︎\nEach interviewer is given a prescribed protocol for selecting a an eligible voter within the household (e.g., oldest eligible female, if present, or else youngest eligible male), with these prescriptions possibly varying among interviewers.↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Study Design</span>"
    ]
  },
  {
    "objectID": "dirichlet-dstn.html",
    "href": "dirichlet-dstn.html",
    "title": "9  The Dirichlet Distribution",
    "section": "",
    "text": "9.1 Background\nLatent Dirichlet Allocation (LDA) was proposed as a method of topic modeling in a 2003 paper by Blei, Ng, and Jordan1. The method is briefly mentioned in Part 1 of the course. Several course participants requested a more detailed description. This note prepares for the requested response by introducing the Dirichlet and Multinomial distributions.2",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Dirichlet Distribution</span>"
    ]
  },
  {
    "objectID": "dirichlet-dstn.html#the-multinomial-distribution",
    "href": "dirichlet-dstn.html#the-multinomial-distribution",
    "title": "9  The Dirichlet Distribution",
    "section": "9.2 The Multinomial Distribution",
    "text": "9.2 The Multinomial Distribution\nConsider a categorical (qualitative) random variable \\(X\\) that randomly selects one of \\(K\\) distinct categories \\(\\{c_1, \\ldots, c_K \\}\\).\n\\[\n\\begin{align}\n  X &\\in (c_1, \\ldots, c_K) \\\\\n  \\\\\n  p_k &= P(X = c_k) &gt; 0 \\\\\n  \\\\\n  \\sum_{k = 1}^K p_k &= 1 \\\\\n\\end{align}\n\\qquad(9.1)\\]\nProbability vector \\(p_{\\bullet} = (p_1, \\ldots, p_K)\\) is thus the probability distribution of \\(X\\) on \\(\\{c_1, \\ldots, c_K \\}\\).\nIf the categories are ordered, we might represent \\(X\\) numerically as a random index \\(k \\in \\{1, \\ldots, K \\}\\). But for present purposes we suppose the categories are not ordered, and we represent \\(X\\) as a vector of random indicator variables3.\n\\[\n\\begin{align}\n  I_{\\bullet} &= (I_1, \\ldots, I_K) \\\\\n  \\\\\n  I_k &= 1 \\text{, if } X = c_k \\\\\n  I_k &= 0 \\text{, if } X \\ne c_k  \\\\\n  \\\\\n  I_{\\bullet} &= \\mathbb{e}_1 = (1, 0, \\ldots, 0)  \\text{, with probability } p_1\\\\\n  I_{\\bullet} &= \\mathbb{e}_2 = (0, 1, \\ldots, 0)  \\text{, with probability } p_2\\\\\n  \\vdots \\\\\n  I_{\\bullet} &= \\mathbb{e}_K = (0, 0, \\ldots, 1)  \\text{, with probability } p_K \\\\\n\\end{align}\n\\qquad(9.2)\\]\nThat is, the random indicator vector \\(I_{\\bullet}\\) selects one of the Euclidean basis vectors \\(\\mathbb{e}_k \\in \\mathbb{R}^K\\) with probability \\(p_k\\). Therefore the expected value of \\(I_{\\bullet}\\) equals the vector of category probabilities \\(p_{\\bullet} = (p_1, \\ldots, p_K)\\).\n\\[\n\\begin{align}\n  E(I_k) &= P(I_k = 1) \\\\\n  & = P(X = c_k) \\\\\n  & = p_k \\\\\n  \\\\\n  E(I_{\\bullet}) &= (E(I_1), \\ldots, E(I_K)) \\\\\n  &= (p_1, \\ldots, p_K) \\\\\n  &= p_{\\bullet} \\\\\n\\end{align}\n\\qquad(9.3)\\]\nNow suppose that \\((X_1, \\ldots, X_n)\\) are independent random variables all having the same distribution as \\(X\\). Corresponding to \\(X_{\\nu}\\) we have an indicator vector that we’ll denote as \\(I_{\\bullet}^{(\\nu)}\\). Let \\(S_{\\bullet}\\) denote the sum over the \\(n\\) indicator vectors \\(\\{ I_{\\bullet}^{(\\nu)} \\}_{\\nu}\\).\n\\[\n\\begin{align}\n  S_{\\bullet} &= \\sum_{\\nu = 1}^n I_{\\bullet}^{(\\nu)} \\text{, so that } \\\\\n  \\\\\n  S_k &= \\sum_{\\nu = 1}^n I_k^{(\\nu)} \\\\\n  &= \\text{ number of } \\nu \\text{ such that } X_{\\nu} = c_k\n\\end{align}\n\\qquad(9.4)\\]\nFor any given set of possible counts \\(s_{\\bullet} = (s_1, \\ldots, s_K)\\), that is, of non-negative integers summing to \\(n\\), and for a given probability vector \\(p_{\\bullet}\\), the probability that \\(S_{\\bullet} = s_{\\bullet}\\) is as follows.\n\\[\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet} \\; | \\; p_{\\bullet}) &= {n \\choose s_{\\bullet}} \\prod_{k = 1}^K p_k^{s_k}\n\\end{align}\n\\qquad(9.5)\\]\nwhere\n\\[\n\\begin{align}\n  {n \\choose s_{\\bullet}} &= \\frac{n!}{s_1! s_2! \\cdots s_K!}\n\\end{align}\n\\qquad(9.6)\\]\ngives the number of assignments of \\(n\\) objects to the \\(K\\) categories such that each category \\(c_k\\) receives the prescribed number \\(s_k\\) of objects.\nThe probability distribution of \\(S_{\\bullet}\\) is called the multinomial distribution.\nFor \\(K = 2\\), this simplifies to the binomial distribution, with parameters \\((n, p)\\), where \\((p_1, p_2) = (p, \\; 1 - p)\\). If \\(\\nu\\) denotes the observed number of succeses in \\(n\\) Bernoulli trials, then \\((s_1, s_2) = (\\nu, \\; n - \\nu)\\).",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Dirichlet Distribution</span>"
    ]
  },
  {
    "objectID": "dirichlet-dstn.html#bayesian-inference",
    "href": "dirichlet-dstn.html#bayesian-inference",
    "title": "9  The Dirichlet Distribution",
    "section": "9.3 Bayesian Inference",
    "text": "9.3 Bayesian Inference\nThe Bayesian approach to statistical estimation provides a framework for representing the state of knowledge, or degree of uncertainty, about a model parameter before and after collecting relevant data.\nAs an example consider the binomial distribution mentioned above, where we are estimating the probability \\(p\\) of success based on a sequence of \\(n\\) independent Bernoulli trials. Following the notation above, we use two dependent indicator random variables \\((I_1, I_2)\\) to represent success and failure respectively, with \\(I_2 = 1- I_1\\), and with \\((p_1, p_2) = (p, \\; 1-p)\\).\nPrior to observing the sequence of Bernoulli trials, we might represent the state of information about \\(p\\) as a uniform distribution, so that each possible value of \\(p \\in [0, 1]\\) is deemed equally likely. Alternatively, if the Bernoulli sequence represents the outcomes of tossing a coin that is presumed fair, or nearly fair, we might represent that information as a probability distribution having a mode at the value \\(p = 1/2\\).\nMore specifically, it turns out to be mathematically convenient to represent prior information about probability \\(p\\) as a member of the beta family of probability distributions over the unit interval. The uniform distribution is the special case of setting beta shape parameters to the values \\((1, 1)\\). Alternatively setting the shape parameters to \\((2, 2)\\) gives a density function symmetric about the mode at \\(p = 1/2\\).\n\n\n\n\n\n\n\n\nFigure 9.1: Alternative prior densities of parameter p\n\n\n\n\n\nIn general, a beta distribution having shape parameters \\((\\alpha, \\beta)\\) has the following density function.\n\\[\n\\begin{align}\n  P(p) &= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\\n  \\\\\n  & \\text{with }  \\alpha &gt; 0 \\text{, and } \\beta &gt; 0\n\\end{align}\n\\qquad(9.7)\\]\nSuppose now that we adopt the distribution just mentioned, \\(\\mathcal{Beta}(\\alpha, \\beta)\\), as the prior distribution of success probability \\(p\\), for some specified positive parameter values \\((\\alpha, \\beta)\\). We then observe \\(s_1\\) successes and \\(s_2\\) failures from \\(n = s_1 + s_2\\) independent Bernoulli trials. Based on these observations we update the prior distribution to form the posterior distribution of \\(p\\) as follows.\n\\[\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\\n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\\n\\end{align}\n\\qquad(9.8)\\]\nNote that\n\\[\n\\begin{align}\n  P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) &= {n \\choose s_1} p^{s_1} (1 - p)^{n - s_1} \\times \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\\n  &= {n \\choose s_1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{s_1 + \\alpha - 1} (1 - p)^{n - s_1 + \\beta - 1} \\\\\n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\\\\n\\end{align}\n\\qquad(9.9)\\]\nso that\n\\[\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet}) &= \\int_0^1 P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) \\,dp \\\\\n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\int_0^1 p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\,dp \\\\\n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s_1)\\Gamma(\\beta + s_2)}{\\Gamma(\\alpha + \\beta + s_1 + s_2)} \\\\\n\\end{align}\n\\qquad(9.10)\\]\nConsequently, the ratio of the last two expressions gives\n\\[\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\\n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\\n  &= \\frac{\\Gamma(\\alpha + \\beta + s_1 + s_2)}{\\Gamma(\\alpha + s_1) \\Gamma(\\beta + s_2)} p^{\\alpha + s_1 - 1} (1-p)^{\\beta + s_2 -1}\n\\end{align}\n\\qquad(9.11)\\]\nThat is, the posterior distribution is \\(\\mathcal{Beta}(\\alpha + s_1, \\beta + s_2)\\).\nThis is the “mathematical convenience” previously alluded to: the prior and posterior probability distributions of \\(p\\) belong to the same family of parametric probability distributions, namely the \\(\\mathcal{Beta}\\) family. For this reason the \\(\\mathcal{Beta}\\) family is said to be conjugate to the binomial family.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Dirichlet Distribution</span>"
    ]
  },
  {
    "objectID": "dirichlet-dstn.html#the-dirichlet-probability-distribution",
    "href": "dirichlet-dstn.html#the-dirichlet-probability-distribution",
    "title": "9  The Dirichlet Distribution",
    "section": "9.4 The Dirichlet Probability Distribution",
    "text": "9.4 The Dirichlet Probability Distribution\n\n9.4.1 Definition\nThe binomial distribution is a special case of the multinomial distribution in which the number of categories \\(K\\) is equal to 2. (In the discussion above we referred to the categories as success and failue, respectively.) More generally, for a fixed integer \\(K \\ge 2\\), the family of distributions conjugate to the multinomial family of distributions over \\(K\\) categories is the following Dirichlet family.\n\\[\n\\begin{align}\n  P(p_{\\bullet}) &= \\frac{\\Gamma(\\alpha_1+ \\cdots + \\alpha_K)}{\\Gamma(\\alpha_1) \\times \\cdots \\times \\Gamma(\\alpha_K)} \\prod_{k = 1}^K p_k^{\\alpha_k - 1} \\\\\n  \\\\\n  & \\text{with }  \\alpha_k &gt; 0 \\text{ for } k \\in \\{1, \\ldots, K \\}\n\\end{align}\n\\qquad(9.12)\\]\nThis is the \\(\\mathcal{Dirichlet}(\\alpha_{\\bullet})\\) distribution over \\(p_{\\bullet}\\), where probability vector \\(p_{\\bullet}\\) ranges over the \\(K - 1\\) dimesional simplex such that each component \\(p_k\\) is non-negative and all the components \\((p_1, \\cdots, p_K)\\) together sum to unity.\n\n\n9.4.2 Special Case: \\(\\alpha_k = \\frac{\\alpha_{+}}{K}\\)\nWe will denote the sum of the components of \\(\\alpha_{\\bullet} = (\\alpha_1, \\ldots, \\alpha_K)\\) as \\(\\alpha_{+}\\).4\n\\[\n\\begin{align}\n  \\alpha_{+} &= \\sum_{k = 1}^K \\alpha_k \\\\\n\\end{align}\n\\qquad(9.13)\\]\nConsider the special case in which all the components of \\(\\alpha_{\\bullet}\\) have the same value\n\\[\n\\begin{align}\n  \\alpha_k &= \\frac{\\alpha_{+}}{K} \\\\\n  & \\text{for } k \\in \\{1, \\dots, K \\}\n\\end{align}\n\\qquad(9.14)\\]\nThe probability distribution is then symmetric in the components of \\(p_{\\bullet}\\), and \\(\\alpha_{+}\\) is referred to as the concentration parameter. The uniform distribution over the domain of \\(p_{\\bullet}\\) (that is, over the \\(K - 1\\) dimensional simplex) is obtained by setting \\(\\alpha_{+} = K\\). Setting \\(\\alpha_{+} &gt; K\\) concentrates the distribution around the centroid\n\\[\n\\begin{align}\n  p_{\\textbf{ctr}} &= (\\frac{1}{K}, \\ldots, \\frac{1}{K}) \\\\\n\\end{align}\n\\qquad(9.15)\\]\nFor example the previous figure shows two alternative concentrations of the symmetric beta density function of scalar parameter \\(p\\), where \\((p_1, p_2) = (p, \\; 1 - p)\\), and \\(p_{\\textbf{ctr}} = (\\frac{1}{2}, \\frac{1}{2})\\).\nAlternatively, setting \\(\\alpha_{+} &lt; K\\) concentrates the distribution away from the centroid toward the corners of the simplex.\n\n\n9.4.3 Posterior Distribution\nRecall that \\(S_{\\bullet}\\) constructed above has a multinomial distribution if it can be represented as the sum of \\(n\\) independent trials, each trial yielding one of the Euclidean basis vectors \\(\\mathbb{e}_k\\) with probability \\(p_k\\). Then \\(S_k\\), the \\(k^{th}\\) component of \\(S_{\\bullet}\\), counts the number of trials yielding \\(\\mathbb{e}_k\\).\nNow suppose that \\(p_{\\bullet}\\) has prior distribution \\(\\mathcal{Dirichlet}(\\alpha_{\\bullet})\\), for some specification of \\(\\alpha_{\\bullet}\\), and that the observed outcome of the \\(n\\) trials is a given vector \\(s_{\\bullet}\\) of counts. What is the posterior distribution of \\(p_{\\bullet}\\) given the observation \\(S_{\\bullet} = s_{\\bullet}\\)? This turns out to be\n\\[\n\\begin{align}\n  \\{ p_{\\bullet} \\; | \\; s_{\\bullet} \\} &\\sim \\mathcal{Dirichlet}(\\alpha_{\\bullet} + s_{\\bullet})\n\\end{align}\n\\qquad(9.16)\\]\nThat is, the Dirichlet family is conjugate to the multinomial family, just as the beta family is conjugate to the binomial family.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Dirichlet Distribution</span>"
    ]
  },
  {
    "objectID": "dirichlet-dstn.html#references",
    "href": "dirichlet-dstn.html#references",
    "title": "9  The Dirichlet Distribution",
    "section": "9.5 References",
    "text": "9.5 References\n\n\n\n\n“Dirichlet Distribution | Wikipedia.” 2025. Wikipedia, July. https://en.wikipedia.org/wiki/Dirichlet_distribution.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Dirichlet Distribution</span>"
    ]
  },
  {
    "objectID": "dirichlet-dstn.html#footnotes",
    "href": "dirichlet-dstn.html#footnotes",
    "title": "9  The Dirichlet Distribution",
    "section": "",
    "text": "See (BNJ2003LDA?).↩︎\nSee “Dirichlet Distribution | Wikipedia” (2025).↩︎\nIn machine learning this mapping of a categorical variable to an indicator vector is called “one-hot encoding”.↩︎\nAn alternative notation for the sum of \\((\\alpha_1, \\ldots, \\alpha_K)\\) is \\(\\alpha_0\\).↩︎",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Dirichlet Distribution</span>"
    ]
  },
  {
    "objectID": "latent-dirichlet-alloc.html",
    "href": "latent-dirichlet-alloc.html",
    "title": "10  Latent Dirichlet Allocation",
    "section": "",
    "text": "10.1 Introduction",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Latent Dirichlet Allocation</span>"
    ]
  },
  {
    "objectID": "latent-dirichlet-alloc.html#introduction",
    "href": "latent-dirichlet-alloc.html#introduction",
    "title": "10  Latent Dirichlet Allocation",
    "section": "",
    "text": "10.1.1 Text Analysis\nPart 1, Session 2b, of the course presents an overview of text analysis based on Silge and Robinson (2017). The authors describe topic models, and in particular the method of Latent Dirichlet Allocation (LDA). Several course participants requested a more detailed description. This note is a response to that request.\n\n\n10.1.2 Topic Models1\nA topic model treats documents as mixtures of topics, where a topic is a probability distribution over the elements of a vocabulary. According to the model, each document is generated in the following steps:\n\nGenerate a document-specific probability distribution over a fixed set of topics; and then\nFor each word-position in that document:\n\n\nSelect a topic at random; and\nGenerate a vocabulary term from the selected topic.\n\nStandard statistical techniques can be used to invert this process, inferring the set of topics that were responsible for generating a collection of documents.\n\n\n10.1.3 LDA Method\nLatent Dirichlet Allocation (LDA) was proposed by (BNJ2003LDA?) as a method of topic modeling. Silge and Robinson (2017) use the R function topicmodels::LDA() with default parameter setting method = VEM, which denotes the Variational Expectation Maximization (EM) algorithm. In this note we use an alternative implementation of LDA described below.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Latent Dirichlet Allocation</span>"
    ]
  },
  {
    "objectID": "latent-dirichlet-alloc.html#book-review-example",
    "href": "latent-dirichlet-alloc.html#book-review-example",
    "title": "10  Latent Dirichlet Allocation",
    "section": "10.2 Book Review Example",
    "text": "10.2 Book Review Example\nAs an experiment we construct a small set of documents from two distinct sources. We then run the documents through the LDA algorithm (without identifying the source of each document) specifying that \\(K = 2\\) topics are to be constructed to see whether and how well the constructed topics match the original sources.\n\n10.2.1 Data\n“Animal Farm” by George Orwell was published in 19452, and nearly forty years later (1984) “The Butter Battle Book” by Dr. Seuss was published3. The books are quite different of course, but they both allude to the Soviet Union in its earlier and later years, respectively.\nAs a toy example of a corpus of documents, we have extracted Wikipedia’s description of the plots of the two books, counting each paragraph as a separate document.\nHere are the successive paragraphs (stripped of punctuation and stop-words) summarizing the two books.\n\nShow the code\naf_bbb_para_tbl |&gt; print(width = 72)\n\n\n\n\nTable 10.1: Paragraphs summarizing ‘Animal Farm’ (AF), then ‘The Butter Battle Book’ (BBB)\n\n\n\n# A tibble: 9 × 3\n  src     pdx para                                                      \n  &lt;chr&gt; &lt;int&gt; &lt;chr&gt;                                                     \n1 AF        1 animal populace poorly run manor farm near willingdon eng…\n2 AF        2 napoleon enacts changes political structure farm replacin…\n3 AF        3 mr frederick neighbouring farmer attacks farm using blast…\n4 AF        4 years pass windmill rebuilt another windmill constructed …\n5 AF        5 napoleon holds dinner party pigs newly allied human farme…\n6 BBB       1 yooks zooks live opposite sides long curving wall yooks w…\n7 BBB       2 race begins zook patrolman named vanitch slingshots yook …\n8 BBB       3 yooks create gun called kickapoo kid loaded powerful pooa…\n9 BBB       4 resolution reached books end generals sides wall poised d…\n\n\n\n\nFrom these paragraphs (each treated as a document) we create the following DocumentTermMatrix object, which is a list that includes:\n\na sparse matrix giving the document-index, the term-index, and the frequency (number of occurrences) of the indexed term; and\na vector of the terms themselves\n\n\n\nShow the code\naf_bbb_dtm_lst &lt;- \n  tm::DocumentTermMatrix(x = af_bbb_para_tbl$ para)\n\n# date()\n# [1] \"Fri Aug  1 10:55:32 2025\"\n\n# af_bbb_dtm_lst  |&gt; object.size(): 44 KB\n# 44464 bytes\n\naf_bbb_dtm_lst\n\n\n&lt;&lt;DocumentTermMatrix (documents: 9, terms: 485)&gt;&gt;\nNon-/sparse entries: 593/3772\nSparsity           : 86%\nMaximal term length: 18\nWeighting          : term frequency (tf)\n\n\nHere is a table showing the frequency of each term within each document (paragraph).\n\nShow the code\naf_bbb_tf_tbl &lt;- af_bbb_dtm_lst |&gt; tidytext::tidy()\naf_bbb_tf_tbl\n\n\n\n\nTable 10.2: Term-frequency: number of occurrences of each term within each document\n\n\n\n# A tibble: 593 × 3\n   document term       count\n   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n 1 1        adopt          1\n 2 1        adult          1\n 3 1        alcoholic      1\n 4 1        animal         3\n 5 1        animalism      2\n 6 1        animals        5\n 7 1        announces      1\n 8 1        aside          1\n 9 1        associates     1\n10 1        assume         1\n# ℹ 583 more rows\n\n\n\n\n\n\n10.2.2 LDA function\nWe now employ function topicmodels::LDA()4 to perform latent Dirichlet allocation. The function determines the probable topic of each word within each document by one of two methods: (1) “VEM” (Variational Expectation Maximization); or else (2) “Gibbs” (Gibbs Sampling). The function output, depending on the choice of method, is a list conforming to the format of either LDA_VEM or else LDA_Gibbs.\nHere we choose the Gibbs Sampling method because it requires less memory. The DocumentTermMatrix (DTM) representing the corpus of documents (paragraphs) is the primary input to topicmodels::LDA(). (Note that the DTM does not identify the source of each document.) We specify that LDA() should construct \\(K = 2\\) topics from this input corpus.\n\n\nShow the code\n# Latent Dirichlet Allocation via Gibbs Sampling\n\nextract_from_list &lt;- FALSE\nif (extract_from_list && topicmodels_loaded) {\n  af_bbb_gs &lt;- af_bbb_dtm_lst |&gt; \n    topicmodels::LDA(\n      # number of topics\n      k = 2, \n      method = \"Gibbs\", \n      # set seed to ensure results can be reproduced\n      control = list(seed = 1234)\n    )\n  \n  ## \n  # extract tibbles from LDA() output\n  ## \n  af_bbb_gs_tbl_lst &lt;- af_bbb_gs |&gt; list_lda_tbls()\n  \n  # z: topic assignment of successive words across corpus\n  af_bbb_gs_z &lt;- af_bbb_gs_tbl_lst$ z_tbl\n  \n  # conc: initial Dirichlet concentration parameters\n  af_bbb_gs_conc &lt;- af_bbb_gs_tbl_lst$ conc_tbl\n  \n  # terms: unique words across the corpus\n  af_bbb_gs_terms &lt;- af_bbb_gs_tbl_lst$ terms_tbl\n  \n  # beta: topic-specific ln(probability) across unique terms\n  af_bbb_gs_beta &lt;- af_bbb_gs_tbl_lst$ beta_wide\n  \n  # beta_terms: append terms, then pivot longer\n  af_bbb_gs_beta_terms &lt;- af_bbb_gs_tbl_lst$ beta_terms\n  \n  # gamma: prob(topic | doc), a D-by-K matrix\n  af_bbb_gs_gamma &lt;- af_bbb_gs_tbl_lst$ gamma_tbl\n  \n  # dw_assign: topic assignment per (doc, term)\n  af_bbb_gs_dw_assign &lt;- af_bbb_gs_tbl_lst$ dw_assign_tbl\n  \n  ## \n  # save output components as TSV files: only as needed\n  ## \n  save_file &lt;- FALSE\n  if (save_file) {\n    \n    # z: topic assignment of successive words across corpus\n    af_bbb_gs_z |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_gs_z.txt\"\n    ))\n    \n    # conc: initial Dirichlet concentration parameters\n    af_bbb_gs_conc |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_gs_conc.txt\"\n    ))\n    \n    # terms: unique words across the corpus\n    af_bbb_gs_terms |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_gs_terms.txt\"\n    ))\n    \n    # beta: topic-specific ln(probability) across unique terms\n    af_bbb_gs_beta |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_gs_beta.txt\"\n    ))\n    \n    # beta_terms: append terms, then pivot longer\n    af_bbb_gs_beta_terms |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_gs_beta_terms.txt\"\n    ))\n    \n    # gamma: prob(topic | doc), a D-by-K matrix\n    af_bbb_gs_gamma |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_gs_gamma.txt\"\n    ))\n    \n    # dw_assign: topic assignment per (doc, term)\n    af_bbb_gs_dw_assign |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_gs_dw_assign.txt\"\n    ))\n    \n  }\n} else {\n  # read previously saved LDA-Gibbs components\n  \n  # z: topic assignment of successive words across corpus\n  af_bbb_gs_z &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_gs_z.txt\"\n  ))\n  \n  # conc: initial Dirichlet concentration parameters\n  af_bbb_gs_conc &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_gs_conc.txt\"\n  ))\n  \n  # terms: unique words across the corpus\n  af_bbb_gs_terms &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_gs_terms.txt\"\n  ))\n  \n  # beta: topic-specific ln(probability) across unique terms\n  af_bbb_gs_beta &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_gs_beta.txt\"\n  ))\n  \n  # beta_terms: append terms, then pivot longer\n  af_bbb_gs_beta_terms &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_gs_beta_terms.txt\"\n  ))\n  \n  # gamma: prob(topic | doc), a D-by-K matrix\n  af_bbb_gs_gamma &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_gs_gamma.txt\"\n  ))\n  \n  # dw_assign: topic assignment per (doc, term)\n  af_bbb_gs_dw_assign &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_gs_dw_assign.txt\"\n  ))\n}\n\n\nFrom the LDA_Gibbs output we extract the following “beta” matrix consisting of two fitted probability vectors: the respective probability distributions of topic 1 and 2 across the unique terms of the corpus.\n\n\nShow the code\n# af_bbb_topics: beta matrix in long format with column \"term\"\n\n# Note: tidytext::tidy(&lt;LDA_Gibbs&gt;) generates error \n# when &lt;LDA_Gibbs&gt; is large and memory is constrained.\n# In the meantime, patch as follows\n\nextract_from_list &lt;- FALSE\nif (extract_from_list) {\n  # beta: K (topics) by N (terms) of log-probabilities\n  beta_mat           &lt;- af_bbb_gs@beta\n  colnames(beta_mat) &lt;- af_bbb_gs@terms\n  rownames(beta_mat) &lt;- paste0(\"topic_\", 1:2)\n  \n  beta_tbl           &lt;- beta_mat |&gt; \n    tibble::as_tibble(rownames = \"topic\") |&gt; \n    dplyr::mutate(topic = 1:2)\n  \n  beta_long          &lt;- beta_tbl |&gt; \n    tidyr::pivot_longer(\n      cols      = - topic, \n      names_to  = \"term\", \n      values_to = \"ln_prob\"\n    ) |&gt; \n    dplyr::arrange(term)\n  \n  # af_bbb_topics: N by K tibble of probabilities\n  af_bbb_topics &lt;- beta_long |&gt; \n    dplyr::mutate(\n      beta = exp(ln_prob)\n    ) |&gt; \n    dplyr::select(- ln_prob)\n  \n  # save af_bbb_topics as TSV file only as needed\n  save_file &lt;- FALSE\n  if (save_file) {\n    af_bbb_topics |&gt; readr::write_tsv(here::here(\n      \"data\", \"retain\", \"af_bbb_topics.txt\"\n    ))\n  }\n  \n} else {\n  # read in previously saved file\n  af_bbb_topics &lt;- readr::read_tsv(here::here(\n    \"data\", \"retain\", \"af_bbb_topics.txt\"\n  ))\n}\n\n# af_bbb_topics |&gt; object.size(): 49 KB\n# 48992 bytes\n\naf_bbb_topics |&gt; print(n = 4, digits = 2)\n\n\n# A tibble: 970 × 3\n  topic term          beta\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1     1 abolishes 0.00271 \n2     2 abolishes 0.000245\n3     1 abridged  0.000247\n4     2 abridged  0.00269 \n# ℹ 966 more rows\n\n\n\n\n10.2.3 Output interpretation\nHere are the most probable terms for each of the two constructed topics, along with their probabilities (beta), shown as a bar chart.\n\n\nShow the code\ng_af_bbb_top_terms &lt;- af_bbb_top_terms |&gt; \n  dplyr::mutate(term = tidytext::reorder_within(\n    x = term, \n    by = beta, \n    within = topic)) |&gt; \n  ggplot2::ggplot(mapping = aes(\n    x = beta, y = term, fill = factor(topic)\n  )) + \n  ggplot2::geom_col(show.legend = FALSE) + \n  ggplot2::facet_wrap(~ as_factor(topic), scales = \"free\") + \n  tidytext::scale_y_reordered() + \n  ggplot2::labs(title = \"AF-BBB paragraphs: most probable terms by topic\")\ng_af_bbb_top_terms\n\n\n\n\n\n\n\n\nFigure 10.1: AF-BBB paragraphs: most probable terms by topic\n\n\n\n\n\nTopic 1’s top ten terms include “yooks” and “zooks”, which do not appear in the top ten terms of topic 2. So topic 1 seems to match “The Butter Battle Book”. Similarly, topic 2’s top ten terms include “napoleon”, “snowball”, and “jones”, which match “Animal Farm”. Note however that “farm” appears under topic 1 and “battle” appears under topic 2.\nAnother way to compare topics 1 and 2 is to examine the terms shared by the two topics and then find the terms having the biggest disparity in (topic, term) probability (beta). Here’s a bar chart showing the more prominent differences, expressed as\n\\[\n\\begin{align}\n  \\log_2 \\left( \\frac{\\beta_2}{\\beta_1} \\right)\n\\end{align}\n\\qquad(10.1)\\]\nrestricting the set of terms to those assigned to both topics \\((\\min(\\beta_1, \\beta_2) &gt; 0)\\) with at least one of them exceeding a probability threshhold, say \\((\\max(\\beta_1, \\beta_2) &gt; 0.001)\\).\n\n\nShow the code\ng_af_bbb_b_ratio_wide &lt;- af_bbb_b_ratio_wide |&gt; \n  dplyr::group_by(direction = log_ratio &gt; 0) |&gt; \n  dplyr::slice_max(abs(log_ratio), n = 10) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::mutate(term = reorder(term, log_ratio)) |&gt; \n  ggplot2::ggplot(aes(log_ratio, term)) +\n  ggplot2::geom_col() +\n  ggplot2::labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\ng_af_bbb_b_ratio_wide\n\n\n\n\n\n\n\n\nFigure 10.2: Log2 ratio of beta in topic 2 / topic 1\n\n\n\n\n\nThis figure further supports the earlier conjecture that topic 1 best matches “The Butter Battle Book” and topic 2 best matches “Animal Farm”. But the match is not perfect, since “farm” is a key term distinguishing topic 1 from topic 2.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Latent Dirichlet Allocation</span>"
    ]
  },
  {
    "objectID": "latent-dirichlet-alloc.html#mathematical-framework",
    "href": "latent-dirichlet-alloc.html#mathematical-framework",
    "title": "10  Latent Dirichlet Allocation",
    "section": "10.3 Mathematical Framework",
    "text": "10.3 Mathematical Framework\n\n10.3.1 Notation\nThe data set to be analyzed is a corpus (set) \\(\\mathcal{D}\\) of documents. Each document \\(d \\in \\mathcal{D}\\) is a vector (sequence) of words, that may include multiple instances of the same term.5\n\\[\n\\begin{align}\n  d &= (w_1, \\ldots, w_{|d|})\n\\end{align}\n\\qquad(10.2)\\]\nThe vocabulary \\(V\\) of the corpus is the set of distinct terms that occur in one or more of the documents.\n\\[\n\\begin{align}\n  V &= \\cup_{d \\in \\mathcal{D}} \\;  \\{ v \\in d \\} \\\\\n  &= \\left \\{ v_\\nu \\right \\}_{\\nu = 1}^N\n\\end{align}\n\\qquad(10.3)\\]\nwhere the elements of \\(V\\) may be indexed, typically in alphabetical order. These elements \\(v_\\nu\\) are referred to as terms. We will use word to mean an occurrence of a term within the corpus. With this distinction, the total number of words (term occurrences) in the corpus exceeds or equals the number of distinct terms.6 7\n\\[\n\\begin{align}\n  \\left | \\mathcal{D} \\right | &= \\sum_{d \\in \\mathcal{D}} \\; |d| \\\\\n  &\\ge | V |\n\\end{align}\n\\qquad(10.4)\\]\nA topic \\(\\theta\\) is a probability distribution over vocabulary \\(V\\). Since \\(V\\) consists of a finite number \\(N = \\left | V \\right |\\) of distinct terms, \\(\\theta\\) can be represented as probability vector of length \\(N\\).\nThe number \\(K\\) of topics (categories) is prescribed for each analysis. Given \\(K\\) and \\(\\mathcal{D}\\), LDA produces a topic model defined by a set of constructed topics, \\(\\{ \\theta_1, \\ldots, \\theta_K \\}\\). These topic-specific probability vectors are stored in matrix \\(\\beta\\).\n\\[\n\\begin{align}\n  \\beta [k, \\nu] &= \\theta_k[\\nu] \\\\\n  &= \\text{probability assigned by topic } \\theta_k \\text{ to term } v_\\nu \\in V\n\\end{align}\n\\qquad(10.5)\\]\n\n\n10.3.2 Generative Model\nLDA treats each document as a mixture of topics, and each topic as a mixture of terms. This is a bag-of-words model in which each document \\(d\\) in a corpus of documents \\(\\mathcal{D}\\) is randomly generated as follows.8\n\nGenerate \\(K\\) topics, \\(\\{ \\theta_1, \\ldots, \\theta_K \\}\\), from given Dirichlet distribution \\(Dir(\\delta_\\bullet)\\), where topic \\(\\theta_k\\) is a probability vector over vocabulary \\(V = \\{ v_\\nu \\}_{\\nu = 1}^N\\).\nFor each document \\(d\\) generate probability vector \\(\\varphi_d\\) from given Dirichlet distribution \\(Dir(\\alpha_\\bullet)\\), where \\(\\varphi_d\\) is a probability distribution over topics \\(\\{ \\theta_1, \\ldots, \\theta_K \\}\\).\nFor each word-position9 \\(j\\) in document \\(d\\): use probability vector \\(\\varphi_d\\) to select at random a single topic \\(\\theta_k\\); then use \\(\\theta_k\\) to generate a term \\(v_\\nu\\), where \\(z[d, j] = \\nu\\) denotes the index of the selected term.\n\nThese Dirichlet distributions are typically assumed to be symmetric. That is, if we define\n\\[\n\\begin{align}\n  \\bar{\\alpha} &= \\frac{\\alpha_+}{K} = \\frac{1}{K} \\sum_{k = 1}^K\n\\alpha_k \\\\\n  \\bar{\\delta} &= \\frac{\\delta_+}{N} = \\frac{1}{N} \\sum_{\\nu = 1}^N\n\\delta_\\nu\n\\end{align}\n\\qquad(10.6)\\]\nthen current practice is to set all elements of vectors \\(\\alpha_\\bullet\\) and \\(\\delta_\\bullet\\) equal to their respective average values.\n\\[\n\\begin{align}\n  \\alpha_k &= \\bar{\\alpha} \\quad \\text{for } k \\in \\{1, \\ldots, K \\} \\\\\n  \\delta_\\nu &= \\bar{\\delta} \\quad \\text{for } \\nu \\in \\{1, \\ldots, N \\}\n\\end{align}\n\\qquad(10.7)\\]\nThe values \\(\\bar{\\alpha}\\) and \\(\\bar{\\delta}\\) are called concentration parameters.\nFor a symmetric Dirichlet distribution as above, if \\(\\bar{\\alpha} &lt; 1\\) then for each document \\(d\\), \\(Dir(\\alpha_\\bullet)\\) tends to generate a sparse probability vector \\(\\varphi_d\\) that gives most of its weight to just a few of the K topics (with the selection of topics varying across documents). On the other hand, if \\(\\bar{\\alpha} &gt; 1\\) then \\(Dir(\\alpha_\\bullet)\\) tends to generate a more uniform probability vector \\(\\varphi_d\\) spread more evenly across all topics.\nAs for the distribution of terms within a topic, in most applications the number \\(N\\) of distinct terms in the corpus \\(\\mathcal{D}\\) may be a few thousand or even a few tens of thousands. Then each topic is a mixture across a distinct subset of a large number of terms, typically with \\(\\bar{\\delta} &lt; 1\\).\nSee Antoniak (2023) for practical suggestions on these and other settings and workflows.\n\n\n10.3.3 LDA-Gibbs Algorithm\nThe LDA identification of topics is a particular form of Markov chain Monte Carlo (MCMC) method. The data can be summarized as the set of unique terms across the corpus of documents, along with counts of those unique terms per document. The objective is to determine the topic of each term within each document.\nAssignment of topics to terms is intially made at random in a manner similar to the generative model outlined above. We thereby have initial values for:\n\\[\n\\begin{align}\n  n_{d, k} &= \\text{number of assignments of topic } k \\text{ to document } d \\\\\n  n_{\\nu, k} &= \\text{number of assignments of topic } k \\text{ to term } v_\\nu \\\\\n  n_k &= \\text{number of assignments of topic } k \\text{ across corpus } \\mathcal{D}\n\\end{align}\n\\qquad(10.8)\\]\nThe Gibbs sampling procedure treats in turn each word position in the corpus, and estimates the probability of assigning the term in that position to each topic, conditioned on the current topic assignments of all other terms. From this conditional distribution, a topic is generated and stored as the new topic assignment for the given term in the given position.\nThrough many iterations the counts \\(n_{d, k}, n_{\\nu, k}, n_k\\) are updated, thereby updating the constructed topics \\(\\{ \\theta_k \\}\\) and document-specific topic distributions \\(\\{ \\varphi_d \\}\\).\nA great computational savings is achieved by the choice of a Dirichlet prior for the probability vectors \\(\\{ \\theta_k \\}\\) and \\(\\{ \\varphi_d \\}\\), and thus for the multinomial distributions of the counts \\(n_{d, k}, n_{\\nu, k}, n_k\\). This is because the Dirichlet distribution is conjugate to the multinomial distribution. For further details on this updating algorithm see Steyvers and Griffiths (2007).",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Latent Dirichlet Allocation</span>"
    ]
  },
  {
    "objectID": "latent-dirichlet-alloc.html#concluding-remarks",
    "href": "latent-dirichlet-alloc.html#concluding-remarks",
    "title": "10  Latent Dirichlet Allocation",
    "section": "10.4 Concluding Remarks",
    "text": "10.4 Concluding Remarks\nTopic modeling is an active area of research that contributes to the statistical analysis of large document collections. The LDA model of document-generation specifies prior Dirichlet distributions for topics, and for the mixture of topics within each document, respectively. This enables efficient Bayesian analysis of the latent structure of a corpus of documents.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Latent Dirichlet Allocation</span>"
    ]
  },
  {
    "objectID": "latent-dirichlet-alloc.html#references",
    "href": "latent-dirichlet-alloc.html#references",
    "title": "10  Latent Dirichlet Allocation",
    "section": "10.5 References",
    "text": "10.5 References\n\n\n\n\n“Animal Farm | Wikipedia.” 2025. Wikipedia, July. https://en.wikipedia.org/wiki/Animal_Farm.\n\n\nAntoniak, Maria. 2023. “Topic Modeling for the People.” Maria Antoniak. University of Colorado Boulder. https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. O’Reilly Media. https://www.tidytextmining.com/.\n\n\nSteyvers, Mark, and Tom Griffiths. 2007. Probabilistic Topic Models. https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf.\n\n\n“The Butter Battle Book | Wikipedia.” 2025. Wikipedia, July. https://en.wikipedia.org/wiki/The_Butter_Battle_Book.",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Latent Dirichlet Allocation</span>"
    ]
  },
  {
    "objectID": "latent-dirichlet-alloc.html#footnotes",
    "href": "latent-dirichlet-alloc.html#footnotes",
    "title": "10  Latent Dirichlet Allocation",
    "section": "",
    "text": "See Steyvers and Griffiths (2007).↩︎\nSee “Animal Farm | Wikipedia” (2025).↩︎\nSee “The Butter Battle Book | Wikipedia” (2025).↩︎\nSee (Grün_Hornik_2011?).↩︎\n\\(| S |\\) denotes the number of elements in a finite set \\(S\\) (or more generally the cardinality of set \\(S\\)). We extend this notation to a sequence. If \\(d = (w_1, \\ldots, w_n)\\) is a finite sequence of words, we treat \\(d\\) as a vector of length \\(n = |d|\\).↩︎\nIn natural language processing (NLP), text is broken into tokens, a broad term that must be defined for each analysis. In the present context the original text is stripped of punctuation and of commonly occurring words (“stop-words”) so that the remaining words are the tokens analyzed.↩︎\nFor clarity we use term to mean a distinct element of a vocabulary, and we use word to mean an occurrence of a term in the corpus. There does not seem to be an established jargon for this distinction.↩︎\nOther authors use the verb “sample” rather than “generate” to mean random generation from a specified probability distribution.↩︎\nWe assume that document \\(d\\) is a sequence of \\(n_d = |d|\\) words, where \\(n_d\\) is either a given positive integer, or else is generated independently of other random variables.↩︎",
    "crumbs": [
      "Text Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Latent Dirichlet Allocation</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html",
    "href": "em-algorithm.html",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "",
    "text": "15.1 Background\nLatent Dirichlet Allocation (LDA) was proposed as a method of topic modeling in 2003 in a paper by Blei, Ng, and Jordan. The method is briefly mentioned in Part 1 of the course. Several course participants requested a more detailed description. This note prepares for the requested response by introducing the EM Algorithm.1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html#example-old-faithful-eruptions",
    "href": "em-algorithm.html#example-old-faithful-eruptions",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "15.2 Example: Old Faithful eruptions",
    "text": "15.2 Example: Old Faithful eruptions\n\n15.2.1 Delay and Duration\nThe figure below represents the duration of eruptions (“duration”) and the interval between eruptions (“delay”)2 of the “Old Faithful” geyser in Yellowstone National Park, Wyoming from August 1 to August 15, 1985.3\n\n\n\n\n\n\n\n\nFigure 15.1: Old Faithful eruptions: delay and duration (minutes)\n\n\n\n\n\n\n\n15.2.2 Gausian Mixture Model\nThe figure shows two clusters of data points.4 Here is probability model in which the delay and duration of each eruption follows one of two possible bivariate normal distributions depending on the group, \\(G\\), to which it belongs.\n\\[\n\\begin{align}\n  \\{ \\; (\\text{delay}, \\text{duration}) \\; | \\; \\text{group } G \\; \\} &\\sim \\mathcal{N}(\\mu_{\\bullet}^{(G)}, \\Sigma^{(G)}) \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  G &=\n  \\begin{cases}\n    1 & \\text{with probability } p_1 \\\\\n    2 & \\text{with probability } p_2 = 1 - p_1\n  \\end{cases}\n\\end{align}\n\\qquad(15.1)\\]\nAveraging across the two groups, a randomly selected eruption has the following mixture of bivariate normal distributions.\n\\[\n\\begin{align}\n  (\\text{delay}, \\text{duration}) &\\sim p_1 \\; \\mathcal{N}(\\mu_{\\bullet}^{(1)}, \\Sigma^{(1)}) +\n  p_2 \\; \\mathcal{N}(\\mu_{\\bullet}^{(2)}, \\Sigma^{(2)}) \\\\\n  \\\\\n  & \\text{where } p_1 + p_2 = 1\\\\\n\\end{align}\n\\qquad(15.2)\\]\nFitting a single multivariate normal distribution is straightforward5, but less so for the above mixture of bivariate normal distributions. The Expectation-Maximization (EM) algorithm provides an approach for doing so, as follows.\n\n\n15.2.3 Latent Variable: Cluster Membership\nIn the EM framework, we iteratively assign group-membership, \\(G = g \\in \\{ 1, 2 \\}\\), to each data point. Based on this set of assignments we update the parameter estimates (the “E” step of the EM algorithm). The new set of parameter estimates yields a new estimate of the likelihood function. We then re-assign cluster-membership to maximize the updated likelihood estimate (the “M” step). The iteration terminates once the magnitude of changes falls below a prescribed threshold.\nIn R this procedure is implemented by the function mixtools::mvnormalmixEM().\n\n\n15.2.4 Initial Estimates\nBased on the preceding figure we adopt the following initial estimates of the parameters.\n\nCalculate the respective medians of the (delay, duration) variables.\nFor each data point determine whether the delay value is less than the median of all observed delay values. Similarly determine whether the duration value is less than the median of all observed duration values.\nBased on these inequalities, categorize each (delay, duration) data point as belonging to one of four groups: (lower, lower), (lower, upper), (upper, lower), (upper, upper).\nRestrict attention to the (lower, lower), and (upper, upper) groups. Within this restricted set of data points calculate the sample averages and covariance matrices for each of the two groups. Use these as initial estimates of \\((\\mu_{\\bullet}^{(1)}, \\mu_{\\bullet}^{(2)})\\) and \\((\\Sigma^{(1)}, \\Sigma^{(2)})\\).\n\nThe table below summarizes the groups delineated by the respective medians of (delay, duration).\n\n\n\n\nTable 15.1: Summary statistics for initial groups of data points\n\n\n\n\nSummary statistics for initial groups of data points\n\n\n\n\n\n\n\n\n\n\n\n\ndelay_low\nduration_low\ncount\ndelay_mean\nduration_mean\ndelay_sd\nduration_sd\ndd_cor\n\n\n\n\nFALSE\nTRUE\n22\n81.1\n3.7\n3.5\n0.2\n-0.2\n\n\nTRUE\nTRUE\n112\n56.6\n2.2\n7.7\n0.6\n0.7\n\n\nFALSE\nFALSE\n116\n82.4\n4.5\n4.6\n0.3\n0.1\n\n\nTRUE\nFALSE\n22\n73.0\n4.4\n1.9\n0.3\n0.6\n\n\n\n\n\n\n\n\nRestricting attention to the groups in which (delay, duration) jointly fall either below or above their respective medians, we extract the initial parameter estimates from the table above. Variable g is the labeling of each point in the restricted data set to cluster 1 or cluster 2.\n\n\n\n\nTable 15.2: Initial parameter estimates\n\n\n\n\nInitial parameter estimates\n\n\ng\ncount\ndelay_mean\nduration_mean\ndelay_sd\nduration_sd\ndd_cor\n\n\n\n\n1\n112\n56.6\n2.2\n7.7\n0.6\n0.7\n\n\n2\n116\n82.4\n4.5\n4.6\n0.3\n0.1\n\n\n\n\n\n\n\n\nThe figure below summarizes results so far. We’ve restricted attention to data points (delay, duration) such that the two variables are either both below their respective medians or else both above their respective medians. We’ve defined variable g as the grouping variable taking integer values (1, 2) in these two respective cases. For each of the two groups we’ve constructed an ellipse conforming to the sample mean vector and covariance matrix of the group. The ellipse is designed to capture 50% of the area under a bivariate normal distribution having these parameter values.\n\n\n\n\n\n\n\n\nFigure 15.2: Old Faithful eruptions: initial normal estimates\n\n\n\n\n\nThis completes the initial “E” step in the EM algorithm, the estimation of parameter values.\n\n\n15.2.5 Log Likelihood\nWe now begin the initial “M” step (maximization) of the EM algorithm. Using the above parameter estimates we now assign cluster membership (variable g) to the points not yet labeled by maximizing the likelihood function, or equivalently (and more conveniently), the logarithm of the likelihood function.\nIn general, recall that a multivariate normal distribution having mean vector \\(\\mu_{\\bullet} \\in \\mathbb{R}^m\\) and covariance matrix \\(\\Sigma\\) has likelihood function equal to the product of the multivariate normal density evaluated at each observation vector, say \\(x_{\\bullet}^{(\\nu)}\\), (that is, at each row of the design matrix \\(X\\)).\n\\[\n\\begin{align}\n  \\mathcal{L}(X, \\mu_{\\bullet}, \\Sigma) &= \\prod_{\\nu = 1}^n \\mathcal{N}(x_{\\bullet}^{(\\nu)} - \\mu_{\\bullet}, \\;  \\Sigma)\n\\end{align}\n\\]\nTaking the natural logarithm of both sides of this equation, we obtain the log-likelihood function.\n\\[\n\\begin{align}\n  \\mathcal{l}(X, \\mu_{\\bullet}, \\Sigma) &= \\sum_{\\nu = 1}^n \\log_e(\\mathcal{N}(x_{\\bullet}^{(\\nu)} - \\mu_{\\bullet}, \\;  \\Sigma)) \\\\\n  &= -\\frac{m}{2} \\log_e(2 \\pi) - \\frac{1}{2}\\log_e(\\det{\\Sigma}) -\n  \\frac{1}{2} \\sum_{\\nu = 1}^n (x_{\\bullet}^{(\\nu)} - \\mu_{\\bullet})^{\\intercal} \\;  \\Sigma^{-1} (x_{\\bullet}^{(\\nu)} - \\mu_{\\bullet}) \\\\\n\\end{align}\n\\]\nConsequently, fitting parameters \\(\\mu_{\\bullet}\\) and \\(\\Sigma\\) to the design matrix \\(X\\) via maximum likelihood amounts to minimizing the sum of quadratic forms on the right side of this equation. (Each term in the sum is referred to as the squared Mahalanobis distance between the observation vector and the mean vector.)\n\\[\n\\begin{align}\n  (\\hat{\\mu}_{\\bullet}, \\hat{\\Sigma}) & = \\arg \\min \\sum_{\\nu = 1}^n (x_{\\bullet}^{(\\nu)} - \\mu_{\\bullet})^{\\intercal} \\;  \\Sigma^{-1} (x_{\\bullet}^{(\\nu)} - \\mu_{\\bullet}) \\\\\n\\end{align}\n\\]\nwhich yields the sample mean and a variant of the sample covariance matrix.\n\\[\n\\begin{align}\n  \\hat{\\mu}_{\\bullet} & = \\frac{1}{n} \\sum_{\\nu = 1}^n x_{\\bullet}^{(\\nu)} \\\\\n  \\\\\n  \\hat{\\Sigma} & = \\frac{1}{n} \\sum_{\\nu = 1}^n (x_{\\bullet}^{(\\nu)} - \\hat{\\mu}_{\\bullet}) (x_{\\bullet}^{(\\nu)} - \\hat{\\mu}_{\\bullet})^{\\intercal} \\\\\n\\end{align}\n\\]\nFor the Old Faithful data, however, we propose a mixture of two distnct normal distributions, that is, two clusters in which the cluster to which each data point belongs is unknown.\nHaving estimated the normal parameters for clusters 1 and 2, we now label each data point as belonging to the cluster to which it is closest, in the sense of Mahalanobis distance. The results are shown in the next figure.\n\n\n\n\n\n\n\n\nFigure 15.3: Old Faithful: inital clustering of all points\n\n\n\n\n\nThis completes the initial maximization step. The table below shows the revised summary statistics per cluster now that the initial assignment to clusters has been completed.\n\n\n\n\nTable 15.3: Summary statistics for initial clusters of data points\n\n\n\n\nSummary statistics for initial clusters of data points\n\n\ng\ncount\ndelay_mean\nduration_mean\ndelay_sd\nduration_sd\ndd_cor\n\n\n\n\n1\n115\n57.1\n2.3\n8.4\n0.6\n0.7\n\n\n2\n157\n81.0\n4.4\n5.3\n0.3\n0.1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html#em-iterations",
    "href": "em-algorithm.html#em-iterations",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "15.3 EM Iterations",
    "text": "15.3 EM Iterations\nWe illustrate the iterations of the EM algorithm using function mvnormalmixEM() in R package mixtools.6 We provide the function with our initial estimates of normal parameters.\n\n\nShow the code\nem_out &lt;- faithful_tbl |&gt; \n  mixtools::mvnormalmixEM(\n    lambda = c(0.5, 0.5), \n    mu = list(\n      # delay, duration\n      c(56.6, 2.25), # (lower, lower)\n      c(82.4, 4.47)  # (upper, upper)\n    ), \n    sigma = list(\n      # (lower, lower)\n      matrix(\n        nrow = 2, ncol = 2, \n        data = c(\n          59.7,  3.27, \n          3.27,  0.35\n        )), \n      # (upper, upper)\n      matrix(\n        nrow = 2, ncol = 2, \n        data = c(\n          21.5,  0.19, \n          0.19,  0.08\n        ))\n    ), \n    verb = TRUE\n  )\n\n\niteration= 1 diff= 41.60226 log-likelihood -1168.518 \niteration= 2 diff= 14.54077 log-likelihood -1153.978 \niteration= 3 diff= 12.21481 log-likelihood -1141.763 \niteration= 4 diff= 9.461094 log-likelihood -1132.302 \niteration= 5 diff= 1.962679 log-likelihood -1130.339 \niteration= 6 diff= 0.0717014 log-likelihood -1130.267 \niteration= 7 diff= 0.003089365 log-likelihood -1130.264 \niteration= 8 diff= 0.000168951 log-likelihood -1130.264 \niteration= 9 diff= 9.661982e-06 log-likelihood -1130.264 \niteration= 10 diff= 5.582749e-07 log-likelihood -1130.264 \niteration= 11 diff= 3.233686e-08 log-likelihood -1130.264 \niteration= 12 diff= 1.874241e-09 log-likelihood -1130.264 \nnumber of iterations= 12 \n\n\nReports from each iteration were requested by setting function parameter verb = TRUE (verbose). The estimated log-likelihood is shown for each iteration, along with the change in that value from the previous iteration.\nThe final estimates of the normal parameters are shown in the table below, and illustrated in the figure below.\n\n\n\n\nTable 15.4: Summary statistics for final groups of data points\n\n\n\n\nSummary statistics for final groups of data points\n\n\ng\ncount\ndelay_mean\nduration_mean\ndelay_sd\nduration_sd\ndd_cor\n\n\n\n\n1\n97\n54.5\n2.0\n5.8\n0.3\n0.3\n\n\n2\n175\n80.0\n4.3\n6.0\n0.4\n0.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Old Faithful eruptions: final normal estimates",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html#group-membership-over-time",
    "href": "em-algorithm.html#group-membership-over-time",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "15.4 Group Membership over Time",
    "text": "15.4 Group Membership over Time\nAccording to the EM algorithm, here is the pattern of group membership as a function of time. More precisely, here is the posterior probability of group-1 membership as a function of cumulative delay expressed in days.\n\n\n\n\n\n\n\n\nFigure 15.5: Group 1 posterior probability over cumulative delay (days)\n\n\n\n\n\nRecall that group 1 refers to the eruptions having shorter delays and shorter durations. The figure shows that these are interupted by short stretches of eruptions from group 2. The figure also show that posterior probabilities come out to be nearly zero or else nearly unity, with rare exceptions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html#closing-remarks",
    "href": "em-algorithm.html#closing-remarks",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "15.5 Closing Remarks",
    "text": "15.5 Closing Remarks\nSeveral papers present the mathematics underlying the EM algorithm, and one can also find tutorials that illustrate the convergence of the algorithm.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html#resources",
    "href": "em-algorithm.html#resources",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "15.6 Resources",
    "text": "15.6 Resources\nIn addition to the references listed below, see the following.\nThe EM Algorithm Explained blog by Chloe Bi, 2019",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html#references",
    "href": "em-algorithm.html#references",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "15.7 References",
    "text": "15.7 References\n\n\n\n\nAnderson, T. W., and Ingram Olkin. 2002. “Maximum-Likelihood Estimation of the Parameters of a Multivariate Normal Distribution.” Linear Algebra and Its Applications, May. https://www.sciencedirect.com/science/article/pii/0024379585900497.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "em-algorithm.html#footnotes",
    "href": "em-algorithm.html#footnotes",
    "title": "15  EM: the Expectation-Maximization Algorithm",
    "section": "",
    "text": "See (rdpeng2022advstatcomp?, chp. 4), and (EM_algo_Wikipedia_2025?).↩︎\n“Delay” here refers to the length of time preceding the current eruption. The (delay, duration) variables are recorded in minutes.↩︎\nThere are two prominent R packages representing the Old Faithful measurements during August, 1985. The MASS::geyser data set of 299 observations includes nocturnal measurements whose duration was coded as 2, 3 or 4 minutes, having originally been described as ‘short’, ‘medium’ or ‘long’. The datasets::faithful data set of 272 measurements, shown in the figure, excludes 27 of these nocturnal measurements.↩︎\nOld Faithful eruptions are conjectured to occur in 2 distinct temporal patterns due to the presence of an upper and lower chamber beneath the vertical column (tube) that forms the geyser. See (Vandemeulebrouck_Roux_Cros_2013?).↩︎\nSee Anderson and Olkin (2002).↩︎\nSee (mixtools2009?).↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EM: the Expectation-Maximization Algorithm</span>"
    ]
  },
  {
    "objectID": "la-intro.html",
    "href": "la-intro.html",
    "title": "6  Linear Regression",
    "section": "",
    "text": "6.1 Data Examples\nVectors and matrices are the central objects of linear algebra. And central to data science and machine learning is the notion of a data matrix, in which each row is composed of different types of values that represent a single case of data. For example, a single row of a data matrix might represent the recorded characteristics of an individual participant, item, or unit in some study. In contrast, each column (known as a feature vector or data variable) represents multiple instances of just one of these prescribed types of value. 1\nThis chapter presents linear regression from the perspective of linear algebra. A response variable \\(y\\) is modeled as some linear combination of feature vectors plus a residual error term. We begin with some examples.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "la-intro.html#data-sets",
    "href": "la-intro.html#data-sets",
    "title": "6  Some Linear Algebra",
    "section": "",
    "text": "6.1.1 Heights of Parents, Oldest Child\nIn 1885 Sir Francis Galton examined the heights (in inches) of parents and their children to determine the strength of evidence to support height as a hereditary trait. The corresponding data set HistData::GaltonFamilies consists of 934 adult children from a total of 205 families. Restricting attention to the oldest child in each family, there were 26 daughters and 179 sons, shown in the figures below.\n\n\n\n\n\n\n\n\nFigure 6.1: Heights of (daughter, mother, father)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: Heights of (son, mother, father)\n\n\n\n\n\nThe figures indicate that in aggregate the average height of daughters is about 2 inches greater than the average height of mothers. Similarly sons as a group average about 1 inch taller than fathers as a group. Parent-child correlations are stronger within the same sex (mother-daughter and father-son).",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Some Linear Algebra</span>"
    ]
  },
  {
    "objectID": "la-intro.html#example-data-sets",
    "href": "la-intro.html#example-data-sets",
    "title": "6  Linear Algebra for Fitting Models to Data",
    "section": "",
    "text": "6.1.1 Heights of Parents and Oldest Child\nIn 1885 Sir Francis Galton examined the heights (in inches) of parents and their adult children to determine the strength of evidence to support height as a hereditary trait. The corresponding R data set HistData::GaltonFamilies consists of 934 adult children from a total of 205 families. Restricting attention to the oldest child in each family, there were 26 daughters and 179 sons.\nThe table below shows a portion of this data matrix. Each row represents a family and consists of: a family identifier, the father’s height, the mother’s height, the oldest child’s height, and the oldest child’s gender.\n\n\n\n\nTable 6.1: Galton’s heights in inches of (father, mother, oldest child)\n\n\n\n\nHeights of (father, mother, oldest child)\n\n\nfamily\nfather\nmother\nchild\ngender\n\n\n\n\n001\n78.5\n67.0\n73.2\nmale\n\n\n002\n75.5\n66.5\n73.5\nmale\n\n\n003\n75.0\n64.0\n71.0\nmale\n\n\n004\n75.0\n64.0\n70.5\nmale\n\n\n005\n75.0\n58.5\n72.0\nmale\n\n\n006\n74.0\n68.0\n69.5\nfemale\n\n\n\n\n\n\n\n\nThe figure below represents all the families, with the gender of the oldest child distinguished by color: red for daughters and blue for sons.\n\n\n\n\n\n\n\n\nFigure 6.1: (mother, father, child) heights, with daughters shown in red\n\n\n\n\n\nIn Chapter 2 we regressed the son’s height on the father’s height. We obtained the regression line, which approximates the graph of averages: the average son’s height per father’s height. The linear regression can be interpreted as a linear prediction of the height of a son whose father is of some given height.\nWe can now expand on this idea by regressing the son’s height on the heights of both the mother and the father. This is a model in which the predicted son’s height, \\(\\hat{s}\\), is some constant plus some linear combination of the parents’ heights.\n\\[\n\\begin{align}   \n  \\hat{s} & = \\mathcal{l}_{R}(m, f) \\\\   \n  &= \\beta_0 \\; + \\; \\beta_m \\times m \\; + \\; \\beta_f \\times f\n\\end{align}\n\\qquad(6.1)\\]\nwhere\n\\[\n\\begin{align}\n  \\hat{s} &= \\text{predicted height of son} \\\\\n  m &= \\text{height of mother}  \\\\\n  f &= \\text{height of father}\n\\end{align}\n\\qquad(6.2)\\]\nThis equation represents a plane in 3-dimensional space. Using least-squares regression to estimate the coefficients \\((\\beta_0, \\beta_m, \\beta_f)\\), we obtain a plane (Figure 6.2) that gives the best linear approximation \\((\\hat{s})\\) to the son’s height for a given pair of parent heights \\((m, f)\\). 1\n\n\n\n\n\n\n\n\nFigure 6.2: Observed (point) and predicted (plane) height of son given (mother, father) heights\n\n\n\n\n\nIn vector-matrix notation we are seeking a vector \\((\\hat{\\beta}_0, \\hat{\\beta}_m, \\hat{\\beta}_f)\\) of coefficient values that yields the least-squares solution to the following linear approximation problem.\n\\[\n\\begin{align}\n  s_\\bullet &\\approx (1_\\bullet, m_\\bullet, f_\\bullet) \\times\n    \\begin{pmatrix}\n      \\hat{\\beta}_0 \\\\ \\hat{\\beta}_m \\\\ \\hat{\\beta}_f\n    \\end{pmatrix}\n\\end{align}\n\\qquad(6.3)\\]\nwhere\n\\[\n\\begin{align}\n  s_\\bullet &= \\text{data column vector: heights of sons} \\\\\n  1_\\bullet &= \\text{column vector } (1, \\ldots, 1) \\\\\n  m_\\bullet &= \\text{data column vector: heights of mothers}  \\\\\n  f_\\bullet &= \\text{data column vector: heights of fathers}\n\\end{align}\n\\qquad(6.4)\\]\nThis is a statistical estimation problem that corresponds to the following linear algebra problem and notation.\n\\[\n\\begin{align}\n  b_\\bullet &\\approx A_{\\bullet, \\bullet} \\times x_\\bullet\n\\end{align}\n\\qquad(6.5)\\]\nwhere\n\\[\n\\begin{align}\n  b_\\bullet &= s_\\bullet \\\\\n  A_{\\bullet, \\bullet} &= (1_\\bullet, m_\\bullet, f_\\bullet) \\\\\n  x_\\bullet &= (\\hat{\\beta}_0, \\hat{\\beta}_m, \\hat{\\beta}_f)\n\\end{align}\n\\qquad(6.6)\\]\nIt turns out that the least squares solution \\((\\hat{\\beta}_0, \\hat{\\beta}_m, \\hat{\\beta}_f)\\) can be obtained as the vector of coefficients of an orthogonal projection of vector \\(s_\\bullet\\) onto the 3-dimensional subspace spanned by vectors \\((1_\\bullet, m_\\bullet, f_\\bullet)\\). More on this later.\n\n\n6.1.2 Survey Data: Better Life Index\nWe now turn to a data set having several data columns, namely the OECD’s Better Life Index (BLI). 2 The following table shows a portion of the data.\n\n\n\nTable 6.2: Better Life Index (BLI)\n\n\n\n# A tibble: 42 × 26\n  code  country     CG_SENG CG_VOTO EQ_AIRP EQ_WATER ES_EDUA ES_EDUEX ES_STCS\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 AUS   Australia       2.7      92     6.7       92      84       20     499\n2 AUT   Austria         1.3      76    12.2       92      86       17     491\n3 BEL   Belgium         2        88    12.8       79      80       19     500\n4 BRA   Brazil          2.2      80    11.7       70      57       16     400\n5 CAN   Canada          2.9      68     7.1       90      92       17     517\n6 CHE   Switzerland     2.3      45    10.1       96      89       17     498\n# ℹ 36 more rows\n# ℹ 17 more variables: HO_HISH &lt;dbl&gt;, HS_LEB &lt;dbl&gt;, HS_SFRH &lt;dbl&gt;,\n#   IW_HADI &lt;dbl&gt;, IW_HNFW &lt;dbl&gt;, JE_EMPL &lt;dbl&gt;, JE_LMIS &lt;dbl&gt;, JE_LTUR &lt;dbl&gt;,\n#   JE_PEARN &lt;dbl&gt;, PS_FSAFEN &lt;dbl&gt;, PS_REPH &lt;dbl&gt;, SC_SNTWS &lt;dbl&gt;,\n#   SW_LIFS &lt;dbl&gt;, WL_EWLH &lt;dbl&gt;, WL_TNOW &lt;dbl&gt;, HO_BASE &lt;dbl&gt;, HO_NUMR &lt;dbl&gt;\n\n\n\n\nEach row of this data matrix gives specified measurements of an identified country. The first two columns give, respectively, each country’s OECD code and name. The remaining 24 columns are measures pertaining to the well-being of the populace.\nThe column name of each measures consists of a two-letter prefix followed by a suffix. The prefix is associated with a broad indicator of social well-being. The suffix pertains to a particular component of this indicator. Here is an expansion of these prefixes.\n\n\n\n\nTable 6.3: BLI Indicators and Sub-Components\n\n\n\n\nBLI Indicators and Sub-Components\n\n\n\n\n\n\n\n\nprefix\nname\nn_comps\ncomponents\n\n\n\n\nCG\nCivic Engagement\n2\nCG_SENG, CG_VOTO\n\n\nEQ\nEnvironmental Quality\n2\nEQ_AIRP, EQ_WATER\n\n\nES\nEducation System\n3\nES_EDUA, ES_EDUEX, ES_STCS\n\n\nHO\nHousing\n3\nHO_BASE, HO_HISH, HO_NUMR\n\n\nHS\nHealth Status\n2\nHS_LEB, HS_SFRH\n\n\nIW\nIncome and Wealth\n2\nIW_HADI, IW_HNFW\n\n\nJE\nJobs Employment\n4\nJE_EMPL, JE_LMIS, JE_LTUR, JE_PEARN\n\n\nPS\nPersonal Safety\n2\nPS_FSAFEN, PS_REPH\n\n\nSC\nSocial Connections\n1\nSC_SNTWS\n\n\nSW\nSubjective Well-Being\n1\nSW_LIFS\n\n\nWL\nWork Life Balance\n2\nWL_EWLH, WL_TNOW\n\n\n\n\n\n\n\n\nThe component indicators (corresponding to the suffix of the column name) are elaborated in the following table.\n\n\n\n\nTable 6.4: BLI Component Indicators\n\n\n\n\nBLI Component Indicators\n\n\n\n\n\n\n\n\n\nprefix\nsuffix\nunit\nname\ndescription\n\n\n\n\nCG\nSENG\nAVSCORE\nStakeholder Engagement\nExtent to which people can engage with government in rule-making\n\n\nCG\nVOTO\nPC\nVoter Turnout\nPercent of registered voters who voted in recent elections\n\n\nEQ\nAIRP\nMICRO_M3\nAir Pollution\nConcentration of PM2.5 particulate matter (micrograms per cubic meter)\n\n\nEQ\nWATER\nPC\nWater Quality\nPercent satisfied with water quality\n\n\nES\nEDUA\nPC\nEducational Attainment\nPercent aged 25-64 with at least upper-secondary education\n\n\nES\nEDUEX\nYR\nExpected Years of Education\nExpected years of schooling\n\n\nES\nSTCS\nAVSCORE\nStudent Cognitive Skills\nPISA scores in reading, mathematics, and science\n\n\nHO\nBASE\nPC\nDwellings w/o Basic Facilities\nPercentage of dwellings that lack basic sanitary facilities\n\n\nHO\nHISH\nPC\nHousing Expenditure\nPercentage of household gross adjusted disposable income spent on housing\n\n\nHO\nNUMR\nRATIO\nRooms per Person\nNumber of rooms per person in dwelling\n\n\nHS\nLEB\nYR\nLife Expectancy at Birth\nAverage number of years a person can expect to live\n\n\nHS\nSFRH\nPC\nSelf-Reported Health\nPercentage who report being in good or very good health\n\n\nIW\nHADI\nUSD\nHousehold Adjusted Disposable Income\nAverage household income after taxes\n\n\nIW\nHNFW\nUSD\nHousehold Net Financial Wealth\nHousehold net financial wealth (financial assets minus liabilities)\n\n\nJE\nEMPL\nPC\nEmployment Rate\nPercentage of people aged 15-64 in paid employment\n\n\nJE\nLMIS\nPC\nLabour Market Insecurity\nExpected loss of earnings if someone becomes unemployed\n\n\nJE\nLTUR\nPC\nLong-Term Unemployment Rate\nPercentage unemployed for 12+ months\n\n\nJE\nPEARN\nUSD\nPersonal Earnings\nAverage annual earnings per full-time employee\n\n\nPS\nFSAFEN\nPC\nFeeling Safe Walking Alone at Night\nPercentage who feel safe\n\n\nPS\nREPH\nRATIO\nHomicide Rate\nDeaths per 100,000 people\n\n\nSC\nSNTWS\nPC\nSupport Network Quality\nPercentage who believe they have someone to rely on in times of need\n\n\nSW\nLIFS\nAVSCORE\nLife Satisfaction\nAverage self-evaluation on a scale from 0 to 10\n\n\nWL\nEWLH\nPC\nEmployees Working Long Hours\nPercentage of employees working 50+ hours per week\n\n\nWL\nTNOW\nHOUR\nTime Devoted to Leisure and Personal Care\nHours per day spent on leisure, personal care, eating, and sleeping\n\n\n\n\n\n\n\n\nThe unit column in the above table gives the unit of measure, with PC meaning percent, YR meaning number of years, and so on.\nWe now turn to a statistical and algebraic treatment of the BLI data matrix of Table 6.2. Consider the indicator component SW_LIFS (Life Satisfaction) as a response variable, with the remaining 23 indicator components serving as explanatory variables. As with the previous data example, we want to approximate or predict the response variable by a constant \\(\\beta_0\\) plus a linear combination of the explantory variables, as follows.\n\\[\n\\begin{align}\n  L_\\bullet &\\approx (1_\\bullet, C_{1, \\bullet}, \\ldots, C_{d, \\bullet}) \\times\n    \\begin{pmatrix}\n      \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\vdots \\\\ \\hat{\\beta}_d\n    \\end{pmatrix}\n\\end{align}\n\\qquad(6.7)\\]\nwhere\n\\[\n\\begin{align}\n  L_\\bullet &= \\text{life satisfaction indicator per country} \\\\\n  1_\\bullet &= \\text{column vector } (1, \\ldots, 1) \\\\\n  C_{k, \\bullet} &= k^{th} \\text{ indicator component per country} \\\\\n  d &= \\text{number of explanatory indicators}\n\\end{align}\n\\qquad(6.8)\\]\nWe now have more explanatory variables than in the previous example, a fact that merits some comment.\nOn the one hand, the approach to determining least-squares regression coefficients \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_d\\) is unchanged. We project the response vector, now \\(L_\\bullet\\), onto the space spanned by the constant vector \\(1_\\bullet\\) along with the explanatory variables, that is onto the space spanned by \\((1_\\bullet, C_{1, \\bullet}, \\ldots, C_{d, \\bullet})\\). The fitted coefficients yield a function of the explanatory variables that forms a linear hyperplane of dimension 23 that passes through a cloud of data points, \\((C_{1, \\bullet}, \\ldots, C_{d, \\bullet}, L_\\bullet)\\), in a space of dimension 24.\nOn the other hand, we are now estimating 24 regression coefficients based on observations from just 42 countries. From a statistical perspective, this paucity of observations relative to the number of estimates leads to large standard errors for the set of estimated coefficients. From the perspective of numerical linear algebra, the vector of fitted coefficients \\((\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_d)\\) is less stable (more sensitive to error in the data) than it was in the previous example.\n\n\n6.1.3 MNIST: Images of Handwritten Digits\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten decimal digits consisting of 60,000 training images and 10,000 testing images. 3\nThe history of this database goes back to 1988, when the US Postal Service constructed images of digits appearing on handwritten zip codes. Around the same time the US Census Bureau requested NIST to evaluate optical character recognition (OCR) systems. In 1992, NIST and the Census Bureau sponsored a competition in which participating teams were given images of Handwriting Sample Forms (HSFs), including handwritten decimal digits. The initial version of MNIST was constructed sometime before summer 1994.\nHere’s an example of each handwritten digit from the training set of images.\n\n\n\n\n\n\n\n\nFigure 6.3: Example images of handwritten digits from the MNIST dataset\n\n\n\n\n\nEach image is represented by a \\(28 \\times 28\\) matrix of pixels, with each pixel represented as a grayscale integer value from 0 through 255. That is, each image represents a single vector in a space of dimension 784 (since \\(28 \\times 28 = 784\\)).\nThe 1992 competition prompted the development of algorithms to determine the decimal digit represented by any such image. This is a classification problem: to label each case of data (image) as belonging to one of several possible categories (decimal digits).\nOne such method, multinomial logistic regression, assigns a probability that a given image represents a specified digit, resulting in a 10-element probability vector per image. 4\n\n6.1.3.1 Multinomial Logistic Regression\nTo formulate the model, we convert the representation of an image from a \\(28 \\times 28\\) matrix of pixels into a vector of pixels of length 784. 5 We’ll denote such a vector as \\((P_1, \\ldots, P_d)\\), where \\(d = 784\\).\nLet \\(D\\) denote the digit represented by the image. The ordering of the digits from 0 through 9 is not directly relevant to the image-recognition problem, so let us regard \\(D\\) as a categorical variable having the set \\(\\{ 0, 1, \\ldots, 9 \\}\\) as possible values. An alternative representation is the set of indicator vectors \\(e_0 = (1, 0, \\ldots, 0)\\) through \\(e_9 = (0, 0, \\ldots, 1)\\), called “one-hot encoding” in machine learning. 6\nThen the multinomial logistic regression model can be formulated as follows.\n\\[\n\\begin{align}\n  \\log_e{ \\frac{P(D = \\nu)}{P(D = 0)} } &= (1, P_1, \\ldots, P_d) \\times\n    \\begin{pmatrix}\n      \\beta_0^{(\\nu)} \\\\ \\beta_1^{(\\nu)} \\\\ \\vdots \\\\ \\beta_d^{(\\nu)}\n    \\end{pmatrix} & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.9)\\]\nwith\n\\[\n\\begin{align}\n  P(D = 0) &= 1 - \\sum_{\\nu = 1}^9 P(D = \\nu)\n\\end{align}\n\\qquad(6.10)\\]\nFor a more compact notation let \\(X_{\\bullet} = (1, P_1, \\ldots, P_d)\\) and let \\(\\beta_{\\bullet}^{(\\nu)} = (\\beta_0^{(\\nu)}, \\beta_1^{(\\nu)}, \\ldots, \\beta_d^{(\\nu)})\\), with the inner product of these two vectors denoted as \\(X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}\\). Then we have\n\\[\n\\begin{align}\n  \\log_e{ \\frac{P(D = \\nu)}{P(D = 0)} } &= X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)} & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.11)\\]\nExponentiation of Equation 6.11 gives:\n\\[\n\\begin{align}\n  \\{ P(D = \\nu) \\} &= \\{ P(D = 0) \\} \\times e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}} & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.12)\\]\nTaking the sum over \\(\\nu\\) we have:\n\\[\n\\begin{align}\n  \\sum_{\\nu = 1}^9 {P(D = \\nu)} &= \\{ P(D = 0) \\} \\times \\sum_{\\nu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}}\n\\end{align}\n\\qquad(6.13)\\]\nNow applying Equation 6.10 we have\n\\[\n\\begin{align}\n  \\left \\{ 1 - P(D = 0) \\right \\} &= \\{ P(D = 0) \\} \\times \\sum_{\\nu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}}\n\\end{align}\n\\qquad(6.14)\\]\nwhich yields:\n\\[\n\\begin{align}\n  P(D = 0) &= \\frac{1} { 1 + \\sum_{\\nu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}} }\n\\end{align}\n\\qquad(6.15)\\]\nApplying Equation 6.12 gives:\n\\[\n\\begin{align}\n  P(D = \\nu) &= \\frac{ e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}} } { 1 +  \\sum_{\\mu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\mu)}} } & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.16)\\]\n\n\n6.1.3.2 Matrix Representation\nEquation 6.11 pertains to the probability that a single image represents a single digit \\(\\nu \\in \\{1, \\ldots, 9 \\}\\). Therefore, in a data set of \\(n\\) images, with \\(i\\) denoting the index of a particular image, we have:\n\\[\n\\begin{align}\n  \\log_e{ \\frac{P(D_i = \\nu)}{P(D_i = 0)} } &= X_{i, \\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}\n\\end{align}\n\\qquad(6.17)\\]\nExpanding the last equation to matrix notation, with \\(i\\) as the row index and \\(\\nu\\) as a column index, we have\n\\[\n\\begin{align}\n&\n\\begin{pmatrix}\n  \\log_e{ \\frac{P(D_1 = 1)}{P(D_1 = 0)} }, & \\ldots, & \\log_e{ \\frac{P(D_1 = 9)}{P(D_1 = 0)} } \\\\\n  \\vdots & \\vdots & \\vdots \\\\\n  \\log_e{ \\frac{P(D_n = 1)}{P(D_n = 0)} }, & \\ldots, & \\log_e{ \\frac{P(D_n = 9)}{P(D_n = 0)} }\n\\end{pmatrix}  \\\\ \\\\\n&=\n\\begin{pmatrix}\n  X_{1, \\bullet} \\\\\n  \\vdots \\\\\n  X_{n, \\bullet}\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\beta_{\\bullet}^{(1)}, & \\ldots, & \\beta_{\\bullet}^{(9)}\n\\end{pmatrix}\n\\end{align}\n\\qquad(6.18)\\]\nThe matrix on the left side of Equation 6.18 has dimensions \\(n \\times 9\\). On the right side, the first matrix factor has dimensions \\(n \\times 785\\), and the second matrix factor has dimensions \\(785 \\times 9\\).",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Algebra for Fitting Models to Data</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html",
    "href": "ts-forecast.html",
    "title": "12  Time Series Forecasting",
    "section": "",
    "text": "12.1 Introduction\nThis technical note introduces the basic components of widely used statistical models of time series data. Such models support the understanding of underlying processes. They are also heavily used to forecast future values, which is the emphasis of this note.\nThe examples and methods presented here are from (shumway2025?) and the R package astsa (astsa?).",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#mathematical-framework",
    "href": "ts-forecast.html#mathematical-framework",
    "title": "12  Time Series Forecasting",
    "section": "12.2 Mathematical Framework",
    "text": "12.2 Mathematical Framework\n\n12.2.1 Data derived from a random process\nStatistical applications are often based on one or more data frames in which each row represents an observation and each column represents a variable of interest. Consider for example the following predator-prey data (Table 12.1 and Figure 12.9), based on the record of snowshoe hare (astsa::Hare) and lynx (astsa::Lynx) pelts purchased by the Hudson’s Bay Company of Canada from 1845 to 1935.\n\n\n\nTable 12.1: Hare and lynx pelts (thousands)\n\n\n\n# A tibble: 91 × 3\n     yr  hare  lynx\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1845  19.6  30.1\n2  1846  19.6  45.2\n3  1847  19.6  49.2\n4  1848  12.0  39.5\n5  1849  28.0  21.2\n# ℹ 86 more rows\n\n\n\n\nThe distinction of time series analysis is that observations are indexed by time, and are not assumed to be statistically independent. Instead, time series data are typically modeled as a realization of a random process of the following form.1\n\\[\n\\begin{align}\n  X_\\bullet (t) = (X_1 (t), \\ldots, X_d (t)) \\in \\mathbb{R}^d\n\\end{align}\n\\qquad(12.1)\\]\nIf the number of data columns is two or greater \\((d \\ge 2)\\), the process is said to be multivariate. Otherwise, if \\(d = 1\\) the process is said to be univariate, and the notation is simplified to \\(X (t)\\).2\nIn our example there are two data columns (hare, lynx), so that \\(d = 2\\), the unit of time is one year, and the sampling frequency is once per year.\nThe random process is idealized to span all time \\((t \\in \\mathbb{Z})\\), but the data of course span some finite period, \\(\\tau_T\\), of length \\(T\\).\n\\[\n\\begin{align}\n  \\tau_T &=  \\{ t_0, \\; t_0 + 1, \\ldots, t_f \\} \\\\\n  t_0 &=  \\text{ initial data index} = \\min \\tau_T \\\\\n  t_f &=  \\text{ final data index} = \\max \\tau_T \\\\\n  T &=  \\text{ number of observatons } = t_f - t_0 + 1 \\\\\n  \\nu_T &= \\tau_T - t_0 =  \\{ 0, 1, \\ldots, T-1 \\}\n\\end{align}\n\\qquad(12.2)\\]\nThe expected value of the random process may be modeled by various functions of time: a constant, a linear trend, a seasonal component (periodic function), etc.\n\\[\n\\begin{align}\n  m_\\bullet (t) &= E \\{ X_\\bullet (t) \\}\n\\end{align}\n\\qquad(12.3)\\]\n\n\n12.2.2 Stationarity\nIn an additive model (which is most common), the residual random process, \\(X_\\bullet (t) - m_\\bullet (t)\\) , is assumed to be second-order stationary.3 That is, if we shift \\(X_\\bullet (\\cdot)\\) by any number of time units \\(s\\) to obtain a new process \\(Y_\\bullet (\\cdot)\\), we assume that the respective covariance structures of \\(X_\\bullet (\\cdot)\\) and \\(Y_\\bullet (\\cdot)\\) are the same.\n\\[\n\\begin{align}\n  Y_\\bullet (t) &= \\mathcal{B}^s \\{ X_\\bullet (\\cdot) \\} (t)  \\\\\n  &= X_\\bullet (t - s)\n\\end{align}\n\\qquad(12.4)\\]\nHere \\(\\mathcal{B}\\) denotes the back-shift operator that shifts time by one unit, so that \\(\\mathcal{B}^s\\) (that is, \\(s\\) repeated applications of \\(\\mathcal{B}\\)) shifts time by \\(s\\) units. Then second-order stationarity can be expressed as follows.\n\\[\n\\begin{align}\n  Cov \\{ X_a (t + u), X_b (t) \\}\n  &= Cov \\{ Y_a (t + u), Y_b (t) \\} \\\\\n  &= Cov \\{ X_a (t + u - s), X_b (t - s) \\} \\\\\n  \\\\\n  & \\text{for all } s \\in \\mathbb{Z} \\text{ and } a, b \\in \\{ 1, \\ldots, d \\}\n\\end{align}\n\\]\n\n\n12.2.3 ACF: autocorrelation function\nSetting \\(s = t\\) in the last expression we have\n\\[\n\\begin{align}\n  Cov \\{ X_a (t + u), X_b (t) \\}\n  &= Cov \\{ X_a (u), X_b (0) \\}\n\\end{align}\n\\qquad(12.5)\\]\nConsequently, second-order stationarity enables one to define and estimate the following auto-covariance function.\n\\[\n\\begin{align}\n  \\gamma_{\\bullet, \\bullet} (u) &= \\left \\{ \\gamma_{a, b} (u) \\right \\}_{a, b = 1}^d \\\\\n  \\\\\n  \\text{where} \\\\\n  \\gamma_{a, b} (u) &= Cov \\{ X_a (t + u), X_b (t) \\} \\\\\n  &= Cov \\{ X_a (u), X_b (0) \\}\n\\end{align}\n\\qquad(12.6)\\]\nAlthough \\(\\gamma_{\\bullet, \\bullet} (u)\\) is a symmetric matrix at \\(u = 0\\), it is not generally symmetric at \\(u \\ne 0\\). The symmetry relation we do have is that matrix \\(\\gamma_{\\bullet, \\bullet} (-u)\\) is the transpose of matrix \\(\\gamma_{\\bullet, \\bullet} (u)\\).\n\\[\n\\begin{align}\n  \\gamma_{a, b} (-u) &= Cov \\{ X_a (t - u), X_b (t) \\} \\\\\n  &= Cov \\{ X_a (t), X_b (t + u) \\}  \\\\\n  &= Cov \\{ X_b (t + u), X_a (t) \\}   \\\\\n  &= \\gamma_{b, a} (u)\n\\end{align}\n\\qquad(12.7)\\]\nIn particular, \\(\\gamma_{a, a} (\\cdot)\\) is an even function: \\(\\gamma_{a, a} (-u) = \\gamma_{a, a} (u)\\).\nConsequently, the time-reversed process \\(Y_\\bullet (t) = X_\\bullet (-t)\\) has the same auto-covariance structure as the original process, allowing for this matrix transposition.\nThe autocorrelation (ACF) function \\(\\rho_{\\bullet, \\bullet}(\\cdot)\\) is the following scaled version of the auto-covariance function.\n\\[\n\\begin{align}\n\\rho_{\\bullet, \\bullet} (u) &= \\left \\{ \\rho_{a, b} (u) \\right \\}_{a, b = 1}^d \\\\\n  \\\\\n  \\text{where} \\\\\n  \\rho_{a, b} (u) &= corr \\{ X_a (u), X_b (0) \\} \\\\\n  &= \\frac{\\gamma_{a, b} (u)}{\\sqrt{\\gamma_{a, a} (0) \\; \\gamma_{b, b} (0)}}\n\\end{align}\n\\qquad(12.8)\\]\n\n\n12.2.4 PACF: partial autocorrelation function\nThe partial autocorrelation function (PACF)4 is a refinement of the ACF and a useful diagnostic tool.\nWe first recall the related concept of conditional expectation. If \\(X, Y, Z_\\bullet = \\{Z_1, \\ldots, Z_K\\}\\) are random variables having a joint probability distribution, then we denote the conditional expectation of \\(X\\) and \\(Y\\) given \\(Z_\\bullet\\) as follows.\n\\[\n\\begin{align}\n  \\tilde{X} &= E(X \\; | \\; Z_\\bullet) \\\\\n  \\tilde{Y} &= E(Y \\; | \\; Z_\\bullet)\n\\end{align}\n\\qquad(12.9)\\]\nOf all possible functions of \\(Z_\\bullet\\), the conditional expectation above minimizes the mean squared approximation error.\n\\[\n\\begin{align}\n  MSE \\left \\{ X, \\; f(Z_\\bullet) \\right \\} &= E \\left \\{ (X - f(Z_\\bullet))^2 \\right \\} \\\\\n  MSE \\left \\{ Y, \\; g(Z_\\bullet) \\right \\} &= E \\left \\{ (Y - g(Z_\\bullet))^2 \\right \\}\n\\end{align}\n\\qquad(12.10)\\]\nIf \\(X, Y, Z_\\bullet\\) have a joint normal distribution, then \\(\\tilde{X}\\) is an affine function of \\(Z_\\bullet\\), that is, a constant plus a linear combination of the components of \\(Z_\\bullet\\), and so is \\(\\tilde{Y}\\).\nIn general, however \\(X, Y, Z_\\bullet\\) may be jointly distributed, we denote by \\(\\hat{X}\\) and \\(\\hat{Y}\\) the affine functions of \\(Z_\\bullet\\) that minimize the mean squared approximation error.\n\\[\n\\begin{align}\n  \\hat{\\beta}_\\bullet &= \\arg \\min_{\\beta_\\bullet} \\; MSE \\left \\{ X, \\; \\beta_0 + \\sum_{k = 1}^K \\beta_k Z_k \\right \\} \\\\\n  \\hat{X} &= \\hat{\\beta}_0 + \\sum_{k = 1}^K \\hat{\\beta}_k Z_k\n\\end{align}\n\\qquad(12.11)\\]\n\\(\\hat{Y}\\) is similarly defined. In the normal case we have \\(\\tilde{X} = \\hat{X}\\) and \\(\\tilde{Y} = \\hat{Y}\\).\nThen the partial correlation between \\(X\\) and \\(Y\\) given \\(Z_\\bullet\\) is the correlation between the respective \\(X\\) and \\(Y\\) residuals, as follows.\n\\[\n\\begin{align}\n  \\rho_{X, Y \\bullet Z_\\bullet} &= corr \\left (X - \\hat{X} \\; , \\; Y - \\hat{Y} \\right)\n\\end{align}\n\\qquad(12.12)\\]\nIn the time series context we introduce the PACF with reference to a univariate, stationary Gaussian process, \\(\\mathcal{N}(t)\\), having zero mean.5 6\nAt lags \\(|u| \\le 1\\) the partial autocorrelation is defined as the autocorrelation \\(\\rho (u)\\). For \\(u &gt; 1\\) the PACF is defined to be the correlation between \\(\\mathcal{N}(t + u)\\) and \\(\\mathcal{N}(t)\\) conditioned on the intervening variables \\(\\mathcal{N}(t + 1), \\ldots, \\mathcal{N}(t + u - 1)\\). Let \\(\\Delta_u\\) denote these intervening integer increments \\(\\{ 1, 2, \\ldots, u-1 \\}\\).\n\\[\n\\begin{align}\n   \\Delta_u &= \\{ 1, 2, \\ldots, u-1 \\} \\\\\n   -\\Delta_u &= \\{ -1, -2, \\ldots, 1-u \\} \\\\\n   \\\\\n   & \\text{ for } u \\ge 2\n\\end{align}\n\\]\nThe calculations are as follows.\n\\[\n\\begin{align}\n   \\hat{\\mathcal{N}}(t + u \\; | -\\Delta_u)\n   &= E \\left \\{ \\mathcal{N}(t + u) \\;\n     | \\; \\mathcal{N}(t + u + \\nu), \\; \\nu \\in -\\Delta_u \\right \\} \\\\\n   &= \\beta_1 \\mathcal{N}(t + u - 1) + \\cdots\n     + \\beta_{u-1} \\mathcal{N}(t + 1)\n\\end{align}\n\\qquad(12.13)\\]\n\\[\n\\begin{align}\n   \\hat{\\mathcal{N}}(t \\; | \\; \\Delta_u)\n   &= E \\left \\{ \\mathcal{N}(t + u) \\;\n     | \\; \\mathcal{N}(t + \\nu), \\; \\nu \\in \\Delta_u \\right \\} \\\\\n   &= \\beta_1 \\mathcal{N}(t + 1) + \\cdots\n     + \\beta_{u-1} \\mathcal{N}(t + u - 1)\n\\end{align}\n\\qquad(12.14)\\]\nThe conditional expectation is linear in the conditioning variables thanks to the assumption that \\(\\mathcal{N}(t)\\) is a Gaussian process having zero mean. In the non-Gaussian case the linear combination is defined as the one that minimizes the mean-squared approximation error.\nThe coefficients in the linear combination are shared by \\(\\hat{\\mathcal{N}}(t)\\) and \\(\\hat{\\mathcal{N}}(t + u)\\) but are applied in reverse order. This is because the time-reversed process \\(\\mathcal{N}(-t)\\) has the same auto-covariance structure as the original process \\(\\mathcal{N}(t)\\).7\nThe partial autocorrelation function is now defined as the correlation of the residual variables:8\n\\[\n\\begin{align}\n  \\rho (u | u) &= corr \\left (\\mathcal{N}(t + u) - \\hat{\\mathcal{N}}(t + u \\; | -\\Delta_u) \\; , \\; \\mathcal{N}(t) - \\hat{\\mathcal{N}}(t \\; | \\; \\Delta_u) \\right)\n\\end{align}\n\\qquad(12.15)\\]\nThe calculation of the PACF can be based on the recursive Durbin-Levinson algorithm9:\n\\[\n\\begin{align}\n  \\rho (u | u) &= \\frac{\\rho(u) - \\sum_{\\nu = 1}^{u - 1} \\rho(u-1 | \\nu) \\;  \\rho(u-\\nu) }{1 - \\sum_{\\nu = 1}^{u - 1} \\rho(u-1 | \\nu) \\;  \\rho(\\nu) } \\\\\n  \\\\\n  \\text{where} \\\\\n  \\\\\n  \\rho (u | \\nu) &= \\rho (u-1 | \\nu) \\; - \\; \\rho (u | u) \\; \\rho (u-1 | u-\\nu)\n\\end{align}\n\\qquad(12.16)\\]",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#operations-on-time-series",
    "href": "ts-forecast.html#operations-on-time-series",
    "title": "12  Time Series Forecasting",
    "section": "12.3 Operations on Time Series",
    "text": "12.3 Operations on Time Series\n\n12.3.1 Transformation of Each Observation\nThe data transformations used on independent observations are applicable to time series data. For example, in the case of Johnson and Johnson quarterly earnings (Figure 12.10), we see that the transformed time series, \\(Y(t) = \\log_e (X(t))\\), can be more closely approximated by a linear trend.\n\n\n12.3.2 Smoothing\nIn addition to the data transformations used on independent observations, we often want to smooth out local fluctuations in time series data to show the overall trend more clearly.\n\n12.3.2.1 Running Average\nA running average (also called a moving average) of a random process \\(X(\\cdot)\\) is a new process \\(Y(\\cdot)\\) formed as a local average of \\(X\\) values. A weighted running average, with weights \\(\\{ w_u \\}_u\\) takes the following form.\n\\[\n\\begin{align}\n  Y (t) &= \\sum_{u = -K}^K w_u \\; X (t - u) \\\\\n  \\\\\n  & \\text{where } \\\\\n  \\\\\n  w_u &\\ge 0 \\quad \\text{for all } u \\\\\n  1 &= \\sum_{u = -K}^K w_u\n\\end{align}\n\\qquad(12.17)\\]\nAs an example, Figure 12.1 shows a 5-year simple moving average\n\\[\n\\begin{align}\n  Y_\\bullet (t)\n  &= \\frac{1}{5} \\sum_{u = -2}^2 X_\\bullet (t - u)\n\\end{align}\n\\qquad(12.18)\\]\nFigure 12.11 shows the original global temperature data. As mentioned, the operation smooths out local fluctuations to show the overall trend more clearly.\n\n\n\n\n\n\n\n\nFigure 12.1: Global temperatures: 5-year moving average\n\n\n\n\n\n\n\n12.3.2.2 Exponential Smoothing\nExponential smoothing (also called an exponentially weighted moving average, or EWMA) assigns a prescribed weight \\(\\alpha \\in (0, 1)\\) to the current observation and exponentially decaying weights to prior observations.\nFor a mathematically defined process \\(X(\\cdot)\\) spanning all time, the operation can be represented by the following formula.\n\\[\n\\begin{align}\n  Y(t)\n  &= \\alpha \\; \\sum_{u \\ge 0} (1 - \\alpha)^u X(t - u)\n\\end{align}\n\\qquad(12.19)\\]\nFor finite time series data the operation can be implemented by the following recursive algorithm.\n\\[\n\\begin{align}\n  Y(t) &=\n  \\begin{cases}\n    X(t) & \\text{for } t = t_0 \\\\\n    \\alpha \\; X(t) + (1 - \\alpha) \\; Y(t - 1) & \\text{for } t &gt; t_0\n  \\end{cases}\n\\end{align}\n\\qquad(12.20)\\]\n\n\n\n12.3.3 Differences of Successive Observations\nTime series data often exhibit trends over time. The data fluctuate around some level that varies consistently and can be modeled as some function of time, \\(m(t)\\). Suppose we want to extract the fluctuations about \\(m(t)\\) from the original data. One approach is to estimate \\(m(t)\\) and then form the residual series. This is called “de-trending”. Another approach is to apply an operator that removes the trend directly.\nFor example, suppose that the process \\(X(t)\\) has a linear trend \\(m(t) = \\beta_0 + \\beta_1 t\\).\n\\[\n\\begin{align}\n  m(t) &= E\\{ X(t) \\} = \\beta_0 + \\beta_1 t\n\\end{align}\n\\qquad(12.21)\\]\nThen forming successive differences removes the linear trend.\n\\[\n\\begin{align}\n  E \\left \\{ X(t) - X(t-1) \\right \\} &= \\beta_1\n\\end{align}\n\\qquad(12.22)\\]\nHere we have applied the differencing operator \\(\\nabla\\).\n\\[\n\\begin{align}\n  \\{\\nabla \\; X(\\cdot) \\} \\; (t) &= X(t) - X(t-1) \\\\\n  \\\\\n  \\text{where } \\\\\n  \\\\\n  \\nabla &= \\mathcal{I} - \\mathcal{B}\n\\end{align}\n\\qquad(12.23)\\]\nThe DJIA stock index (Figure 12.12) illustrates the differencing operation. Returns are defined as the log-ratio of successive closing values, calculated as the difference in successive values of the logarithm of the closing values.\nTo remove a polynomial trend of degree \\(d\\), one can apply \\(\\nabla^d\\), meaning \\(d\\) repeated applications of \\(\\nabla\\).\n\n\n12.3.4 Seasonal Differencing\nTo remove a seasonal component one can apply a variant of \\(\\nabla\\), namely seasonal differencing, \\(\\nabla_\\sigma\\): 10\n\\[\n\\begin{align}\n  \\nabla_\\sigma &= \\mathcal{I} - \\mathcal{B}^\\sigma\n\\end{align}\n\\qquad(12.24)\\]\nHere \\(\\sigma\\) denotes the number of time indices that span one season. For analyses of economic or business performance, seasonal differences are often used to show the difference between current values versus those from one year prior. In that context one would set \\(\\sigma = 4\\) for quarterly data, \\(\\sigma = 12\\) for monthly data, etc.\n\n\n12.3.5 Integration: Restoring Differenced Data\nTime series analysis may entail differencing the original process \\(Y(\\cdot) = \\nabla \\; X(\\cdot)\\) and then building a model of the differenced process \\(\\hat{Y}(\\cdot) \\sim Y(\\cdot)\\). We can apply the model to the original process using the following simple recursive algorithm.\n\\[\n\\begin{align}\n  \\hat{X} (t) &=\n  \\begin{cases}\n    X(t) & \\text{ for } t = t_0 \\\\\n    \\hat{Y}(t) + \\hat{X}(t-1) & \\text{ for } t &gt; t_0\n  \\end{cases}\n\\end{align}\n\\qquad(12.25)\\]\nFigure 12.4 illustrates the use of such integration to forecast values of a random walk with drift.\n\n\n12.3.6 Filtering\nRunning averages and successive differences are each an example of a linear, time-invariant filter, having the following general form.11\n\\[\n\\begin{align}\n  Y_\\bullet (t)\n  &= a_{\\bullet, \\bullet} (\\cdot) \\; * \\; X_\\bullet (\\cdot)  \\\\\n  &= \\sum_u a_{\\bullet, \\bullet} (u) \\; \\times \\; X_\\bullet (t - u)\n\\end{align}\n\\qquad(12.26)\\]\nwhere \\(a_{\\bullet, \\bullet} (\\cdot)\\) is a sequence of \\(d \\times d\\) matrices and \\(*\\) denotes discrete convolution.\nFor example, here’s the 5-year moving average above expressed in this format.\n\\[\n\\begin{align}\n  a_{\\bullet, \\bullet} (u) &=\n    \\begin{cases}\n      \\frac{1}{5} I & \\text{ for } |u| \\le 2 \\\\\n      0 & \\text{ for } |u| &gt; 2\n    \\end{cases}\n\\end{align}\n\\qquad(12.27)\\]\nAnd here’s the difference operator \\(\\nabla\\) in the filtering format.\n\\[\n\\begin{align}\n  a_{\\bullet, \\bullet} (u) &=\n    \\begin{cases}\n      I  & \\text{ for } u = 0 \\\\\n      -I & \\text{ for } u = 1 \\\\\n      0  & \\text{ otherwise }\n    \\end{cases}\n\\end{align}\n\\qquad(12.28)\\]\nA moving average operation smooths the time series on which it operates. That is, it allows low-frequency components to pass, while diminishing high-frequency components. Therefore the moving average operation is categorized as a low-pass filter. The differencing operator can be categorized as a high-pass filter.\nIn signal-processing applications a linear, time-invariant filter \\(a_{\\bullet, \\bullet} (\\cdot)\\) may be designed to extract certain types of signals that are contaminated by noise.(hamming1989?)",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#model-components",
    "href": "ts-forecast.html#model-components",
    "title": "12  Time Series Forecasting",
    "section": "12.4 Model Components",
    "text": "12.4 Model Components\nThe process of building a statistical model (time series or not) is typically cyclic:\n\nexamine data (or model residuals)\npropose model (or refinement of current model)\nfit proposed model\nexamine model residuals\n\nOne might continue this cycle until the model residuals seem to be reasonably free of trend and autocorrelation.\n\n12.4.1 Covariates and Trend\nTime series and other statistical data sets are often collected in order to study the possible effects of one set of variables on one or more of the remaining variables.\nConsider, for example, the cardiovascular mortality data shown in Figure 12.13, where mortality is modeled as a response to temperature and the level of airborne particulate matter. In this example we can formulate the expected value of mortality in the following terms.\n\\[\n\\begin{align}\n  m(t, X_{2:3}(t))\n    &= E \\left \\{ X_1(t) \\; | \\; X_2(t), \\; X_3(t) \\; \\right \\}\n\\end{align}\n\\]\nwhere\n\\[\n\\begin{align}\n  X_1(t) = M(t) &= \\text{mortality count in week } t \\\\\n    X_2(t) = T(t) &= \\text{temperature } (^\\circ F) \\text{ in week } t \\\\\n    X_3(t) = P(t) &= \\text{level of airborne particulate matter in week } t\n\\end{align}\n\\]\nEach of the three components of \\(X_\\bullet (t)\\) exhibits an annual periodicity, as one might expect. The textbook authors initially propose a model for \\(X_1(t)\\) of the following form12.\n\\[\n\\begin{align}\n  X_1(t) &= m(t, X_{2:3}(t)) + W(t)\\\\\n   \\\\\n    \\text{ where } \\\\\n    \\\\\n    m(t, X_{2:3}(t)) &= \\beta_0 + f(t) + \\beta_2 X_2(t) + \\beta_3 X_3(t) \\\\\n    \\\\\n    W(\\cdot) &\\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\]\nHere \\(wn(0, \\sigma_W^2)\\) denotes an uncorrelated sequence having zero mean and variance \\(\\sigma_W^2\\), referred to as white noise.\nAccording to this model13 the pattern over time of \\(X_1(t)\\) is captured by the mean over time, \\(m(t, X_{2:3}(t))\\). Once the mean level is accounted for, the residual series \\(W(t) = X_1 (t) - m(t, X_{2:3}(t))\\) is modeled as being free of autocorrelation. The model is consistent with the assumption that, apart from a trend component, weekly numbers of cardiovascular deaths are independent, conditioned on temperature and particulate levels.\nLater in the text (page 155) the authors refine this initial model. After examining the autocorrelation function (ACF) of the residual series (no longer labeled \\(W(t)\\)), they develop an autocorrelated (\\(AR(2)\\)) model.\nThis is an example of the cyclic model-building process previously outlined. A model is developed, examined, and revised.\n\n\n12.4.2 Auto-regression (AR)\nAuto-regressive models are based on the idea that the current value of the series, \\(X(t)\\), can be approximated, or forecast, as a function of \\(p\\) past values, \\(X(t−1), X(t−2), \\ldots, X(t−p)\\), which leads to the following formulation of an auto-regressive model of order \\(p\\), abbreviated as \\(AR(p)\\).\n\\[\n\\begin{align}\n  X(t) &= \\alpha \\; + \\; \\sum_{\\nu = 1}^p \\phi_\\nu X(t-\\nu) \\; + \\; W(t) \\\\\n  \\text{where } \\\\\n  \\alpha &= \\mu_X \\; - \\; \\mu_X \\; \\sum_{\\nu = 1}^p \\phi_\\nu \\\\\n    \\\\\n    W(t) &\\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\qquad(12.29)\\]\nThat is, \\(W(t)\\) again denotes a residual “white noise” stationary process: free of autocorrelation and having zero mean.\n\n12.4.2.1 Polynomial representation\nThe \\(AR(p)\\) model can also be expressed using the back-shift operator \\(\\mathcal{B}\\) as follows.\n\\[\n\\begin{align}\n  \\left \\{\\phi (\\mathcal{B}) \\; (X(\\cdot) - \\mu_X) \\right \\} \\; (t) &= W(t) \\\\\n  \\\\\n  \\text{where } \\\\\n  \\phi (z) &= 1 - \\sum_{\\nu = 1}^p \\phi_\\nu \\; z^\\nu\n  \\\\\n  \\text{so that } \\\\\n  \\phi (\\mathcal{B}) &= \\mathcal{I} \\; - \\; \\sum_{\\nu = 1}^p \\phi_\\nu \\mathcal{B}^\\nu\n\\end{align}\n\\qquad(12.30)\\]\nThe polynomial \\(\\phi(\\cdot)\\) applied to the back-shift operator \\(\\mathcal{B}\\) is an auto-regressive operator of order \\(p\\).\nNow, the \\(AR(p)\\) model is defined to be that of a stationary process \\(X(t)\\), which constrains the coefficients of polynomial \\(\\phi(\\cdot)\\). Additional requirements that the model be causal14 and invertible15 restrict attention to polynomials \\(\\phi(\\cdot)\\) whose roots are all greater than unity in magnitude. This ensures that the multiplicative inverse function \\(\\phi (z)^{-1}\\) is well defined and has a power series expansion for \\(|z| \\le 1\\).16\n\n\n12.4.2.2 AR(1)\nConsider the \\(AR(1)\\) model. We have\n\\[\n\\begin{align}\n  X(t) &= \\mu_X \\; + \\; \\phi_1 (X(t-1) - \\mu_X) \\; + \\; W(t)\n\\end{align}\n\\]\nso that\n\\[\n\\begin{align}\n  W(t) &= (X(t) - \\mu_X)  \\; - \\; \\phi_1 (X(t-1) - \\mu_X) \\\\\n  &= \\left \\{ (\\mathcal{I} - \\phi_1 \\mathcal{B}) \\; (X(\\cdot) - \\mu_X) \\right \\} \\; (t) \\\\\n  &= \\left \\{ \\phi (\\mathcal{B}) \\; (X(\\cdot) - \\mu_X) \\right \\} \\; (t)\n\\end{align}\n\\qquad(12.31)\\]\nThe auto-regressive polynomial \\(\\phi (\\cdot)\\) has degree 1.\n\\[\n\\begin{align}\n  \\phi (z) &= 1 \\; - \\; \\phi_1 z & \\text{ with } | \\phi_1 | &&lt; 1\n\\end{align}\n\\]\nThe restriction on the real-valued coefficient \\(\\phi_1\\) ensures that \\(\\phi (z) \\ne 0\\) for \\(|z| \\le 1\\), as previously mentioned.\nNow one can show by induction that for \\(u &gt; 0\\) we have\n\\[\n\\begin{align}\n  X(t) - \\mu_X  \\\\\n  &= \\phi_1^u \\; (X(t-u) - \\mu_X) + \\sum_{\\nu = 0}^{u-1} \\phi_1^\\nu \\; W(t-\\nu)\n\\end{align}\n\\]\nIn the limit, as \\(u \\rightarrow \\infty\\), we see that \\(\\phi_1^u \\rightarrow 0\\) so that\n\\[\n\\begin{align}\n  X(t) - \\mu_X \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\phi_1^\\nu \\; W(t-\\nu) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\phi_1^\\nu \\; \\mathcal{B}^\\nu \\; W(t)\n\\end{align}\n\\]\nThis result can be obtained more directly by inverting the auto-regressive operator \\(\\phi (\\mathcal{B})\\).\n\\[\n\\begin{align}\n  W(t) &= \\left \\{ \\phi (\\mathcal{B}) \\; (X(\\cdot) - \\mu_X) \\right \\} \\; (t) \\\\\n  X(t) - \\mu_X &= \\left \\{ \\phi (\\mathcal{B})^{-1} \\; W(\\cdot) \\right \\} (t)\n\\end{align}\n\\qquad(12.32)\\]\nsince\n\\[\n\\begin{align}\n  \\phi (z)^{-1} &= \\frac{1}{1 - \\phi_1 \\; z} \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\phi_1^\\nu \\; z^\\nu & \\text{ for } |z| \\le 1\n\\end{align}\n\\]\nBased on these expressions one can show17 that the auto-covariance and autocorrelation functions of \\(X(\\cdot)\\) take the following forms.\n\\[\n\\begin{align}\n  \\gamma_X (u) &= \\frac{\\phi_1^{|u|}}{1 - \\phi_1^2} \\; \\sigma_W^2\\\\\n  \\\\\n  \\rho_X (u) &= \\phi_1^{|u|}\n\\end{align}\n\\qquad(12.33)\\]\nUsing the Durbin-Levinson algorithm, one can go on to show18 that the partial autocorrelation function (PACF) of \\(X(\\cdot)\\) has the following simple form.\n\\[\n\\begin{align}\n  \\rho_X (u | u) &=\n  \\begin{cases}\n    1 & \\text{ for } u = 0 \\\\\n    \\phi_1 & \\text{ for } u = \\pm 1 \\\\\n    0 & \\text{ for } |u| &gt; 1\n  \\end{cases}\n\\end{align}\n\\qquad(12.34)\\]\n\n\n12.4.2.3 AR(p)\nAs previously noted, an \\(AR(p)\\) model can be defined via a polynomial \\(\\phi(\\cdot)\\) that is applied to the back-shift operator \\(\\mathcal{B}\\). The coefficients of \\(\\phi(\\cdot)\\) are real, but it is useful to apply the polynomial to the complex variable \\(z\\). From Equation 12.30 we have\n\\[\n\\begin{align}\n  \\phi(z) &= 1 \\; - \\; \\sum_{\\nu = 1}^p \\; \\phi_\\nu \\; z^\\nu\n\\end{align}\n\\qquad(12.35)\\]\nAlso as previously mentioned, we require the roots of \\(\\phi(\\cdot)\\) to lie outside the closed unit disc, that is, to be greater than unity in magnitude. This requirement ensures that the model is stationary, causal, and invertible.\nConsequently we have the following causal form of the model.19\n\\[\n\\begin{align}\n  X(t) - \\mu_X &= \\left \\{ \\phi(\\mathcal{B})^{-1} \\; W(\\cdot) \\right \\} (t) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\; \\chi_\\nu \\; \\mathcal{B}^\\nu \\; W(t) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\; \\chi_\\nu \\; W(t - \\nu) \\\\\n  \\text{where } \\\\\n  & \\chi_0 = 1 \\\\\n  \\\\\n  \\text{and } \\\\\n  & \\sum_{\\nu = 0}^\\infty \\; |\\chi_\\nu| \\; &lt; \\; \\infty\n\\end{align}\n\\qquad(12.36)\\]\nEquation 12.30 also shows that \\(W(t)\\) depends on current and past values of \\(X(t)\\), with past values terminating at \\(X(t-p)\\).\n\\[\n\\begin{align}\n  W(t) &= \\left \\{\\phi (\\mathcal{B}) \\; (X(\\cdot) - \\mu_X) \\right \\} \\; (t) \\\\\n  &= X(t) - \\mu_X \\; - \\; \\sum_{\\nu = 1}^p \\phi_\\nu \\; (X(t-\\nu) - \\mu_X)\n\\end{align}\n\\]\nFrom these equations one can show that for lag \\(u &gt; p\\) we have\n\\[\n\\begin{align}\n  \\hat{X}(t+u \\; | -\\Delta_u) &= \\mu_X + \\sum_{\\nu = 1}^p \\; \\phi_\\nu \\; (X(t + u - \\nu) - \\mu_X) \\\\\n  X(t+u) \\; - \\; \\hat{X}(t+u \\; | -\\Delta_u) &= W(t+u)\n\\end{align}\n\\]\nand\n\\[\n\\begin{align}\n  \\hat{X}(t \\; | \\Delta_u) &= \\mu_X + \\sum_{\\nu = 1}^p \\; \\phi_\\nu \\; (X(t + \\nu) - \\mu_X) \\\\\n  X(t) \\; - \\; \\hat{X}(t \\; | \\Delta_u) &=  \\sum_{\\nu = 1}^p \\; \\phi_\\nu \\; (X(t - \\nu) - X(t + \\nu)) \\; + W(t)\n\\end{align}\n\\]\nConsequently, for \\(u &gt; p\\) we have\n\\[\n\\begin{align}\n  & Cov(X(t+u) \\; - \\; \\hat{X}(t+u \\; | -\\Delta_u), \\; X(t) \\; - \\; \\hat{X}(t \\; | \\Delta_u)) \\\\\n  &= Cov(W(t+u), \\; X(t) \\; - \\; \\hat{X}(t \\; | \\Delta_u)) \\\\\n  &= Cov(W(t+u), \\; X(t)) \\; - \\; Cov(W(t+u), \\; \\hat{X}(t \\; | \\Delta_u)) \\\\\n  &= 0\n\\end{align}\n\\]\nTherefore the PACF is zero at lags \\(u &gt; p\\).\n\\[\n\\begin{align}\n  \\rho_X (u | u) &= 0 & \\text{ for } u &gt; p\n\\end{align}\n\\]\n\n\n\n12.4.3 Moving Average (MA)\nA moving average model of order \\(q\\) has the following form.\n\\[\n\\begin{align}\n  X(t) &= \\mu_X \\; + \\; W(t) \\; + \\; \\sum_{\\nu = 1}^q \\; \\theta_\\nu \\; W(t - \\nu) \\\\\n  \\text{where } \\\\\n  W(\\cdot) &\\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\]\nLike AR models, an MA model can be expressed by applying a polynomial \\(\\theta(\\cdot)\\) to the back-shift operator \\(\\mathcal{B}\\).\n\\[\n\\begin{align}\n  X(t) - \\mu_x  &= W(t) \\; + \\; \\sum_{\\nu = 1}^q \\; \\theta_\\nu \\; W(t - \\nu) \\\\\n  &= \\left \\{\\mathcal{I} + \\sum_{\\nu = 1}^q \\; \\theta_\\nu \\; \\mathcal{B}^\\nu \\right \\} \\; W(t) \\\\\n  &= \\theta(\\mathcal{B}) \\; W(t) \\\\\n  \\\\\n  \\text{where} \\\\\n  \\theta (z) &= 1 + \\sum_{\\nu = 1}^q \\; \\theta_\\nu \\; z^\\nu\n\\end{align}\n\\]\nThat is, \\(X(\\cdot)\\) is filtered white noise, and is therefore stationary for any finite set of filter coefficients. But some constraint on polynomial \\(\\theta(\\cdot)\\) is required to eliminate ambiguity.\n\n12.4.3.1 Invertibility\nConsider the following two MA(1) models.20\n\\[\n\\begin{align}\n  X(t) &= W(t) + \\frac{1}{5} W(t-1) & \\text{with } W(\\cdot) \\sim N(0, 25) \\\\\n  \\\\\n  Y(t) &= V(t) + 5 \\; V(t-1) & \\text{with } V(\\cdot) \\sim N(0, 1)\n\\end{align}\n\\]\nThe probability distributions of the two processes \\(X(\\cdot)\\) and \\(Y(\\cdot)\\) are identical21, so anyone attempting to identify the respective models could not distinguish them based on their realizations as time series data.\nA solution to this conundrum is to follow the example set by AR models, namely by requiring the MA model to be invertible. That is, we require the roots of polynomial \\(\\theta(\\cdot)\\) to have magnitude greater than unity. In the example above we therefore choose the model for \\(X(\\cdot)\\) rather than the model for \\(Y(\\cdot)\\).\n\n\n12.4.3.2 MA(1)\nConsider the \\(MA(1)\\) model\n\\[\n\\begin{align}\n  X(t) &= W(t) + \\theta_1 W(t-1) \\\\\n  \\\\\n  &\\text{ with } |\\theta_1| &lt; 1 \\\\\n  &\\text{ and } W(\\cdot) \\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\qquad(12.37)\\]\nHere is the auto-covariance function for this process.\n\\[\n\\begin{align}\n  \\gamma_X (0) &= \\sigma_X^2 \\\\\n  &= \\sigma_W^2 \\; (1 + \\theta_1^2) \\\\\n  \\\\\n  \\gamma_X (1) &= Cov(W(t+1) + \\theta_1 W(t), \\; W(t) + \\theta_1 W(t-1)) \\\\\n  &= \\sigma_W^2 \\; \\theta_1 \\\\\n  \\\\\n  \\gamma_X (u) &= 0 \\quad \\text{for } |u| \\ge 2\n\\end{align}\n\\qquad(12.38)\\]\nAnd here is the corresponding autocorrelation function (ACF)22.\n\\[\n\\begin{align}\n  \\rho_X (u) &=\n  \\begin{cases}\n    1 & \\text{ for } u = 0 \\\\\n    \\frac{\\theta_1}{1 + \\theta_1^2} & \\text{ for } u = \\pm 1 \\\\\n    0 & \\text{ for } |u| \\ge 2 \\\\  \n  \\end{cases}\n\\end{align}\n\\qquad(12.39)\\]\nThe PACF turns out to have the following form23.\n\\[\n\\begin{align}\n  \\rho_X (u | u) &=  \\frac{(- \\theta_1)^u(1 - \\theta_1^2)}{1 - \\theta_1^{2 (u+1)}} & \\text{ for } u \\ge 2\n\\end{align}\n\\qquad(12.40)\\]\n\n\n12.4.3.3 MA(q)\nAn \\(MA(q)\\) model is generated by a polynomial \\(\\theta(\\cdot)\\) of degree \\(q\\) of the following form.\n\\[\n\\begin{align}\n  \\theta(z) &= 1 + \\sum_{\\nu = 1}^q \\theta_\\nu \\; z^\\nu \\\\\n  &= \\sum_{\\nu = 0}^q \\theta_\\nu \\; z^\\nu & \\text{with } \\theta_0 = 1\n\\end{align}\n\\qquad(12.41)\\]\nApplying \\(\\theta(\\cdot)\\) to the back-shift operator \\(\\mathcal{B}\\), we have the following \\(MA(q)\\) model.\n\\[\n\\begin{align}\n  X(t) &= W(t) + \\sum_{\\nu = 1}^q \\theta_\\nu \\; W(t - \\nu) \\\\\n  &= \\left \\{ \\theta(\\mathcal{B}) \\; W(\\cdot) \\right \\} (t)\n  \\\\\n  \\\\\n  &\\text{ with } W(\\cdot) \\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\qquad(12.42)\\]\nWith \\(MA(1)\\) serving as an example, it is easy to see that for an \\(MA(q)\\) model of \\(X(\\cdot)\\), the auto-covariance function \\(\\gamma_X (u)\\) and the autocorrelation function \\(\\rho_X (u)\\) equal zero at lags \\(|u| &gt; q\\).\n\\[\n\\begin{align}\n  \\text{for } X &\\sim MA(q) \\\\\n  \\text{and } |u| &&gt; q \\\\\n  \\\\\n  \\rho_X (u) &= 0\n\\end{align}\n\\qquad(12.43)\\]\nWe require the roots of polynomial \\(\\theta(\\cdot)\\) to have magnitudes greater than unity to ensure that its multiplicative inverse function, \\(\\theta(z)^{-1}\\), has a power series expansion valid for \\(|z| \\le 1\\).\n\\[\n\\begin{align}\n  \\theta (z)^{-1} &= \\frac{1}{\\theta (z)} \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\psi_\\nu \\; z^\\nu \\\\\n  \\\\\n  & \\text{for } |z| \\le 1 \\\\\n  \\\\\n  & \\text{where } \\psi_0 = 1\n\\end{align}\n\\qquad(12.44)\\]\nThen \\(\\theta(\\mathcal{B})^{-1}\\) is well-defined, and we have\n\\[\n\\begin{align}\n  W(t) &= \\left \\{ \\theta(\\mathcal{B})^{-1} \\; X(\\cdot) \\right \\} (t)  \\\\\n  &= X(t) + \\sum_{\\nu = 1}^\\infty \\psi_\\nu \\; X(t - \\nu)\n\\end{align}\n\\qquad(12.45)\\]\nEquivalently, we have\n\\[\n\\begin{align}\n  X(t) &= W(t) - \\sum_{\\nu = 1}^\\infty \\psi_\\nu \\; X(t - \\nu)\n\\end{align}\n\\qquad(12.46)\\]\nwhich can be viewed as an infinite-order auto-regressive model. Now for an \\(AR(p)\\), with \\(p\\) finite, the PACF can be shown to be non-zero at lag \\(u = p\\) (and zero thereafter). In the case of the \\(MA(q)\\) model, equivalent to an infinite-order \\(AR\\) model, the PACF tends to zero but does not vanish as \\(|u| \\rightarrow \\infty\\).",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#armap-q",
    "href": "ts-forecast.html#armap-q",
    "title": "12  Time Series Forecasting",
    "section": "12.5 ARMA(p, q)",
    "text": "12.5 ARMA(p, q)\nThe \\(AR(p)\\) and \\(MA(q)\\) models can be combined into the following \\(ARMA(p,q)\\) model.\n\\[\n\\begin{align}\n  \\phi(\\mathcal{B}) \\; (X(\\cdot) - \\mu_X) &= \\theta(\\mathcal{B}) \\; W(\\cdot) & \\text{with } W(\\cdot) \\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\qquad(12.47)\\]\nso that\n\\[\n\\begin{align}\n  X(t) &= \\mu_X + \\sum_{j = 1}^p \\phi_j \\; (X(t - j) - \\mu_X) \\; + \\; W(t) \\; + \\; \\sum_{k = 1}^q \\theta_k \\; W(t - k)\n\\end{align}\n\\qquad(12.48)\\]\nor more succinctly\n\\[\n\\begin{align}\n  X(t) &= \\alpha + \\sum_{j = 1}^p \\phi_j \\; X(t - j) \\; + \\; W(t) \\; + \\; \\sum_{k = 1}^q \\theta_k \\; W(t - k) \\\\\n  \\text{where } \\\\\n  \\alpha &= \\mu_x \\; \\left ( 1  - \\sum_{j = 1}^p \\phi_j \\right )\n\\end{align}\n\\qquad(12.49)\\]\n\n12.5.1 Co-prime Polynomials\nWe must impose a restriction on the polynomials \\(\\phi(\\cdot)\\) and \\(\\theta(\\cdot)\\) in addition to requiring their respective roots to all have magnitude greater than unity. The additional requirement is that they have no roots in common.\nConsider for example the following putative \\(ARMA(1,1)\\) model.24\n\\[\n\\begin{align}\n  X(t) &= \\beta_1 \\; X(t - 1) \\; + \\; W(t) \\; - \\; \\beta_1 \\; W(t - 1) \\\\   \\\\\n  \\text{so that } \\\\\n  \\\\\n  p(\\mathcal{B}) \\; X(\\cdot) &= p(\\mathcal{B}) \\; W(\\cdot) \\\\   \\\\\n  \\text{where } \\\\\n  \\\\\n  p(z) &= 1 - \\beta_1 \\; z \\quad \\text{with } |\\beta_1| &lt; 1\n\\end{align}\n\\qquad(12.50)\\]\nThe operator \\(p(\\mathcal{B})\\) can be cancelled from both sides of the equation, leaving only a white noise process: \\(X(\\cdot) = W(\\cdot)\\).\nMore generally we require the elimination of any polynomial factors common to both \\(\\phi(\\cdot)\\) and \\(\\theta(\\cdot)\\). Equivalently, we require the roots of \\(\\phi(\\cdot)\\) to be distinct from those of \\(\\theta(\\cdot)\\).\n\n\n12.5.2 Causal Form\nThe \\(ARMA(p,q)\\) model can be expressed in causal form as follows.25 26\n\\[\n\\begin{align}\n  X(t) - \\mu_X &= \\phi(\\mathcal{B})^{-1} \\;  \\theta(\\mathcal{B}) \\; W(t) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\; \\psi_\\nu \\; \\mathcal{B}^\\nu \\; W(t) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\; \\psi_\\nu \\; W(t - \\nu) \\\\\n  \\text{where } \\\\\n  & \\psi_0 = 1 \\\\\n  \\\\\n  \\text{and } \\\\\n  & \\sum_{\\nu = 0}^\\infty \\; |\\psi_\\nu| \\; &lt; \\; \\infty\n\\end{align}\n\\qquad(12.51)\\]\n\n\n12.5.3 Inverted Form\nIt is also useful to invert the MA operator in an ARMA model to obtain the white noise process \\(W(t)\\) as a linear function of current and past values of \\(X(t)\\).27 28\n\\[\n\\begin{align}\n  W(t) &= \\theta(\\mathcal{B})^{-1} \\phi(\\mathcal{B}) \\; (X(t) - \\mu_X) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\; \\pi_\\nu \\; \\mathcal{B}^\\nu \\; (X(t) - \\mu_X) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\; \\pi_\\nu \\; (X(t - \\nu) - \\mu_X) \\\\\n  \\text{where } \\\\\n  & \\pi_0 = 1 \\\\\n  \\\\\n  \\text{and } \\\\\n  & \\sum_{\\nu = 0}^\\infty \\; |\\pi_\\nu| \\; &lt; \\; \\infty\n\\end{align}\n\\qquad(12.52)\\]",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#linear-predictors",
    "href": "ts-forecast.html#linear-predictors",
    "title": "12  Time Series Forecasting",
    "section": "12.6 Linear Predictors",
    "text": "12.6 Linear Predictors\n\n12.6.1 Problem Statement\nLet \\(X(\\cdot)\\) be a stationary process, whose \\(T\\) time indices, \\(\\tau_T\\), begin with \\(t_0\\) and end with \\(t_f\\) per Equation 12.2.\nFor \\(u \\ge 1\\) let \\(\\hat{X} (u | \\tau_T)\\) denote the linear least squares predictor of \\(X(u + t_f)\\) as an affine function of \\(\\{ X(t) \\}_{t \\in \\tau_T}\\).\n\\[\n\\begin{align}\n  (\\hat{\\alpha}, \\hat{\\beta}_\\bullet) &= \\arg \\min_{\\alpha, \\beta_\\bullet} \\; MSE \\left \\{ X(u + t_f), \\; \\alpha + \\sum_{\\nu = 0}^{T-1} \\beta_\\nu X(t_0 + \\nu) \\right \\} \\\\\n  \\hat{X} (u | \\tau_T) &= \\hat{\\alpha} + \\sum_{\\nu = 0}^{T-1} \\hat{\\beta}_\\nu X(t_0 + \\nu)\n\\end{align}\n\\qquad(12.53)\\]\n\n\n12.6.2 Linear Equations\nWe assume for the moment that the first and second-order statistics (mean, variance, auto-covariance, …) of \\(X(\\cdot)\\) are known in order to express the coefficients \\(\\hat{\\alpha}, \\hat{\\beta}_\\bullet\\) as functions of those statistics.\nConsider the expression for mean-squared error (MSE).\n\\[\n\\begin{align}\n  MSE \\left \\{ X(u + t_f), \\; \\alpha + \\sum_{\\nu = 0}^{T-1} \\beta_\\nu X(t_0 + \\nu) \\right \\} \\\\\n  = E \\left \\{ \\left ( X(u + t_f) \\; - \\alpha - \\sum_{\\nu = 0}^{T-1} \\beta_\\nu X(t_0 + \\nu) \\right )^2 \\right \\}\n\\end{align}\n\\qquad(12.54)\\]\nWe take the derivative of this expression with respect to each coefficient \\((\\alpha, \\beta_\\bullet)\\) and set that derivative to zero to define \\((\\hat{\\alpha}, \\hat{\\beta}_\\bullet)\\). For \\(\\hat{\\alpha}\\) we have\n\\[\n\\begin{align}\n  0 &= E \\left \\{ \\left ( X(u + t_f) \\; - \\hat{\\alpha} - \\sum_{\\nu = 0}^{T-1} \\hat{\\beta}_\\nu X(t_0 + \\nu) \\right ) \\right \\} \\\\\n  &= \\mu_X - \\hat{\\alpha}  \\; - \\; \\sum_{\\nu = 0}^{T-1} \\hat{\\beta}_\\nu \\; \\mu_X\n\\end{align}\n\\qquad(12.55)\\]\nso that\n\\[\n\\begin{align}\n  \\hat{\\alpha} &= \\mu_X \\left (1 - \\sum_{\\nu = 0}^{T-1} \\hat{\\beta}_\\nu \\right )\n\\end{align}\n\\qquad(12.56)\\]\nFor each of the remaining coefficients \\(\\hat{\\beta}_j\\) we obtain\n\\[\n\\begin{align}\n  0 &= E \\left \\{ \\left ( X(u + t_f) \\; - \\hat{\\alpha} - \\sum_{\\nu = 0}^{T-1} \\hat{\\beta}_\\nu X(t_0 + \\nu) \\right ) \\; X(t_0 + j) \\right \\} \\\\\n  &= E \\left \\{ \\left ( X(u + t_0 + T - 1) - \\mu_x - \\sum_{\\nu = 0}^{T-1} \\hat{\\beta}_\\nu \\left ( X(t_0 + \\nu) - \\mu_x \\right ) \\right ) \\; X(t_0 + j) \\right \\} \\\\\n  &= \\gamma_X(u - j + T - 1) \\; - \\; \\sum_{\\nu = 0}^{T-1} \\hat{\\beta}_\\nu \\; \\gamma_X(\\nu - j)\n\\end{align}\n\\qquad(12.57)\\]\nIn vector-matrix notation we have the following linear system.29\n\\[\n\\begin{align}\n  \\Gamma[\\cdot, \\cdot] \\; \\hat{\\beta}_\\bullet &= \\tilde{\\gamma} [\\cdot]\n\\end{align}\n\\qquad(12.58)\\]\nwhere we set the vector and matrix indices to \\(j, \\nu \\in \\nu_T = \\{ 0, \\dots, T-1 \\}\\), so that\n\\[\n\\begin{align}\n  \\Gamma [j, \\nu] & = \\gamma_X(\\nu - j) \\\\\n  \\tilde{\\gamma} [j] &= \\gamma_X (u - j + T - 1)\n\\end{align}\n\\qquad(12.59)\\]\n\\(\\Gamma\\) is a covariance matrix, and is therefore non-negative definite. In fact \\(\\Gamma\\) is positive definite (non-singular) unless \\(X(\\cdot)\\) is a degenerate process30. In any case \\(\\hat{\\beta}_\\bullet\\) can be defined by applying the inverse (or a generalized inverse) of \\(\\Gamma\\) to \\(\\tilde{\\gamma} [\\cdot]\\).\nAs noted above, we have assumed for the sake of discussion that the entire auto-covariance function \\(\\gamma_X (\\cdot)\\) is known. In practice such statistics must be estimated from the data. But direct estimation is not possible, due to the restricted range of differences in data indices.31 Instead, we must rely on a model of the process for such estimates.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#arma-forecastingtxt-arma-forecasting",
    "href": "ts-forecast.html#arma-forecastingtxt-arma-forecasting",
    "title": "12  Time Series Forecasting",
    "section": "12.7 ARMA Forecasting32",
    "text": "12.7 ARMA Forecasting32\n\n12.7.1 Notation and Assumptions\nFor presentation purposes we continue to assume that all random process parameters are known. And to simplify notation we first consider the case of a stationary Gaussian process \\(\\mathcal{N} (\\cdot)\\) having mean 0.\n\\[\n\\begin{align}\n  \\mathcal{N}(t) &\\sim N(0, \\sigma_\\mathcal{N}^2) \\quad \\forall \\; t\n\\end{align}\n\\qquad(12.60)\\]\nIn this case we can express the intended least squares linear approximation of \\(\\mathcal{N}(t)\\) based on \\(\\{ \\mathcal{N}(s) \\}_{s \\in \\mathcal{S}}\\) as a conditional expectation. In fact it will be convenient to be able to condition on the entire past of the random process covering indices \\(\\wp (t_f)\\) up to and including the index \\(t_f\\) of the final observation.\n\\[\n\\begin{align}\n  \\wp (t_f) &= \\left \\{ s \\in \\mathbb{Z} \\; | \\; s \\le t_f \\right \\}\n\\end{align}\n\\qquad(12.61)\\]\nWe adopt the following notation for the expectation of \\(\\mathcal{N}(u + t_f)\\), the process at a future time, conditioned on \\(\\{ \\mathcal{N}(s) \\}_{s \\le t_f}\\), the history of the process prior to and including the final observation.33\n\\[\n\\begin{align}\n  \\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp (t_f)) &= E \\left \\{ \\mathcal{N} (u + t_f) \\; | \\; \\mathcal{N} (s), \\; s \\le t_f \\right \\}\n\\end{align}\n\\qquad(12.62)\\]\nDue to the normality of \\(\\mathcal{N}(\\cdot)\\) this conditional expectation \\(\\tilde{\\mathcal{N}}\\) coincides with the linear least squares predictor \\(\\hat{\\mathcal{N}}\\), which is the defining linear predictor for non-normal processes.\n\\[\n\\begin{align}\n  \\hat{\\mathcal{N}} (u + t_f \\; | \\; \\wp (t_f)) &= \\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp (t_f))\n\\end{align}\n\\qquad(12.63)\\]\nWe also assume that this Gaussian process follows an \\(ARMA(p, q)\\) model,\n\\[\n\\begin{align}\n  \\phi(\\mathcal{B}) \\; \\mathcal{N} (\\cdot) &= \\theta(\\mathcal{B}) \\; W(\\cdot) & \\text{with iid } W(\\cdot) \\sim N(0, \\sigma_W^2)\n\\end{align}\n\\qquad(12.64)\\]\nwhere the roots of both the AR polynomial, \\(\\phi(\\cdot)\\), and the MA polynomial, \\(\\theta(\\cdot)\\), have magnitudes greater than unity. This ensures that \\(\\phi(\\mathcal{B})^{-1}\\) and \\(\\theta(\\mathcal{B})^{-1}\\) are well defined.\n\n\n12.7.2 Predicted Variable\nFor fixed \\(u \\ge 1\\) the predicted variable is \\(\\mathcal{N} (u + t_f)\\), expressed below in both causal and inverted forms.\n\\[\n\\begin{align}\n  \\mathcal{N} (u + t_f) &= \\sum_{\\nu = 0}^\\infty \\; \\psi_\\nu \\; W(u + t_f - \\nu), \\quad \\psi_0 = 1\n\\end{align}\n\\qquad(12.65)\\]\n\\[\n\\begin{align}\n  W(u + t_f) &= \\sum_{\\nu = 0}^\\infty \\; \\pi_\\nu \\; \\mathcal{N} (u + t_f - \\nu), \\quad \\pi_0 = 1\n\\end{align}\n\\qquad(12.66)\\]\nSince \\(\\pi_0 = 1\\) the inverted form can also be expressed as an infinite AR model.\n\\[\n\\begin{align}\n  \\mathcal{N} (u + t_f) &= W(u + t_f) -  \\sum_{\\nu = 1}^\\infty \\; \\pi_\\nu \\; \\mathcal{N} (u + t_f - \\nu)\n\\end{align}\n\\qquad(12.67)\\]\n\n\n12.7.3 Predictor: Causal Form\nWe now take the expectations in Equation 12.65 conditioned on \\(\\{ \\mathcal{N} (s) \\}_{s \\le t_f}\\).\n\\[\n\\begin{align}\n  \\tilde{\\mathcal{N}} (u \\; | \\; \\wp(t_f)) &= \\sum_{\\nu = 0}^\\infty \\; \\psi_\\nu \\; E \\left \\{ W(u + t_f - \\nu) \\; | \\; \\mathcal{N} (s), \\; s \\le t_f \\right \\}\n\\end{align}\n\\qquad(12.68)\\]\nExpanding \\(\\mathcal{N} (s)\\) in causal form, we see that\n\\[\n\\begin{align}\n  E \\left \\{ W(u + t_f - \\nu) \\; | \\; \\mathcal{N} (s), \\; s \\le t_f \\right \\} &=\n  \\begin{cases}\n    0 & \\text{ for } \\nu &lt; u \\\\\n    W(u + t_f - \\nu) & \\text{ for } \\nu \\ge u\n  \\end{cases}\n\\end{align}\n\\]\nThis eliminates the first \\(u\\) terms in Equation 12.68 and simplifies the remaning terms.\n\\[\n\\begin{align}\n  \\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp(t_f)) &= \\sum_{\\nu = u}^\\infty \\; \\psi_\\nu \\; W(u + t_f - \\nu)\n\\end{align}\n\\qquad(12.69)\\]\n\n\n12.7.4 Predictor: Inverted Form\nNext, applying the same conditional expectations to Equation 12.66, we have\n\\[\n\\begin{align}\n  0 &= \\sum_{\\nu = 0}^\\infty \\; \\pi_\\nu \\; \\tilde{\\mathcal{N}} (u + t_f - \\nu \\; | \\; \\wp (t_f))\n\\end{align}\n\\]\nSince \\(\\pi_0 = 1\\), this is equivalent to:\n\\[\n\\begin{align}\n  \\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp (t_f) &= - \\sum_{\\nu = 1}^\\infty \\; \\pi_\\nu \\; \\tilde{\\mathcal{N}} (u + t_f - \\nu \\; | \\; \\wp (t_f))\n\\end{align}\n\\qquad(12.70)\\]\nNote that the terms on the right can be simplified for \\(\\nu \\ge u\\).\n\\[\n\\begin{align}\n  \\tilde{\\mathcal{N}} (u - \\nu +t_f \\; | \\; \\wp (t_f)\n  &= \\mathcal{N} (u - \\nu + t_f) \\quad \\text{for } \\nu \\ge u\n\\end{align}\n\\]\nApplying this result we have\n\\[\n\\begin{align}\n  \\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp (t_f) &= - \\sum_{\\nu = 1}^{u - 1} \\; \\pi_\\nu \\; \\tilde{\\mathcal{N}} (u + t_f - \\nu \\; | \\; \\wp (t_f)) \\; - \\; \\sum_{\\nu = u}^\\infty \\; \\pi_\\nu \\; \\mathcal{N} (u + t_f - \\nu)\n\\end{align}\n\\qquad(12.71)\\]\n\n\n12.7.5 Predictor Residuals\nWe now examine the residuals from the linear least squares predictors defined above. Let \\(R(\\cdot \\; | \\; \\wp (t_f))\\) denote the residuals of the predictors.\n\\[\n\\begin{align}\n  R(t \\; | \\; \\wp (t_f)) &= \\mathcal{N} (t) \\; - \\; \\tilde{\\mathcal{N}} (t \\; | \\; \\wp (t_f)), \\quad t &gt; t_f\n\\end{align}\n\\qquad(12.72)\\]\nCombining Equation 12.67 and Equation 12.71 we can express process \\(R(u + t_f \\; | \\; \\wp (t_f))\\) for \\(u \\ge 1\\) in a finite auto-regressive form (that expands with \\(u\\)).\n\\[\n\\begin{align}\n  R(u + t_f \\; | \\; \\wp (t_f)) &= \\mathcal{N} (u + t_f) \\; - \\; \\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp (t_f)) \\\\\n  &= W(u + t_f) - \\sum_{\\nu = 1}^{u - 1} \\; \\pi_\\nu \\; R(u - \\nu\\; | \\; \\wp (t_f))\n\\end{align}\n\\qquad(12.73)\\]\nSimilarly, combing Equation 12.65 and Equation 12.69 we can express \\(R(u + t_f \\; | \\; \\wp (t_f))\\) as the following finite moving average of the white noise terms (where the number of terms again increases with \\(u\\)).\n\\[\n\\begin{align}\n  R(u + t_f \\; | \\; \\wp (t_f)) &= \\mathcal{N} (u + t_f) \\; - \\; \\tilde{\\mathcal{N}} (u \\; | \\; \\wp (t_f) \\\\\n  &= \\sum_{\\nu = 0}^{u - 1} \\; \\psi_\\nu \\; W(u + t_f - \\nu)\n\\end{align}\n\\qquad(12.74)\\]\nFrom this last result we see that the residual random variable \\(R(u + t_f \\; | \\; \\wp (t_f))\\) has zero mean.\n\\[\n\\begin{align}\n  E\\{ R(u + t_f \\; | \\; \\wp (t_f)) \\} &= 0\n\\end{align}\n\\qquad(12.75)\\]\nAlso note that \\(R(u + t_f \\; | \\; \\wp (t_f))\\) and \\(R(u + k + t_f \\; | \\; \\wp (t_f))\\) are correlated in general, for \\(u \\ge 1\\) and \\(k \\ge 0\\).\n\\[\n\\begin{align}\n  E \\left \\{ R(u + t_f \\; | \\; \\wp (t_f)) \\times R(u + k + t_f \\; | \\; \\wp (t_f)) \\right \\} &= \\sigma_W^2 \\; \\sum_{\\nu = 0}^{u - 1} \\; \\psi_\\nu \\; \\psi_{\\nu + k}\n\\end{align}\n\\qquad(12.76)\\]\nSetting \\(k = 0\\) we obtain the following expected square of \\(R(u + t_f \\; | \\; \\wp (t_f))\\), called mean squared prediction error.34\n\\[\n\\begin{align}\n  E\\{ R(u + t_f \\; | \\; \\wp (t_f))^2 \\} &= \\sigma_W^2 \\; \\sum_{\\nu = 0}^{u - 1} \\; \\psi_\\nu^2\n\\end{align}\n\\qquad(12.77)\\]\nAs \\(u \\rightarrow \\infty\\) this residual variance approaches the variance of the original process \\(\\mathcal{N} (\\cdot)\\).\n\\[\n\\begin{align}\n  \\lim_{u \\rightarrow \\infty} E\\{ R(u\\; | \\; \\wp (t_f))^2 \\} &= \\sigma_W^2 \\; \\sum_{\\nu = 0}^\\infty \\; \\psi_\\nu^2 = \\sigma_\\mathcal{N}^2\n\\end{align}\n\\qquad(12.78)\\]\n\n\n12.7.6 Truncated Predictors35\nUsing the entire history of the process \\(\\{ \\mathcal{N} (s), \\; s \\le t_f \\}\\) preceding and including the observed values, we’ve expressed the linear predictor \\(\\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp(t_f))\\) as an infinite moving average (Equation 12.69) and, alternatively, as an infinite auto-regression (Equation 12.71).\nNow, restricting the predictor to the available data by using the \\(ARMA(p, q)\\) model, we recursively define the following “truncated” predictor, \\(\\breve{\\mathcal{N}} (t \\; | \\; \\tau_T)\\), of the process \\(\\mathcal{N} (t)\\) at time \\(t\\).\n\\[\n\\begin{align}\n  \\breve{\\mathcal{N}} (u + t_f \\; | \\; \\tau_T) &= \\sum_{j = 1}^p \\phi_j \\; \\breve{\\mathcal{N}} (u - j + t_f \\; | \\; \\tau_T) \\\\\n  &+ \\sum_{k = 0}^q \\theta_k \\; \\breve{W} (u - k + t_f \\; | \\; \\tau_T) \\\\\n  \\\\\n  \\text{where } \\\\\n  \\\\\n  \\breve{W} (t \\; | \\; \\tau_T) &= E \\left \\{ W(t) \\; | \\; \\mathcal{N} (s), \\; s \\in \\tau_T \\right \\}\n\\end{align}\n\\qquad(12.79)\\]\nIn this recursive definition, when the time index \\(t\\) is within the range \\(\\tau_T\\) of observed values, the truncated predictor of the data value is just the data value. And the truncated predictor of the process prior to the data is set to the mean value of the process, which is zero here. That is\n\\[\n\\begin{align}\n  \\breve{\\mathcal{N}} (t \\; | \\; \\tau_T) &=\n  \\begin{cases}\n    \\mathcal{N} (t) & \\text{if } t \\in \\tau_T \\\\\n    0 & \\text{if } t &lt; t_0\n  \\end{cases}\n\\end{align}\n\\qquad(12.80)\\]\nIn addition, the conditional expectation of the white noise terms are based on the \\(ARMA(p, q)\\) model as follows.\n\\[\n\\begin{align}\n  \\breve{W} (t \\; | \\; \\tau_T) \\\\\n  &=\n  \\begin{cases}\n    \\phi (\\mathcal{B}) \\mathcal{N} (t) + (\\mathcal{I} - \\theta(\\mathcal{B})) \\; \\breve{W} (t \\; | \\; \\tau_T)  & \\text{if } t \\in \\tau_T \\\\\n    0 & \\text{if } t \\not \\in \\tau_T\n  \\end{cases}\n\\end{align}\n\\]\nAs previously noted, we assume all parameters to be known, so that the polynomials \\(\\phi(\\cdot)\\) and \\(\\theta(\\cdot)\\) are completely specified. Then the system of equations above can be solved recursively, as illustrated in the next example.\n\n\n12.7.7 ARMA(1, 1) Simulation36\nWe illustrate the truncated predictor for an \\(ARMA(1, 1)\\) model:\n\\[\n\\begin{align}\n  \\mathcal{N}(t) &= \\phi_1 \\; \\mathcal{N}(t - 1) \\; + \\; W(t) \\; + \\; \\theta_1 \\; W(t - 1)\n\\end{align}\n\\qquad(12.81)\\]\nThe truncated predictor is\n\\[\n\\begin{align}\n  \\breve{\\mathcal{N}} (u + t_f \\; | \\; \\tau_T) &= \\phi_1 \\; \\breve{\\mathcal{N}} (u - 1 + t_f \\; | \\; \\tau_T) \\; + \\; \\breve{W} (u + t_f \\; | \\; \\tau_T) \\; + \\; \\theta_1 \\; \\breve{W} (u - 1 + t_f \\; | \\; \\tau_T)\n\\end{align}\n\\qquad(12.82)\\]\nSetting \\(u = 1\\), we have\n\\[\n\\begin{align}\n  \\breve{\\mathcal{N}} (1 + t_f \\; | \\; \\tau_T) &= \\phi_1 \\; \\breve{\\mathcal{N}} (t_f \\; | \\; \\tau_T) \\; + \\; \\breve{W} (1 + t_f \\; | \\; \\tau_T) \\; + \\; \\theta_1 \\; \\breve{W} (t_f \\; | \\; \\tau_T) \\\\\n  &= \\phi_1 \\; \\mathcal{N} (t_f \\; | \\; \\tau_T) \\; + \\; 0 \\; + \\; \\theta_1 \\; \\breve{W} (t_f \\; | \\; \\tau_T)\n\\end{align}\n\\qquad(12.83)\\]\nFor \\(u \\ge 2\\) we have\n\\[\n\\begin{align}\n  \\breve{\\mathcal{N}} (u + t_f \\; | \\; \\tau_T) &= \\phi_1 \\; \\breve{\\mathcal{N}} (u - 1 + t_f \\; | \\; \\tau_T)\n\\end{align}\n\\qquad(12.84)\\]\nwhich can be calculated recursively.\nTo initialize these recursive calculations we still need to determine \\(\\breve{W} (t_f \\; | \\; \\tau_T)\\). To this end we re-express the ARMA model and set the following initial conditions.\n\\[\n\\begin{align}\n  \\breve{W} (t \\; | \\; \\tau_T) &= \\mathcal{N}(t) \\; - \\; \\phi_1 \\; \\mathcal{N}(t - 1) \\; - \\; \\theta_1 \\; \\breve{W} (t - 1 \\; | \\; \\tau_T) \\\\\n  \\\\\n  &\\text{for } t \\in \\tau_T, \\text{ and setting } \\\\\n  \\\\\n  0 &= \\mathcal{N} (t_0 - 1) = \\breve{W} (t_0 - 1 \\; | \\; \\tau_T)\n\\end{align}\n\\qquad(12.85)\\]\nWe can solve the last equation in a forward recursion with \\(t \\in (t_0, \\ldots, t_f)\\) to obtain \\(\\breve{W} (t_f \\; | \\; \\tau_T)\\), as required.\nThe forecast variance can be approximated from Equation 12.77 using \\(\\psi\\) coefficients that turn out to satisfy 37 38\n\\[\n\\begin{align}\n  \\psi_j &= (\\phi_1 + \\theta_1) \\; \\phi_1^{j-1} \\quad \\text{for } j \\ge 1\n\\end{align}\n\\qquad(12.86)\\]\nWe then have\n\\[\n\\begin{align}\n  Var \\{ \\breve{R} (u + t_f \\; | \\; \\tau_T) \\} &= E \\{ \\breve{R} (u + t_f \\; | \\; \\tau_T)^2 \\} \\\\\n  &= \\sigma_W^2 \\; \\sum_{\\nu = 0}^{u - 1} \\; \\psi_\\nu^2 \\\\\n  &= \\sigma_W^2 \\; \\left ( 1 + \\frac{(\\phi_1 + \\theta_1)^2 (1 - \\phi_1^{2 (u - 1)})}{1 - \\phi_1^2} \\right ) \\\\\n  \\\\\n  & \\text{where } \\\\\n  \\\\\n  \\breve{R} (u + t_f \\; | \\; \\tau_T) &= \\mathcal{N}(u + t_f) \\; - \\; \\breve{\\mathcal{N}} (u + t_f \\; | \\; \\tau_T)\n\\end{align}\n\\qquad(12.87)\\]\nThe precision of the forecast at time index \\(u + t_f\\) can be assessed with a prediction interval of the following form\n\\[\n\\begin{align}\n  \\breve{\\mathcal{N}} (u + t_f \\; | \\; \\tau_T) \\; &\\pm \\; c_{\\alpha / 2} \\; \\sqrt{ Var \\{ \\breve{R} (u + t_f \\; | \\; \\tau_T) \\} } \\\\\n  \\\\\n  & \\text{where coefficient } c_{\\alpha / 2} \\text{ is the normal quantile:} \\\\\n  \\\\\n  c_{\\alpha / 2} &= \\Phi^{-1} (1 - \\alpha / 2)\n\\end{align}\n\\qquad(12.88)\\]\nWe now set \\(\\phi_1 = \\theta_1 = 0.5\\) in order to simulate 150 observations from this model and forecast the last 50 of the 150 observations. This example uses astsa functions sarima.sim() and sarima.for().39\n\n\n\n\n\n\n\n\nFigure 12.2: ARMA(1, 1) simulation, with coefficients (.5, .5)\n\n\n\n\n\n\n\n12.7.8 Recruitment Forecast\nFigure 12.14 shows the Southern Oscillation Index (SOI, astsa::soi) and associated Recruitment (astsa::rec), an index of the number of young fish entering the cohort available for commercial fishing. Both series consist of 453 monthly values during the years 1950–1987.\n(shumway2025?) fit the following preliminary AR(2) model to the recruitment index.\n\\[\n\\begin{align}\n  \\mathcal{N} (t) &= \\alpha + \\phi_1 \\mathcal{N} (t-1) + \\phi_2 \\mathcal{N} (t-2) + W(t) \\\\\n  \\\\\n  & \\text{with estimated coefficients } \\\\\n  \\\\\n  \\hat{\\alpha} &= 6.74 \\\\\n  \\hat{\\phi}_1 &= 1.35 \\\\\n  \\hat{\\phi}_2 &= -0.46 \\\\\n  \\hat{\\sigma}_W^2 &= 89.72\n\\end{align}\n\\qquad(12.89)\\]\nFor purposes of discussion let us accept the coefficient estimates as given constants that specify the \\(AR(2)\\) model. Then the truncated predictor can be determined recursively from the following formula.\n\\[\n\\begin{align}\n  \\breve{\\mathcal{N}} (t \\; | \\; \\tau_T) &= \\alpha +  \\phi_1 \\breve{\\mathcal{N}} (t-1 \\; | \\; \\tau_T) + \\phi_2 \\breve{\\mathcal{N}} (t-2 \\; | \\; \\tau_T) + \\breve{W}(t \\; | \\; \\tau_T) \\\\\n  \\\\\n  \\text{with } \\\\\n  \\\\\n  \\breve{\\mathcal{N}} (t \\; | \\; \\tau_T) &=\n  \\begin{cases}\n    \\mathcal{N} (t) & \\text{if } t \\in \\tau_T \\\\\n    0 & \\text{if } t &lt; t_0\n  \\end{cases} \\\\\n  \\\\\n  \\text{and } \\\\\n  \\\\\n  \\breve{W}(t \\; | \\; \\tau_T) &= 0 \\quad \\text{if } t \\not \\in \\tau_T\n\\end{align}\n\\qquad(12.90)\\]\nTo approximate prediction variance we need the \\(\\psi\\) coefficients of the causal form of the \\(AR(2)\\) model. From (shumway2025?) we have\n\\[\n\\begin{align}\n  \\psi_j &= \\phi_1 \\psi_{j-1} + \\phi_2 \\psi_{j-2} \\quad \\text{for } j \\ge 2 \\\\\n  \\\\\n  &\\text{with initial conditions } \\\\\n  \\\\\n  \\psi_0 &= 1 \\\\\n  \\psi_1 &= \\phi_1\n\\end{align}\n\\]\nThese calculations are incorporated into the following astsa forecast.\n\n\n\n\n\n\n\n\nFigure 12.3: Recruitment index followed by a 24-month AR(2) forecast",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#non-stationary-models",
    "href": "ts-forecast.html#non-stationary-models",
    "title": "12  Time Series Forecasting",
    "section": "12.8 Non-Stationary Models",
    "text": "12.8 Non-Stationary Models\n\n12.8.1 ARIMA(p, d, q)\nTime series models have a long history. (whittle1951hypothesis?) introduced ARMA models of stationary processes. To treat non-stationary processes, (box1970time?) popularized auto-regressive integrated moving average (ARIMA) models. Here’s the form of an ARIMA model having \\(p, d, q\\) as its respective orders of auto-regression, differencing, and moving average.40\n\\[\n\\begin{align}\n  \\phi(\\mathcal{B}) \\; \\nabla^d \\; (X(\\cdot) - \\mu_X)\n  &= \\theta(\\mathcal{B}) \\; W(\\cdot) \\\\\n  \\\\\n  \\text{where } \\\\\n  & \\phi (z) = 1 - \\sum_{j = 1}^p \\phi_j \\; z^j \\\\\n  & \\theta (z) = 1 + \\sum_{k = 1}^q \\theta_k \\; z^k\n\\end{align}\n\\qquad(12.91)\\]\nBy convention, \\(d \\in \\{ 0, 1, 2, \\ldots \\}\\) so that \\(ARIMA(p, 0, q)\\) is synonymous with \\(ARMA(p, q)\\).\nIf \\(d \\ge 1\\) then \\(\\nabla^d\\) removes \\(\\mu_X\\), which can be omitted from the formula for simplificaton. As previously noted the operator \\(\\nabla^d\\) removes any polynomial trend of degree \\(d\\) or less.\nIn addition to removing non-random trends in the data, the differencing operator \\(\\nabla\\) can also remove random drift. Consider the following model.\n\\[\n\\begin{align}\n  X(t) &= Y(t) + W(t) \\\\\n  Y(t) &= Y(t-1) + V(t) \\\\\n  \\\\\n  W(\\cdot) &\\sim \\text{iid } wn(0, \\sigma_W^2) \\\\\n  V(\\cdot) &\\sim \\text{iid } wn(0, \\sigma_V^2) \\\\\n  W(\\cdot), \\; V(\\cdot) &\\sim \\text{independent }\n\\end{align}\n\\qquad(12.92)\\]\nThe process \\(Y(\\cdot)\\) is a random walk whose variance increases over time and is thus non-stationary. Therefore \\(X(\\cdot)\\) is also non-stationary. But applying the differencing operator \\(\\nabla\\) to \\(X(\\cdot)\\) induces stationarity.41\n\\[\n\\begin{align}\n  \\nabla \\; X(t) &= \\nabla \\; Y(t) + \\nabla \\; W(t) \\\\\n  &= V(t) \\; + \\; \\nabla \\; W(t)\n\\end{align}\n\\]\n\n\n12.8.2 Random Walk with Drift\nNow consider the following model which, like the example just given, is a variation on a simple random walk.\n\\[\n\\begin{align}\n  X(t) &= \\alpha \\; + \\; X(t-1) + \\; W(t)\n\\end{align}\n\\qquad(12.93)\\]\n\\(X(\\cdot)\\) is not stationary since its mean and variance change over time. In this case, however, \\(\\nabla X(t)\\) is simply a constant plus white noise.\n\\[\n\\begin{align}\n  \\nabla X(t) &= X(t) \\; - \\; X(t-1) \\\\\n  &= \\alpha \\; + \\; W(t)\n\\end{align}\n\\]\nThe coefficient \\(\\alpha\\) is called drift in this context.\nThe truncated predictor at \\(u = 1\\) (called the one step ahead forecast) can be determined as follows. Consider the case in which both \\(X(\\cdot)\\) and \\(W(\\cdot)\\) are Gaussian processes. This enables us to use conditional expectations to derive the form of the best linear predictor in the non-Gaussian case. We have\n\\[\n\\begin{align}\n  \\breve{X} (1 + t_f \\; | \\; \\tau_T) &= E \\left \\{ X(1 + t_f) \\; | \\; X(t_0), \\; \\ldots, \\; X(t_f) \\right \\} \\\\\n  &= E \\left \\{ \\alpha \\; + \\; X(t_f) + \\; W(1 + t_f) \\; | \\; X(t_0), \\; \\ldots, \\; X(t_f) \\right \\} \\\\\n  &= \\alpha \\; + \\; X(t_f)\n\\end{align}\n\\qquad(12.94)\\]\nAt \\(u = 2\\) we have\n\\[\n\\begin{align}\n  \\breve{X} (2 + t_f \\; | \\; \\tau_T) &= E \\left \\{ X(2 + t_f) \\; | \\; X(t_0), \\; \\ldots, \\; X(t_f) \\right \\} \\\\\n  &= E \\left \\{ \\alpha \\; + \\; X(1 +t_f) + \\; W(2 + t_f) \\; | \\; X(t_0), \\; \\ldots, \\; X(t_f) \\right \\} \\\\\n  &= \\alpha \\; + \\; \\breve{X} (1 + t_f \\; | \\; \\tau_T) \\\\\n  &= 2 \\alpha \\; + \\; X(t_f)\n\\end{align}\n\\qquad(12.95)\\]\nBy induction we have\n\\[\n\\begin{align}\n  \\breve{X} (u + t_f \\; | \\; \\tau_T) &= u \\times  \\alpha \\; + \\; X(t_f) \\quad \\text{for } u \\ge 1\n\\end{align}\n\\qquad(12.96)\\]\nThus the sequence of predictors forms a ray anchored at \\(X(t_f)\\) having slope \\(\\alpha\\).\nThe error bounds for this predictor are based on the mean squared prediction error, that is the expected value of the squared residual:\n\\[\n\\begin{align}\n  E \\{ \\breve{R} (u + t_f \\; | \\; \\tau_T)^2 \\}\n\\end{align}\n\\]\nwhere\n\\[\n\\begin{align}\n  \\breve{R} (u + t_f \\; | \\; \\tau_T) &= X(u + t_f) \\; - \\; \\breve{X} (u + t_f \\; | \\; \\tau_T)\n\\end{align}\n\\]\nNow by induction one can show that\n\\[\n\\begin{align}\n  X(u + t_f) &= u \\times \\alpha \\; + \\; X(t_f) + \\; \\sum_{\\nu = 1}^u W(\\nu + t_f) \\\\\n  &= \\breve{X} (u + t_f \\; | \\; \\tau_T) + \\; \\sum_{\\nu = 1}^u W(\\nu + t_f)\n\\end{align}\n\\]\nTherefore\n\\[\n\\begin{align}\n  \\breve{R} (u + t_f \\; | \\; \\tau_T) &= X(u + t_f) \\; - \\; \\breve{X} (u + t_f \\; | \\; \\tau_T) \\\\\n  &= \\sum_{\\nu = 1}^u W(\\nu + t_f)\n\\end{align}\n\\]\nso that\n\\[\n\\begin{align}\n  E \\{ \\breve{R} (u + t_f \\; | \\; \\tau_T)^2 \\} &= u \\times \\sigma_W^2\n\\end{align}\n\\]\nWe now simulate this model, setting the drift parameter \\(\\alpha = 0.2\\) and simulating \\(W(\\cdot)\\) as iid standard normal variables, so that \\(\\sigma_W^2 = 1\\). We simulate a total of 150 values, with the first 100 serving as the observations from which the predictors are calculated and compared to the final 50 values. The results are shown in figure Figure 12.4.\n\\[\n\\begin{align}\n  \\breve{R} (u + t_f \\; | \\; \\tau_T) &= X(u + t_f) \\; - \\; \\breve{X} (u + t_f \\; | \\; \\tau_T)\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nFigure 12.4: Random walk with drift: cumsum of N(drift = 0.2, sd = 1)\n\n\n\n\n\n\n\n12.8.3 EWMA Example\nThe following \\(ARIMA(0, 1, 1)\\) model has been applied to many economic time series.42\n\\[\n\\begin{align}\n  \\nabla \\; X(\\cdot) &= (\\mathcal{I - \\lambda B}) \\; W(\\cdot) \\\\\n  \\\\\n  & \\text{so that } \\\\\n  \\\\\n  X(t) &= X(t-1) + W(t) - \\lambda \\; W(t-1) \\\\\n  \\\\\n  & \\text{with } | \\lambda | &lt; 1, \\text{ and } W(\\cdot) \\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\qquad(12.97)\\]\nThe model is also referred to as an integrated moving average model, or \\(IMA(1, 1)\\). The model leads to a frequently used forecasting method called exponentially weighted moving averages (EWMA), and also called simple exponential smoothing (SES).\nNote that the multiplicative inverse of the polynomial \\(1 - \\lambda z\\) is a geometric series in \\(\\lambda z\\). This gives the following inverted form of the model.\n\\[\n\\begin{align}\n  W(t) &= \\left ( \\mathcal{I} - \\lambda \\mathcal{B} \\right )^{-1} \\; \\nabla X (t) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\lambda^\\nu \\; \\mathcal{B}^\\nu \\; \\nabla X (t) \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\lambda^\\nu \\; (X(t - \\nu) \\; - \\; X(t - \\nu - 1)) \\\\\n  &= X(t) \\; - \\; (X(t - 1) - \\; \\lambda \\; X(t - 1)) \\; - \\cdots \\\\\n  &= X(t) \\; - \\; \\sum_{\\nu = 1}^\\infty (1 - \\lambda) \\; \\lambda^{\\nu - 1} \\; X(t - \\nu)\n\\end{align}\n\\]\nRearranging terms we have\n\\[\n\\begin{align}\n  X(t) &= (1 - \\lambda) \\; \\sum_{\\nu = 1}^\\infty \\lambda^{\\nu - 1} \\; X(t - \\nu) \\; + \\; W(t)\n\\end{align}\n\\qquad(12.98)\\]\nThat is, the current value \\(X(t)\\) is an exponential smoothing of all past values plus \\(W(t)\\), the white noise term.\nThe linear prediction of the current value based on past values is thus the same exponential smoothing of past values.43\n\\[\n\\begin{align}\n  \\hat{X}(t \\; | \\; \\wp (t-1)) &= (1 - \\lambda) \\; \\sum_{\\nu = 1}^\\infty \\lambda^{\\nu - 1} \\; X(t - \\nu)\n\\end{align}\n\\qquad(12.99)\\]\nso that\n\\[\n\\begin{align}\n  X(t) &= \\hat{X}(t \\; | \\; \\wp (t-1)) \\; + \\; W(t)\n\\end{align}\n\\qquad(12.100)\\]\nThe predictor \\(\\hat{X}(t \\; | \\; \\wp (t-1))\\) is called the one step ahead linear predictor. We now have the following linear recursion formula.\n\\[\n\\begin{align}\n  \\hat{X}(t \\; | \\; \\wp (t-1)) &= (1 - \\lambda) \\; X(t - 1) \\; + \\; (1 - \\lambda) \\; \\sum_{\\nu = 2}^\\infty \\lambda^{\\nu - 1} \\; X(t - \\nu) \\\\\n  &= (1 - \\lambda) \\; X(t - 1) \\; + \\; (1 - \\lambda) \\; \\sum_{\\nu = 1}^\\infty \\lambda^\\nu \\; X(t -1 - \\nu) \\\\\n  &= (1 - \\lambda) \\; X(t - 1) \\; + \\; \\lambda \\left \\{ (1 - \\lambda) \\; \\sum_{\\nu = 1}^\\infty \\lambda^{\\nu - 1} \\; X(t -1 - \\nu) \\right \\}  \\\\\n  &= (1 - \\lambda) \\; X(t - 1) \\; + \\; \\lambda \\; \\hat{X}(t -1 \\; | \\; \\wp (t-2))\n\\end{align}\n\\qquad(12.101)\\]\nThat is, the one step ahead predictor of the current process value \\(X(t)\\) based on the past is a convex combination of \\(X(t-1)\\), the most recent process value, and the preceding one step ahead predictor.\nIn order to predict two steps ahead we modify Equation 12.97 by changing \\(t\\) to \\(t + 1\\), while continuing to take conditional expectations with respect to \\(\\{X(s), \\; s \\le t - 1 \\}\\).\n\\[\n\\begin{align}\n  X(t+1) - X(t) &= W(t+1) - \\lambda \\; W(t)\n\\end{align}\n\\]\nso that\n\\[\n\\begin{align}\n  \\hat{X}(t + 1 \\; | \\; \\wp (t-1)) \\; - \\; \\hat{X}(t \\; | \\; \\wp (t-1) &= 0\n\\end{align}\n\\]\nThat is,\n\\[\n\\begin{align}\n  \\hat{X}(t + 1 \\; | \\; \\wp (t-1)) &= \\hat{X}(t \\; | \\; \\wp (t-1)\n\\end{align}\n\\qquad(12.102)\\]\nThen by induction on \\(u \\ge 1\\) we obtain\n\\[\n\\begin{align}\n  \\hat{X}(t + u \\; | \\; \\wp (t-1)) &= \\hat{X}(t \\; | \\; \\wp (t-1) \\quad \\text{for } u \\ge 1\n\\end{align}\n\\qquad(12.103)\\]\nIn words, forecasting beyond one step reverts back to the one step ahead predictor.\nNow, the truncated one step ahead predictor based on the data follows a similar recursion.\n\\[\n\\begin{align}\n  \\breve{X} \\left ( 1 + t \\; | \\; [t_0, \\; t] \\right ) &=\n  \\begin{cases}\n    X(t_0) & \\text{for } t = t_0 \\\\\n    (1 - \\lambda) \\; X(t) + \\lambda \\breve{X}(t \\; | \\; [t_0, \\; t - 1] & \\text{for } t_0 &lt; t &lt; t_f\n  \\end{cases}\n\\end{align}\n\\qquad(12.104)\\]\nTo calculate predictor error bounds we require \\(\\psi\\) coefficients from the causal form of the model. In this example they are the power series coefficients of the rational function \\(\\psi(z) = (1 - \\lambda \\; z)(1 - z)^{-1}\\). This gives\n\\[\n\\begin{align}\n  \\psi_\\nu &=\n  \\begin{cases}\n    1 & \\text{for } \\nu = 0 \\\\\n    1 - \\lambda & \\text{for } \\nu \\ge 1\n  \\end{cases}\n\\end{align}\n\\]\nSquaring these \\(\\psi\\) coefficients we have\n\\[\n\\begin{align}\n  & MSE \\left\\{ X (u + t_0), \\; \\breve{X}(u + t_0 \\; | \\; \\tau_u) \\right \\} \\\\\n  &= \\sigma_W^2 \\; \\left \\{ 1 + (u-1)\\; (1 - \\lambda)^2 \\right \\} \\quad \\text{for } 0 &lt; u &lt; T\n\\end{align}\n\\]\nTo forecast beyond the data we assume for the moment that \\(X(\\cdot)\\) and \\(W(\\cdot)\\) are Gaussian processes in order to derive least squares predictors form conditional expectations.\n\\[\n\\begin{align}\n  \\breve{X}(u + t_f \\; | \\; \\tau_T)\n  &= E \\left \\{ X (u + t_f) \\; | \\; X(s), s \\in \\tau_T \\right \\}\n\\end{align}\n\\]\nNow, based on the \\(IMA(1, 1)\\) model we have 44\n\\[\n\\begin{align}\n  & X (u + t_f) \\\\\n  &= X(u - 1 + t_f) + W(u + t_f) - \\lambda W(u - 1 + t_f) \\\\\n  & \\vdots \\\\\n  &= X(t_f) + W(u + t_f) - \\lambda W(t_f) + (1 - \\lambda) \\; \\sum_{\\nu = 1}^{u -1} W(u - \\nu + t_f) \\\\\n  \\\\\n  & \\text{where } u \\ge 1\n\\end{align}\n\\]\nConsequently\n\\[\n\\begin{align}\n  & \\breve{X}(u + t_f \\; | \\; \\tau_T) \\\\\n  &= E \\left \\{ X (u + t_f) \\; | \\; X(s), s \\in \\tau_T \\right \\} \\\\\n  &= X(t_f)\n\\end{align}\n\\]\nThe mean squared prediction error is thus the expected square of \\(X(u + t_f) \\; - \\; X(t_f)\\).\n\\[\n\\begin{align}\n  & MSE \\left\\{ X (u + t_f), \\; \\breve{X}(u + t_f \\; | \\; \\tau_T) \\right \\} \\\\\n  &= \\sigma_W^2 \\; \\left \\{ 1 + \\lambda^2 + (u-1)\\; (1 - \\lambda)^2 \\right \\} \\quad \\text{for } u \\ge 1\n\\end{align}\n\\]\nHaving determined the MSE, we summarize results for the one step ahead truncated predictor itself.\n\\[\n\\begin{align}\n  \\breve{X} \\left ( 1 + t \\; | \\; [t_0, \\; t] \\cap \\tau_T \\right ) &=\n  \\begin{cases}\n    X(t_0) & \\text{for } t = t_0 \\\\\n    (1 - \\lambda) \\; X(t) + \\lambda \\breve{X}(t \\; | \\; [t_0, \\; t - 1] & \\text{for } t_0 &lt; t &lt; t_f \\\\\n    X(t_f) & \\text{for } t_f \\le t\n  \\end{cases}\n\\end{align}\n\\qquad(12.105)\\]\nFigure 12.5 shows a simulation of the IMA process \\(X(\\cdot)\\), along with one step ahead exponential smoothing of past values, and the \\(IMA(1, 1)\\) forecast of future values. We set \\(\\lambda =\\) 0.8, equivalent to an EWMA smoothing parameter of \\(\\alpha = 1 - \\lambda =\\) 0.2.\n\n\n\n\n\n\n\n\nFigure 12.5: IMA(1, 1) with exponenetial smoothing\n\n\n\n\n\n\n\n12.8.4 ETS\n\n12.8.4.1 ETS Intro\nExponential smoothing with trend and seasonality (ETS) is a modeling framework, distinct from ARIMA, based on Holt-Winters forecasting, which is widely used in economic analyses. 45 46\nIn this framework the forecast \\(\\breve{X}(u + t \\; | \\; s \\le t)\\), with \\(s, t \\in \\tau_T\\), consists of three components:\n\nlevel \\(\\ell (t)\\);\nlocal trend \\(\\mathit{b} (t)\\); and\nseasonality \\(\\mathscr{s} (u + t \\; | \\; s \\le t)\\).\n\nThe forecast equation expresses \\(\\breve{X}(u + t \\; | \\; s \\le t)\\) as a linear combination of these components, and each component has its own linear equation. Together these equations define a state-space model.\nIn this section we merely outline the ETS framework. For further details see the cited resources. Also, note that R function fable::ETS() selects and fits the models described in this section, requiring only modest guidance from the user.\n\n\n12.8.4.2 Level component\nWe begin with simple exponential smoothing (SES), which entails the level component \\(\\ell(\\cdot)\\) alone, a case covered from the ARIMA perspective in Section 12.8.3. Following ETS conventions we change parameters by setting \\(\\alpha = 1 - \\lambda\\), with \\(\\alpha\\) restricted to the open unit interval. Then we have the following linear recursion for the one step ahead predictor.\n\\[\n\\begin{align}\n  \\breve{X} \\left ( t + 1 \\; | \\; [t_0, \\; t] \\cap \\tau_T \\right ) &=\n  \\begin{cases}\n    X(t_0) & \\text{for } t = t_0 \\\\\n    \\alpha \\; X(t) + (1 - \\alpha) \\breve{X}(t \\; | \\; [t_0, \\; t - 1] & \\text{for } t_0 &lt; t &lt; t_f \\\\\n    X(t_f) & \\text{for } t_f \\le t\n  \\end{cases} \\\\\n  \\\\\n  &\\text{where } \\alpha \\in (0, 1)\n\\end{align}\n\\qquad(12.106)\\]\nNow setting aside these boundary conditions for the moment, the state-space form of this linear recursion is as follows.\n\\[\n\\begin{align}\n  \\breve{X}(t + 1 \\; | \\; s \\le t) &= \\ell (t) \\\\\n  \\ell (t) &= \\alpha \\; X(t) + (1 - \\alpha) \\; \\ell (t - 1)\n\\end{align}\n\\qquad(12.107)\\]\nwhere \\(\\ell (t)\\) predicts \\(X(t + 1)\\) based on \\(\\{ X(s) \\}_{s \\le t}\\).\nFrom Equation 12.103 and Equation 12.106 we see that for \\(u \\ge 1\\), \\(\\breve{X}(t + u \\; | \\; s \\le t)\\) reverts to \\(\\breve{X}(t + 1 \\; | \\; s \\le t)\\). Thus we have\n\\[\n\\begin{align}\n  \\breve{X}(t + u \\; | \\; s \\le t) &= \\ell (t) \\\\\n  \\ell (t) &= \\alpha \\; X(t) + (1 - \\alpha) \\; \\ell (t - 1)\n\\end{align}\n\\qquad(12.108)\\]\nWe model the difference \\(X(t) - \\ell (t - 1)\\) as a Gaussian white noise process, \\(W(t)\\). 47\n\\[\n\\begin{align}\n  W(t) &= X(t) - \\ell (t - 1) \\\\\n  \\\\\n  & \\text{so that } \\\\\n  \\\\\n  X(t) &= \\ell(t-1) + W(t) \\\\\n  \\\\\n  & \\text{with } W(\\cdot) \\sim \\text{iid } N(0, \\sigma_W^2)\n\\end{align}\n\\qquad(12.109)\\]\nThe white noise term \\(W(t)\\) is also called an error term, and this state space model is said to have additive error. Equivalently, the level component \\(\\ell (\\cdot)\\) is said to be additive.\n\n\n12.8.4.3 Trend component\nWe now add a trend component, \\(\\mathit{b} (t)\\), to this system as follows.\n\\[\n\\begin{align}\n  \\breve{X}(t + u \\; | \\; s \\le t) &= \\ell (t) \\; + \\; \\mathit{b}(t) \\times u \\\\\n  \\ell (t) &= \\alpha \\; X(t) + (1 - \\alpha) \\; \\left ( \\ell(t - 1) + \\mathit{b}(t - 1) \\right ) \\\\\n  \\mathit{b} (t) &= \\beta \\; \\left ( \\ell(t) - \\ell(t - 1) \\right ) \\; + \\; (1 - \\beta) \\; \\mathit{b}(t - 1) \\\\\n  \\\\\n  &\\text{where } \\alpha, \\beta \\in (0, 1)\n\\end{align}\n\\qquad(12.110)\\]\nThis forecast system is known as Holt’s trend method, published decades ago. Since then a modification has been introduced to dampen the effect of the trend term \\(\\mathit{b}(t)\\) on long-term forecasts, via damping coefficient \\(\\phi \\in (0, 1)\\), as follows.\n\\[\n\\begin{align}\n  \\breve{X}(u + t \\; | \\; s \\le t) &= \\ell (t) \\; + \\; \\mathit{b}(t) \\times \\sum_{\\nu = 1}^u \\phi^\\nu \\\\\n  \\ell (t) &= \\alpha \\; X(t) + (1 - \\alpha) \\; \\left ( \\ell(t - 1) + \\phi \\;  \\mathit{b}(t - 1) \\right ) \\\\\n  \\mathit{b} (t) &= \\beta \\; \\left ( \\ell(t) - \\ell(t - 1) \\right ) \\; + \\; (1 - \\beta) \\; \\phi \\;  \\mathit{b}(t - 1) \\\\\n  \\\\\n  &\\text{where } \\alpha, \\beta, \\phi \\in (0, 1)\n\\end{align}\n\\qquad(12.111)\\]\n\n\n12.8.4.4 Seasonal component\nStarting with Equation 12.110, we now add a seasonal component, \\(\\mathscr{s}(u + t \\; | \\; s \\le t)\\), where a fixed positive integer \\(\\sigma\\) denotes the number of successive time indices comprising a single season. For \\(t, (u + t) \\in \\tau_T\\) we fit \\(\\mathscr{s}(u + t \\; | \\; s \\le t)\\) recursively. Then for \\((u + t) &gt; t_f\\) we refer back to the most recent fitted value \\(\\mathscr{s} (\\tilde{t})\\) such that \\(\\tilde{t} \\equiv (u + t) \\bmod \\sigma\\).\n\\[\n\\begin{align}\n  \\breve{X}(u + t \\; | \\; s \\le t) &= \\ell (t) \\; + \\; \\mathit{b}(t) \\times u \\; + \\; \\mathscr{s}(u + t \\; | \\; s \\le t) \\\\\n  \\ell (t) &= \\alpha \\; (X(t) - \\mathscr{s}(t - \\sigma)) + (1 - \\alpha) \\; \\left ( \\ell(t - 1) + \\mathit{b}(t - 1) \\right ) \\\\\n  \\mathit{b} (t) &= \\beta \\; \\left ( \\ell(t) - \\ell(t - 1) \\right ) \\; + \\; (1 - \\beta) \\; \\mathit{b}(t - 1) \\\\\n  \\mathscr{s} (t) &= \\gamma \\left ( X(t) - \\ell (t-1) - \\mathit{b} (t-1) \\right ) \\; + \\; (1 - \\gamma) \\mathscr{s}(t - \\sigma) \\\\\n  \\\\\n  &\\text{where } \\alpha, \\beta, \\gamma \\in (0, 1)\\\\\n  \\\\\n  &\\text{and where } \\\\\n  & \\mathscr{s}(u + t \\; | \\; s \\le t) = \\mathscr{s} (\\tilde{t}), \\text{ with} \\\\\n  & \\tilde{t} \\equiv (u + t) \\bmod \\sigma, \\text{ and with}  \\\\\n  & \\tilde{t} \\in (t - \\sigma, \\; t]\n\\end{align}\n\\qquad(12.112)\\]\n\n\n12.8.4.5 ETS model designation\nEach ETS model can be categorized according to the status of model components level, trend, and season.48\nThe status of a component is one of the following:\n\n\\(\\mathbf{A}\\): used as an additive component\n\\(\\mathbf{Ad}\\): damped or otherwise adjusted\n\\(\\mathbf{M}\\): used as a multiplicative component\n\\(\\mathbf{N}\\): not used\n\nHere are some examples of this scheme.\n\nETS(A, N, N): simple exponential smoothing\nETS(A, A, N): Holt’s linear trend method\nETS(A, Ad, N): damped linear trend method\nETS(A, A, A): Holt-Winters with additive seasonality\nETS(A, A, M): Holt-Winters with multiplicative seasonality\n\nNote: (hyndman2021forecasting?) warn that certain combinations may lead to numerical instability when estimating parameters. For example they recommend using \\(ETS(M, *, M)\\) rather than \\(ETS(A, *, M)\\). Much of this guidance is contained in the documentation (and curated options) for function fable:ETS().\n\n\n\n12.8.5 Seasonal ARIMA Models 49\nBoth societies and natural phenomena follow cycles determined by the earth’s daily rotation and annual revolution about the sun. Other cycles also arise naturally or through societal conventions. In modeling a random process one can account for these cycles in several ways. In this section we introduce methods and notation for incorporating seasonality into ARIMA models.\n\n12.8.5.1 Pure Seasonal ARMA model\nAs in Section 12.8.4 we denote by \\(\\sigma &gt; 1\\) the number of consecutive time indices that comprise one season (or cycle). The phase of time index \\(t\\) in this cycle is \\(t \\bmod \\sigma\\).\n\\(ARMA(P, Q)_\\sigma\\) (with capital letters and a \\(\\sigma\\) subscript) denotes the following form of pure seasonal ARMA model.\n\\[\n\\begin{align}\n  \\Phi ( \\mathcal{B}^\\sigma ) \\; X(t) &= \\Theta ( \\mathcal{B}^\\sigma ) \\; W(t) \\\\\n  \\\\\n  & \\text{where } \\\\\n  \\\\\n  \\Phi (z) &= 1 - \\sum_{j = 1}^P \\Phi_j z^j \\\\\n  \\Theta (z) &= 1 + \\sum_{k = 1}^Q \\Theta_k z^k\n\\end{align}\n\\qquad(12.113)\\]\nARMA polynomial requirements: As with a standard ARMA model, this pure seasonal model has a causal form (in which \\(X(t)\\) is expressed in terms of current and past white noise terms \\(W(\\cdot)\\)) if and only if \\(\\Phi (\\mathcal{B})^{-1}\\) exists. It has an inverted form (in which \\(W(t)\\) is expressed in terms of current and past process terms \\(X(\\cdot)\\)) if and only if \\(\\Theta (\\mathcal{B})^{-1}\\) exists. The existence of these multiplicative inverses is equivalent to the roots of the respective polynomials having magnitudes greater than unity, which we shall assume. To ensure the model is expressed as simply as possible, we also require that the two polynomials have no common factors, that is, have no roots in common.\n\n\n12.8.5.2 Mixed Seasonal ARMA Model\nWe can combine the seasonal and nonseasonal operators into the following multiplicative seasonal auto-regressive moving average model, denoted by \\(ARMA(p, q) \\times (P, Q)_\\sigma\\)\n\\[\n\\begin{align}\n  \\Phi ( \\mathcal{B}^\\sigma ) \\; \\phi ( \\mathcal{B} ) \\; X(t)\n  &= \\Theta ( \\mathcal{B}^\\sigma ) \\; \\theta ( \\mathcal{B} ) \\; W(t) \\\\\n  \\\\\n  & \\text{where } \\\\\n  \\\\\n  \\Phi (z) &= 1 - \\sum_{j = 1}^P \\Phi_j z^j \\\\\n  \\Theta (z) &= 1 + \\sum_{k = 1}^Q \\Theta_k z^k \\\\\n  \\\\\n  \\phi (z) &= 1 - \\sum_{j = 1}^p \\phi_j z^j \\\\\n  \\theta (z) &= 1 + \\sum_{k = 1}^q \\theta_k z^k\n\\end{align}\n\\qquad(12.114)\\]\nMultiplicative seasonal ARMA polynomial requirements: We require that the seasonal and non-seasonal components separately meet the ARMA requirements described above. Now, the seasonal component acts on \\(\\mathcal{B}^\\sigma\\), with \\(\\sigma &gt; 1\\), whereas the non-seasonal component acts on \\(\\mathcal{B}\\). We want to ensure that the polynomial \\(\\Phi (z^\\sigma) \\times \\phi (z)\\) has no roots in common with the polynomial \\(\\Theta (z^\\sigma) \\times \\theta (z)\\). Therefore the additional requirements here are that: (1) \\(\\{ \\Phi (z^\\sigma), \\theta (z) \\}\\) have no roots in common; and (2) \\(\\{ \\Theta (z^\\sigma), \\phi (z) \\}\\) also have no roots in common.\n\n\n12.8.5.3 SARIMA Model\nWe now add seasonal and non-seasonal differencing to the previous model to define the following multiplicative seasonal auto-regressive integrated moving average model (also called SARIMA), denoted by \\(ARIMA(p, d, q) \\times (P, D, Q)_\\sigma\\)\n\\[\n\\begin{align}\n  \\Phi ( \\mathcal{B}^\\sigma ) \\; \\phi ( \\mathcal{B} ) \\; \\nabla_\\sigma^D \\; \\nabla^d \\; X(t)\n  &= \\Theta ( \\mathcal{B}^\\sigma ) \\; \\theta ( \\mathcal{B} ) \\; W(t) \\\\\n  \\\\\n  & \\text{where } \\\\\n  \\\\\n  \\Phi (z) &= 1 - \\sum_{j = 1}^P \\Phi_j z^j \\\\\n  \\Theta (z) &= 1 + \\sum_{k = 1}^Q \\Theta_k z^k \\\\\n  \\nabla_\\sigma^D &= (\\mathcal{I} - \\mathcal{B}^\\sigma)^D \\\\\n  \\\\\n  \\phi (z) &= 1 - \\sum_{j = 1}^p \\phi_j z^j \\\\\n  \\theta (z) &= 1 + \\sum_{k = 1}^q \\theta_k z^k \\\\\n  \\nabla^d &= (\\mathcal{I} - \\mathcal{B})^d\n\\end{align}\n\\qquad(12.115)\\]\nThe polynomial requirements remain unchanged from the seasonal ARMA case described above.\n\n\n\n12.8.6 CO2 Example 50\nFigure 12.15 shows the atmospheric concentration of carbon dioxide \\((CO_2)\\) in parts per million (ppm). These are monthly averages from March 1958 to March 2023 provided by the Mauna Loa Observatory. The data are available in R as the module asta::cardox.\nFigure 12.6 distinguishes the trend from the seasonality of the same data.\n\n\n\n\n\n\n\n\nFigure 12.6: CO2 per month within each year\n\n\n\n\n\nTo build a SARIMA model of the data we first remove (or at least reduce) trend and seasonality by applying differencing operators \\(\\nabla\\) and \\(\\nabla_{12}\\). Figure 12.7 shows the residuals, \\(R(t) = \\nabla \\nabla_{12} X(t)\\), from this double differencing.\n\n\n\n\n\n\n\n\nFigure 12.7: Residuals from applying trend and seasonal differencing\n\n\n\n\n\nUsing R function astsa::sarima() the authors arrive at an \\(ARMA(1, 1) \\times ARMA(0, 1)_{12}\\) model of the residuals \\(R(t)\\). For the process \\(X(t)\\) this is an \\(ARIMA(1, 1, 1) \\times ARIMA(0, 1, 1)_{12}\\) model. A five-year forecast is shown below, using R function astsa::sarima.for().\n\n\n\n\n\n\n\n\nFigure 12.8: CO2 5-year forecast based on ARIMA(1, 1, 1) * (0, 1, 1)_12",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#closing-remarks",
    "href": "ts-forecast.html#closing-remarks",
    "title": "12  Time Series Forecasting",
    "section": "12.9 Closing Remarks",
    "text": "12.9 Closing Remarks\nThis technical note introduces families and examples of models used to forecast time series data. The presentation is based on (shumway2025?) along with the supporting R package astsa (astsa?).51\nThis introduction sets aside for another time the building and evaluation of time series models. Here are some online resources for those interested.\n\nRegression and Time Series Primer gives a more detailed introduction to the topics presented here.\nFun with asta introduces R package astsa.\nForecasting: Principles and Practice (3e) is another highly regarded textbook available online.\nBerkeley Intro to Time Series contains excellent lecture notes.\nCRAN, from the Comprehensive R Archive Network, describes time series packages in R.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#a-glossary-of-math-symbols",
    "href": "ts-forecast.html#a-glossary-of-math-symbols",
    "title": "12  Time Series Forecasting",
    "section": "12.10 A: Glossary of Math Symbols",
    "text": "12.10 A: Glossary of Math Symbols\n\\[\n\\begin{align}\n  {a_{\\bullet, \\bullet} (u)}\n    \\quad & \\quad \\text{matrix of filter coefficients at time-shift } u \\\\\n  \\\\\n  \\mathcal{B}\n     \\quad & \\quad \\text{back-shift operator } \\\\\n  \\\\\n  \\nabla\n     \\quad & \\quad \\text{difference operator } \\mathcal{I - B} \\\\\n  \\\\\n  \\nabla_\\sigma\n     \\quad & \\quad \\text{seasonal difference operator } \\mathcal{I - B}^\\sigma \\\\\n  \\\\\n  \\Delta_u\n     \\quad & \\quad \\{ 1, 2, \\ldots, u-1 \\} \\text{ for } u \\ge 2 \\\\\n  \\\\\n  \\phi(\\cdot)\n     \\quad & \\quad \\text{AR polynomial, } 1 - \\sum_{\\nu = 1}^p \\phi_\\nu \\; z^\\nu \\\\\n  \\\\\n  \\Gamma\n     \\quad & \\quad \\text{auto-covariance matrix, } \\Gamma [j, \\nu] = \\gamma_X (\\nu - j) \\text{, for } j, \\nu \\in \\nu_T \\\\\n  \\\\\n  \\gamma_{\\bullet, \\bullet} (u)\n     \\quad & \\quad \\text{auto-covariance matrix at time-shift } u \\\\\n  \\\\\n  \\mathcal{I}\n     \\quad & \\quad \\text{identity operator } \\\\\n  \\\\\n  MSE\n     \\quad & \\quad \\text{mean squared error } \\\\\n  \\\\\n  m_\\bullet (t)\n     \\quad & \\quad E \\{ X_\\bullet (t) \\} \\text{, expected value of } X_\\bullet (t) \\\\\n  \\\\\n  \\mu_X\n     \\quad & \\quad E\\{ X (t_0) \\} \\text{, expected value of } X (t_0) \\\\\n  \\\\   \n  \\hat{\\mu}_X\n     \\quad & \\quad \\text{sample mean, } S_{X, T} / T \\\\\n  \\\\   \n  \\mathcal{N}(\\cdot)\n     \\quad & \\quad \\text{a univariate, stationary, Gaussian process having mean 0} \\\\\n  \\\\   \n  \\tilde{\\mathcal{N}} (u + t_f \\; | \\; \\wp (t_f))\n     \\quad & \\quad E \\left \\{ \\mathcal{N} (u + t_f) \\; | \\; \\mathcal{N} (s), \\; s \\le t_f \\right \\} \\\\\n  \\\\\n  \\nu_T\n     \\quad & \\quad \\{ 0, 1, \\dots, T-1 \\} \\\\\n  \\\\\n  \\wp (t_f)\n     \\quad & \\quad \\{ s \\in \\mathbb{Z} \\; | \\; s \\le t_f \\} \\\\\n  \\\\\n  \\rho_{\\bullet, \\bullet} (u)\n     \\quad & \\quad \\text{autocorrelation matrix at time-shift } u \\\\\n  \\\\\n  \\rho (u | u)\n     \\quad & \\quad corr(X(t+u) - \\hat{X}(t+u \\; | \\; - \\Delta_u), \\; X(t) - \\hat{X}(t \\; | \\; \\Delta_u)) \\\\\n  \\\\\n  S_{X, T}\n     \\quad & \\quad \\text{the sample sum of T observations of } X (\\cdot) \\\\\n  \\\\\n  \\sigma_X^2\n     \\quad & \\quad Var \\{ X (t_0) \\} \\text{, variance of } X (t_0) \\\\\n  \\\\\n  T\n     \\quad & \\quad \\text{number of time series observations } \\\\\n  \\\\\n  t_0\n     \\quad & \\quad \\text{initial time index of time series observations } \\\\\n  \\\\\n  t_f\n     \\quad & \\quad \\text{final time index of time series observations } \\\\\n  \\\\\n  \\tau_T\n     \\quad & \\quad t_0 + \\{ 0, 1, \\ldots, T-1  \\} \\\\\n  \\\\\n  \\theta(\\cdot)\n     \\quad & \\quad \\text{MA polynomial, } 1 + \\sum_{\\nu = 1}^q \\theta_\\nu \\; z^\\nu \\\\\n  \\\\\n  W(\\cdot)\n     \\quad & \\quad \\text{white noise process, } W(\\cdot) \\sim wn(0, \\sigma_W^2)  \\\\\n  \\\\\n  X_\\bullet (t)\n     \\quad & \\quad (X_1 (t), \\ldots, X_d (t)) \\text{, a multivariate random process } \\\\\n  \\\\\n  \\hat{X}(t \\; | \\; \\Delta_u)\n     \\quad & \\quad \\text{approximates } X(t) \\text{ as affine function of } \\{ X(t + \\nu) \\}_{\\nu \\in \\Delta_u} \\\\\n  \\\\\n  \\hat{X} (u + t_f \\; | \\; \\wp (t_f))\n     \\quad & \\quad \\text{predicts } X(u + t_f) \\text{ as affine function of } \\{ X(t) \\}_{t \\in \\wp (t_f)} \\\\\n  \\\\\n  \\breve{X} (t \\; | \\; \\tau_T)\n     \\quad & \\quad \\text{trucnated predictor of } X(t) \\text{ based on } \\{ X(t) \\}_{t \\in \\tau_T}\n\\end{align}\n\\]",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#b-data-examples",
    "href": "ts-forecast.html#b-data-examples",
    "title": "12  Time Series Forecasting",
    "section": "12.11 B: Data Examples",
    "text": "12.11 B: Data Examples\n\n12.11.1 Hare and Lynx Populations\nOne of the classic studies of predator–prey interactions is based on the record of lynx (astsa::Lynx) and snowshoe hare (astsa::Hare) pelts purchased by the Hudson’s Bay Company of Canada from 1845 to 1935. Assuming pelt purchases are proportional to animals in the wild, the data are an indirect measure of predation.\nThese predator–prey interactions often lead to cyclical patterns of predator and prey abundance. The units of the data shown in Figure 12.9 are thousands of pelts per year.\n\n\n\n\n\n\n\n\nFigure 12.9: Hare and lynx: purchased pelts\n\n\n\n\n\n\n\n12.11.2 JJ Quarterly Earnings\nFigure 12.10 below shows Johnson & Johnson quarterly earnings per share in US dollars from 1960 through 1980 (astsa::jj). The bottom panel shows the same data on a \\(\\log_e\\) scale. Superimposed on the upward trend is an annual pattern, including a sharp rise to first quarter earnings from those of the previous quarter.\n\n\n\n\n\n\n\n\nFigure 12.10: J&J: earnings per share at linear and log scales\n\n\n\n\n\n\n\n12.11.3 Global Temperatures\nFigure 12.11 below shows the time series gtemp_land and gtemp_ocean from R package astsa. The series are annual temperature deviations (in ◦C) from averages for the period 1991-2020. The temperatures are based on averages over the Earth’s land area and over the part of the ocean that is free of ice at all times (open ocean). The time period is from 1850 to 2023. Note that the trend is not linear, with periods of leveling off followed by sharp increases.\n\n\n\n\n\n\n\n\nFigure 12.11: Deviations in global surface temperatures\n\n\n\n\n\n\n\n12.11.4 Dow Jones Industrial Average\nFigure 12.12 shows the trading day closings and returns (percent change)52 of the Dow Jones Industrial Average (DJIA, astsa::djia) from 2006 to 2016. It is easy to spot the financial crisis of 2008.\n\n\n\n\n\n\n\n\nFigure 12.12: Dow Jones Industrial Average\n\n\n\n\n\n\n\n12.11.5 Cardiovascular Mortality in Los Angeles\nFigure 12.13 shows data from a study (shumway1988?) of the possible effects of temperature and pollution on weekly cardiovascular mortality in Los Angeles County. Note the strong seasonal components in all of the series corresponding to winter–summer variations and the downward trend in the cardiovascular mortality over the 10-year period. The data are available as R module astsa::cmort.\n\n\n\n\n\n\n\n\nFigure 12.13: Weekly levels of mortality, temperature, and particulates in Los Angeles\n\n\n\n\n\n\n\n12.11.6 El Niño and Fish Population\nFigure 12.14 shows the Southern Oscillation Index (SOI, astsa::soi) and associated Recruitment (astsa::rec), an index of the number of young fish entering the cohort available for commercial fishing. Both series consist of 453 monthly values ranging over the years 1950–1987.\nThe two time series show two types of oscillation: an annual cycle (warm in the summer, cool in the winter), and a slower cycle that seems to repeat about every 4 years.\n\n\n\n\n\n\n\n\nFigure 12.14: Southern Oscillation Index and fish recruitment\n\n\n\n\n\n\n\n12.11.7 CO2, Mauna Loa Observatory\nThe concentration of \\(CO_2\\) in the atmosphere is a key indicator of global warming, and in recent years has reached unprecedented levels. In March 2015, the average of all of the global measuring sites showed a concentration above 400 parts per million (ppm). The observatory at Mauna Loa has recorded monthly \\(CO_2\\) concentrations consistently above 400 ppm since late 2015.\nFigure 12.15 shows the monthly Mauna Loa readings, \\(X(t)\\) (asta::cardox), from March 1958 to March 2023 at the Mauna Loa Observatory. The trend and seasonal persistence are evident in the plot.\n\n\n\n\n\n\n\n\nFigure 12.15: Monthly mean carbon dioxide (in ppm) measured at Mauna Loa Observatory, Hawaii.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#references",
    "href": "ts-forecast.html#references",
    "title": "12  Time Series Forecasting",
    "section": "12.12 References",
    "text": "12.12 References",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-forecast.html#footnotes",
    "href": "ts-forecast.html#footnotes",
    "title": "12  Time Series Forecasting",
    "section": "",
    "text": "We assume the components of \\(X_\\bullet(t)\\) to be real-valued, unless stated otherwise.↩︎\nFor a univariate time series \\(X(t)\\) we denote distributional parameters either omitting a subscript or else using the subscript \\(X\\).↩︎\nThe assumption of second-order (or wide-sense) stationarity suffices for most time series models used in practice, but some cases may call for the assumption of strict stationarity. This means that for any finite set of times \\((t_1, \\ldots, t_K)\\) and any time-shift \\(s\\), the joint probability distribution of \\((X_\\bullet (t_1), \\ldots, X_\\bullet (t_K))\\) is identical to that of \\((X_\\bullet (t_1 - s), \\ldots, X_\\bullet (t_K - s))\\).↩︎\nSee (shumway2025?).↩︎\nThat is, we assume that \\(\\mathcal{N}(t)\\) is a stationary, mean-zero, autocorrelated process such that for any set of distinct time indices \\(t_1, \\ldots, t_K\\) the joint probability distribtuion of \\((\\mathcal{N}(t_1), \\ldots, \\mathcal{N}(t_K)\\) is multivariate normal.↩︎\nNormality allows us to define approximations as conditional expectations. To define PACF more generally, replace conditional expectation with the affine function that minimizes mean squared approximation error.↩︎\nRecall that for a univariate process the auto-covariance and autocorrelation functions are even, e.g., \\(\\rho (-u) = \\rho (u)\\). The PACF of a univariate process is also defined to be an even function.↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nThe filters described here are called “auto-regressive” or “finite impulse response”. Filtering is a large subject only touched on here.↩︎\nSee (shumway2025?).↩︎\nThe authors consider various forms for the function \\(f(t)\\): a linear function, a quadratic function, or a locally smoothed version of \\(X_1(t)\\).↩︎\nAn ARMA model is said to be causal if the modeled process \\(X(t)\\) can be expressed as a function of current and past white noise terms, \\(\\{ W(t-u) \\}\\) for \\(u \\ge 0\\). See (shumway2025?).↩︎\nA model is said to be invertible if the model’s white noise process \\(W(t)\\) can be expressed as a function of current and past terms of the modeled process, \\(\\{ X(t-u) \\}\\) for \\(u \\ge 0\\). See (shumway2025?).↩︎\nRequiring the roots of a polynomial \\(p(\\cdot)\\) to be greater than unity in magnitude ensures that \\(p(z)^{-1}\\) is finite and analytic for \\(|z| \\le 1\\) and thus has a power series expansion whose radius of convergence is the minimum magnitude of the roots of \\(p(\\cdot)\\). As a consequence the coefficients of that power series are absolutely summable.↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nTransform the second equation into the first as follows. For all time indices \\(\\nu\\) set \\(U(\\nu) = 5 V(\\nu - 1)\\), and then reverse time by setting \\(Y'(\\nu) = Y(- \\nu)\\) and \\(U'(\\nu) = U(- \\nu)\\).↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nThe rational function \\(\\theta(z) \\; \\div \\phi(z)\\) can be expressed as a power series, namely the power series of \\(\\phi(z)^{-1}\\) multiplied by the polynomial \\(\\theta(z)\\).↩︎\nSee (shumway2025?).↩︎\nThis inversion requires each root of polynomial \\(\\theta(\\cdot)\\) to have magnitude greater than unity.↩︎\n(shumway2025?) introduce the \\(\\Gamma\\) matrix.↩︎\nBy “degenerate process” we mean a process \\(X(\\cdot)\\) for which there is a filter \\(a(u)\\), equal to zero for all but finitely many lags \\(u\\), that annihilates \\(X(\\cdot)\\): \\(a * (X - \\mu_x) = 0\\).↩︎\nAny two time indices within the \\(T\\) observations can differ by no more than \\(T-1\\). Yet the needed estimates include \\(\\gamma_X(T + u - 1 - j)\\), with \\(u \\ge 1\\) and \\(0 \\le j &lt; T\\). For \\(u = 1\\) and \\(j = 0\\) we have \\(\\gamma_X(T)\\), which exceeds the limits of direct estimation.↩︎\n(shumway2025?)↩︎\nWhen attempting to predict future values \\(\\mathcal{N} (u + t_f)\\) from data values indexed by \\(t \\in (t_0, \\ldots, t_f)\\) the index symbol \\(h\\) is often used in place of the generic indexing difference \\(u\\) to denote forecast horizon.↩︎\nRecall that the formula given here for the mean squared error of prediction assumes that the coefficients of the model and the parameters of the process are known. In applications these quantities must be estimated, which increases mean squared prediction error.↩︎\n(shumway2025?)↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nThe formula is obtained by expanding \\(\\frac{1 + \\theta_1 z}{1 - \\phi_1 z}\\) in a power series.↩︎\nThe simulation code is based on a similar example in Fun with asta.↩︎\nSee (shumway2025?).↩︎\nFrom covariance calculations, \\(\\nabla \\; X(\\cdot)\\) is an \\(AR(1)\\) process.↩︎\nSee (shumway2025?).↩︎\n\\(\\hat{X}(t \\; | \\; \\wp (t-1))\\) is the orthogonal projection of \\(X(t)\\) onto the space spanned by \\(\\{ X(s), s &lt; t \\}\\). See (shumway2025?).↩︎\nIn the case \\(u = 1\\) the sum is over an empty set of indices \\(\\nu\\) such that \\(\\nu \\ge 1\\) and \\(\\nu \\le 0\\). We define such empty sums to be zero.↩︎\nSee (hyndman2021forecasting?) [chapter 8] and (ryantibs2023timeseries?).↩︎\nSee (holt1957forecasting?) and (winters1960forecasting?).↩︎\nEquation 12.109 is the state-space version of Equation 12.100.↩︎\nSee (hyndman2002state?).↩︎\nSee (shumway2025?).↩︎\nSee (shumway2025?).↩︎\nAnother textbook, (brillinger2001?), treats a time series as a function of time, denoted \\(X(t)\\), which is the practice followed in this note.↩︎\nThe return is here calculated as \\(log_e\\) of the ratio: current day’s closing price divided by that of the preceding day. The median value is about 6 basis points.↩︎",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Time Series Forecasting</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html",
    "href": "ts-fourier.html",
    "title": "13  Time Series Spectrum Analysis",
    "section": "",
    "text": "13.1 Introduction\nThe analysis of time series has proved to have both pragmatic and scientific value. For time series that track business activity or broader economic activity, the ability to forecast future values with some degree of reliability has evident practical value. The potential of such forecasting has influenced the development of time domain models, notably auto-regressive moving average (ARMA) models.\nOn the other hand, natural processes often exhibit periodic behavior, and have given rise to the Fourier decomposition of such time series, that is, the representation of the process as a sum of sine waves over a range of frequencies.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#sec-intro",
    "href": "ts-fourier.html#sec-intro",
    "title": "13  Time Series Spectrum Analysis",
    "section": "",
    "text": "13.1.1 Sunspots\nAn early example of such periodic behavior is shown in Figure 13.8, the time series of monthly sunspot numbers. Sunspots have been observed for millennia, but systematic scientific observations began in the 1600’s.1\nSunspots are a key indicator of the solar magnetic activity cycle in which the sun’s magnetic polarity reverses every 11 years or so. At the peak of a cycle the sun emits bursts of charged particles (solar flares) across the solar system that affect the Earth. Consequently, understanding the solar cycle has both scientific and practical value.\nGaps between peak sunspot numbers vary, so a simple sinusoidal wave (or other function) having a fixed period does not fit the data well. On the other hand, the variability of the data is clarified by Fourier decomposition and subsequent grouping of fitted sine waves within successive bands of frequencies. This approach is discussed in Section 13.2.\n\n\n13.1.2 Average daily temperatures\nFigure 13.9 shows average daily temperatures for Honolulu and New York City over a 25-year period (1995-2020). The overall average temperature in NYC is about 56 (F), well below that of Honolulu (77). Also note that the standard deviation of NYC temperatures,17.1 (F), is well above that of Honolulu (3.4). So Honolulu temperatures are higher than those of NYC and fluctuate less (as visitors to those two cities may testify). Less clear, however, is the degree to which seasonal variation accounts for the overall variation in each city. This is a question that can be addressed by Fourier decomposition.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#sec-math-fmwk",
    "href": "ts-fourier.html#sec-math-fmwk",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.2 Mathematical Framework",
    "text": "13.2 Mathematical Framework\n\n13.2.1 Data derived from a random process\nStatistical data usually take the form of one or more data frames in which each row represents an observation and each column represents a variable of interest. For example, here are the first few values of the temperature data shown in Figure 13.9.\n\n\n\nTable 13.1: Temperatures (F) from Honolulu and New York City, 1995-2020\n\n\n\n# A tibble: 9,265 × 3\n  date         HNL   NYC\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 1995-01-01  71.3  44  \n2 1995-01-02  72.5  41.8\n3 1995-01-03  73.2  28.1\n# ℹ 9,262 more rows\n\n\n\n\nThe distinction of time series analysis is that observations are indexed by time2, and are not assumed to be statistically independent. Instead, time series data are typically modeled as a realization of a random process of the following form.3\n\\[\n\\begin{align}\n  X_\\bullet (t) = (X_1 (t), \\ldots, X_d (t)) \\in \\mathbb{R}^d\n\\end{align}\n\\qquad(13.1)\\]\nIf the number of data columns is two or greater \\((d \\ge 2)\\), the process is said to be multivariate. Otherwise, if \\(d = 1\\) the process is said to be univariate, and the notation is simplified to \\(X (t)\\).4\nIn our example there are two data columns (HNL, NYC), so that \\(d = 2\\), the unit of time is one day, and the sampling frequency is once per day.\nThe random process is idealized to span all time \\((t \\in \\mathbb{Z})\\), but the data of course span some finite period, \\(\\tau_T\\), of length \\(T\\).\n\\[\n\\begin{align}\n  \\tau_T &=  \\{ t_0, \\; t_0 + 1, \\ldots, t_f \\} \\\\\n  t_0 &=  \\text{ initial data index} = \\min \\tau_T \\\\\n  t_f &=  \\text{ final data index} = \\max \\tau_T \\\\\n  T &=  \\text{ number of observatons } = t_f - t_0 + 1 \\\\\n  \\nu_T &= \\tau_T - t_0 =  \\{ 0, 1, \\ldots, T-1 \\}\n\\end{align}\n\\qquad(13.2)\\]\nThe expected value of the random process may be modeled by various functions of time: a constant, a linear trend, a seasonal component (periodic function), etc.\n\\[\n\\begin{align}\n  m_\\bullet (t) &= E \\{ X_\\bullet (t) \\}\n\\end{align}\n\\qquad(13.3)\\]\n\n\n13.2.2 Stationarity\nIn an additive model (which is most common), the centered random process, \\(X_\\bullet (t) - m_\\bullet (t)\\) , is assumed to be second-order stationary.5 That is, if we shift \\(X_\\bullet (\\cdot)\\) by any number of time units \\(s\\) to obtain a new process \\(Y_\\bullet (\\cdot)\\), we assume that the respective covariance structures of \\(X_\\bullet (\\cdot)\\) and \\(Y_\\bullet (\\cdot)\\) are the same.\n\\[\n\\begin{align}\n  Y_\\bullet (t) &= \\mathcal{B}^s \\{ X_\\bullet (\\cdot) \\} (t)  \\\\\n  &= X_\\bullet (t - s)\n\\end{align}\n\\qquad(13.4)\\]\nHere \\(\\mathcal{B}\\) denotes the back-shift operator that shifts time by one unit, so that \\(\\mathcal{B}^s\\) (that is, \\(s\\) repeated applications of \\(\\mathcal{B}\\)) shifts time by \\(s\\) units. Then second-order stationarity can be expressed as follows.\n\\[\n\\begin{align}\n  Cov \\{ X_a (t + u), X_b (t) \\}\n  &= Cov \\{ Y_a (t + u), Y_b (t) \\} \\\\\n  &= Cov \\{ X_a (t + u - s), X_b (t - s) \\} \\\\\n  \\\\\n  & \\text{for all } s \\in \\mathbb{Z} \\text{ and } a, b \\in \\{ 1, \\ldots, d \\}\n\\end{align}\n\\]\n\n\n13.2.3 Auto-covariance function\nSetting \\(s = t\\) in the last expression we have\n\\[\n\\begin{align}\n  Cov \\{ X_a (t + u), X_b (t) \\}\n  &= Cov \\{ X_a (u), X_b (0) \\}\n\\end{align}\n\\qquad(13.5)\\]\nConsequently, second-order stationarity enables one to define and estimate the following auto-covariance function.\n\\[\n\\begin{align}\n  \\gamma_{\\bullet, \\bullet} (u) &= \\left \\{ \\gamma_{a, b} (u) \\right \\}_{a, b = 1}^d \\\\ \\\\\n  \\text{where} \\\\ \\\\\n  \\gamma_{a, b} (u) &= Cov \\{ X_a (t + u), X_b (t) \\} \\\\\n  &= Cov \\{ X_a (u), X_b (0) \\}\n\\end{align}\n\\qquad(13.6)\\]\nAlthough \\(\\gamma_{\\bullet, \\bullet} (u)\\) is a symmetric matrix at \\(u = 0\\), it is not generally symmetric at \\(u \\ne 0\\). The symmetry relation we do have is that matrix \\(\\gamma_{\\bullet, \\bullet} (-u)\\) is the transpose of matrix \\(\\gamma_{\\bullet, \\bullet} (u)\\).\n\\[\n\\begin{align}\n  \\gamma_{a, b} (-u) &= Cov \\{ X_a (t - u), X_b (t) \\} \\\\\n  &= Cov \\{ X_a (t), X_b (t + u) \\}  \\\\\n  &= Cov \\{ X_b (t + u), X_a (t) \\}   \\\\\n  &= \\gamma_{b, a} (u)\n\\end{align}\n\\qquad(13.7)\\]\nIn particular, \\(\\gamma_{a, a} (\\cdot)\\) is an even function: \\(\\gamma_{a, a} (-u) = \\gamma_{a, a} (u)\\).\nConsequently, the time-reversed process \\(Y_\\bullet (t) = X_\\bullet (-t)\\) has the same auto-covariance structure as the original process, allowing for this matrix transposition.\nThe autocorrelation (ACF) function \\(\\rho_{\\bullet, \\bullet}(\\cdot)\\) is the following scaled version of the auto-covariance function.\n\\[\n\\begin{align}\n\\rho_{\\bullet, \\bullet} (u) &= \\left \\{ \\rho_{a, b} (u) \\right \\}_{a, b = 1}^d \\\\ \\\\\n  \\text{where} \\\\ \\\\\n  \\rho_{a, b} (u) &= corr \\{ X_a (u), X_b (0) \\} \\\\\n  &= \\frac{\\gamma_{a, b} (u)}{\\sqrt{\\gamma_{a, a} (0) \\; \\gamma_{b, b} (0)}}\n\\end{align}\n\\qquad(13.8)\\]\n\n\n13.2.4 Spectrum\nThe term “spectrum” in its modern sense was introduced by Sir Isaac Newton6 7, who used a prism to decompose sunlight into a rainbow of colors (light of different wavelengths or frequencies). The term came to mean the intensity8 (denoted “I”) of light (or other wave phenomena) as a function of wavelength or frequency.\nSine waves, more precisely complex-valued functions of the form\n\\[\n\\begin{align}\n  f(t) = \\rho e^{i \\lambda t + \\phi}\n\\end{align}\n\\qquad(13.9)\\]\nare widely used as the basis of this frequency decomposition, for a function of this form is an eigen-function of the back-shift operator \\(\\mathcal{B}\\).\n\\[\n\\begin{align}\n  \\{ \\; \\mathcal{B}^s f(\\cdot) \\; \\} \\; (t) &= f(t-s) \\\\\n  &= \\rho e^{i \\lambda (t - s) + \\phi} \\\\\n  &= e^{- i \\lambda s} \\; \\rho e^{i \\lambda t + \\phi} \\\\\n  &= e^{- i \\lambda s} \\; f (t)\n\\end{align}\n\\qquad(13.10)\\]\n\n\n13.2.5 Spectrum density functions\nThe spectrum density function, \\(f_{\\bullet, \\bullet} (\\lambda)\\), of the random process \\(X_\\bullet(\\cdot)\\) is defined as the Fourier transform of the auto-covariance function \\(\\gamma_{\\bullet, \\bullet} (u)\\).\n\\[\n\\begin{align}\n  f_{\\bullet, \\bullet} (\\lambda) &= \\sum_{u = -\\infty}^\\infty \\gamma_{\\bullet, \\bullet} (u) \\times e^{- i \\lambda u}\n\\end{align}\n\\qquad(13.11)\\]\nwhere we assume that the elements of \\(\\gamma_{\\bullet, \\bullet} (\\cdot)\\) are absolutely summable.\n\\[\n\\begin{align}\n  \\sum_{u = -\\infty}^\\infty \\left | \\; \\gamma_{a, b} (u) \\; \\right | &&lt; \\infty \\\\\n  &\\text{for } a, b \\in \\{1, \\ldots, d \\}\n\\end{align}\n\\qquad(13.12)\\]\nEquation 13.12 is an example of a “mixing” condition. It implies that \\(X_\\bullet(t)\\) and \\(X_\\bullet(t + u)\\) have vanishingly small correlation as \\(|u| \\rightarrow \\infty\\).\nSetting \\(b = a\\), we obtain \\(f_{a, a} (\\cdot)\\), called the power spectrum of \\(X_a (\\cdot)\\).\nIf \\(b \\ne a\\) then \\(f_{a, b} (\\cdot)\\) is called the cross-spectrum of \\(\\{ X_a (\\cdot), X_b (\\cdot) \\}\\). The coherency of \\(X_a(\\cdot), X_b(\\cdot)\\) at frequency \\(\\lambda\\), denoted \\(R_{a, b} (\\lambda)\\), is the following normalization of the cross-spectrum.\n\\[\n\\begin{align}\n  R_{a, b} (\\lambda) &= \\frac{f_{a, b}(\\lambda)}{\\sqrt{f_{a, a}(\\lambda) f_{b, b}(\\lambda)}}\n\\end{align}\n\\qquad(13.13)\\]\nThe modulus-squared of coherency, \\(| R_{a, b} (\\lambda) |^2\\), is called the coherence of \\(\\{ X_a (\\cdot), X_b (\\cdot) \\}\\) at frequency \\(\\lambda\\). Similar to a squared correlation coefficient, this is a non-negative value no greater than unity. And the radian angle, \\(Arg( R_{a, b} (\\lambda) )\\), is called the phase of \\(\\{ X_a (\\cdot), X_b (\\cdot) \\}\\) at frequency \\(\\lambda\\).\nFrom Equation 13.11 we see that \\(f_{\\bullet, \\bullet} (\\lambda)\\) is periodic with period \\(2 \\pi\\).\n\\[\n\\begin{align}\n  f_{\\bullet, \\bullet} (\\lambda + 2 \\pi) &= f_{\\bullet, \\bullet} (\\lambda)\n\\end{align}\n\\qquad(13.14)\\]\nDue to the symmetries of the auto-covariance function (Equation 13.7), the spectrum density matrix is Hermitian, that is, equal to its own conjugate transpose.\n\\[\n\\begin{align}\n  f_{\\bullet, \\bullet} (\\lambda)^* &= \\sum_{u = -\\infty}^\\infty \\gamma_{\\bullet, \\bullet} (u)^* \\times e^{i \\lambda u} \\\\\n  &= \\sum_{u = -\\infty}^\\infty \\gamma_{\\bullet, \\bullet} (-u) \\times e^{i \\lambda u} \\\\\n  &= \\sum_{\\nu = -\\infty}^\\infty \\gamma_{\\bullet, \\bullet} (\\nu) \\times e^{- i \\lambda \\nu} \\\\\n  &= f_{\\bullet, \\bullet} (\\lambda)\n\\end{align}\n\\qquad(13.15)\\]\nWe also have that matrix \\(f_{\\bullet, \\bullet} (- \\lambda)\\) is the transpose of \\(f_{\\bullet, \\bullet} (\\lambda)\\).\n\\[\n\\begin{align}\n  f_{\\bullet, \\bullet} (- \\lambda) &= \\sum_{u = -\\infty}^\\infty \\gamma_{\\bullet, \\bullet} (u) \\times e^{i \\lambda u} \\\\\n  &= \\sum_{u = -\\infty}^\\infty \\gamma_{\\bullet, \\bullet} (-u)^\\prime \\times e^{i \\lambda u} \\\\\n  &= \\sum_{\\nu = -\\infty}^\\infty \\gamma_{\\bullet, \\bullet} (\\nu)^\\prime \\times e^{- i \\lambda \\nu} \\\\\n  &= f_{\\bullet, \\bullet} (\\lambda)^\\prime\n\\end{align}\n\\qquad(13.16)\\]\nThat is\n\\[\n\\begin{align}\n  f_{a, b} (- \\lambda) &= f_{b, a} (\\lambda)\n\\end{align}\n\\qquad(13.17)\\]\nSetting \\(b = a\\) we see that the power spectrum is an even function.\n\\[\n\\begin{align}\n  f_{a, a} (- \\lambda) &= f_{a, a} (\\lambda)\n\\end{align}\n\\qquad(13.18)\\]\nLastly, Equation 13.11 has the following inverse.\n\\[\n\\begin{align}\n  \\gamma_{\\bullet, \\bullet} (u) &= \\frac{1}{2 \\pi} \\int_{- \\pi}^\\pi f_{\\bullet, \\bullet} (\\lambda) \\times e^{i \\lambda u} \\; d\\lambda\n\\end{align}\n\\qquad(13.19)\\]\nSetting \\(u = 0\\) we see that \\(\\gamma_{\\bullet, \\bullet} (0)\\), the variance-covariance matrix of \\(X_\\bullet (t)\\), is the average value of \\(f_{\\bullet, \\bullet} (\\cdot)\\).\n\\[\n\\begin{align}\n  \\gamma_{\\bullet, \\bullet} (0) &= \\frac{1}{2 \\pi} \\int_{- \\pi}^\\pi f_{\\bullet, \\bullet} (\\lambda) \\; d\\lambda\n\\end{align}\n\\qquad(13.20)\\]\n\n\n13.2.6 Cramér representation\nIt turns out9 that Equation 13.12 implies the following Cramér representation of a centered version of \\(X_\\bullet (\\cdot)\\).\n\\[\n\\begin{align}\n  X_\\bullet (t) - m_\\bullet (t) &= \\int_{0}^{2 \\pi} e^{i \\lambda t} \\; dZ_\\bullet (\\lambda)\n\\end{align}\n\\qquad(13.21)\\]\nwhere \\(Z_\\bullet (\\cdot)\\) is a random complex-valued measure having mean value zero and having orthogonal increments10:\n\\[\n\\begin{align}\n  E \\left \\{ Z_\\bullet ( \\Delta_1 ) \\; Z_\\bullet ( \\Delta_2 )^* \\right \\} &= 0 \\\\ \\\\\n  \\text{whenever } \\\\ \\\\\n  \\Delta_1 \\; \\cap \\; \\Delta_2 &= \\emptyset\n\\end{align}\n\\qquad(13.22)\\]\nThe magnitude of \\(dZ_\\bullet (\\lambda)\\) (which has mean zero) can be measured by the following complex-valued version of the variance-covariance matrix.\n\\[\n\\begin{align}\n  E \\left \\{ dZ_\\bullet ( \\lambda ) \\; dZ_\\bullet ( \\lambda )^* \\right \\} &= f_{\\bullet, \\bullet} (\\lambda) \\; d \\lambda\n\\end{align}\n\\qquad(13.23)\\]\n\n\n13.2.7 Filtering\nSuppose that \\(X(t)\\) is a univariate random process satisfying Equation 13.12 and having mean zero. Let \\(a(\\nu)\\) be a non-random summable sequence, and let \\(Y(\\cdot)\\) denote the convolution of \\(a(\\cdot)\\) with \\(X(\\cdot)\\).\n\\[\n\\begin{align}\n  Y(\\cdot) &= a(\\cdot) * X(\\cdot) \\\\\n  \\\\\n  &\\text{so that } \\\\\n  \\\\\n  Y(t) &= \\sum_{\\nu = - \\infty}^\\infty a(\\nu) \\times X(t - \\nu)\n\\end{align}\n\\qquad(13.24)\\]\nThen \\(Y(\\cdot)\\) is said to be a filtered version of \\(X(\\cdot)\\). One can show11 that \\(Y(\\cdot)\\) has the following Cramér representation.\n\\[\n\\begin{align}\n  Y (t) &= \\int_{0}^{2 \\pi} e^{i \\lambda t} \\; dZ_Y (\\lambda) \\\\\n  &= \\int_{0}^{2 \\pi} e^{i \\lambda t} \\; A(\\lambda) \\; dZ_X (\\lambda)\n\\end{align}\n\\qquad(13.25)\\]\nwhere \\(A(\\lambda)\\) is the Fourier transform of \\(a(\\nu)\\).\n\\[\n\\begin{align}\n  A(\\lambda) &= \\sum_{\\nu = - \\infty}^\\infty a (\\nu) \\times e^{- i \\lambda \\nu}\n\\end{align}\n\\qquad(13.26)\\]\nThe Fourier transform \\(A (\\lambda)\\) is called the transfer function of the filter, and the function \\(a(\\nu)\\) is called the impulse response function of the filter. (If one replaces \\(X (\\cdot)\\) with an “impulse” equal to unity at zero and equal to zero elsewhere one obtains \\(Y(t) = a(t)\\).)\nFrom Equation 13.23 we see that\n\\[\n\\begin{align}\n  f_{Y, Y} (\\lambda) \\; d \\lambda &= E \\left \\{ dZ_Y ( \\lambda ) \\; dZ_Y ( \\lambda )^* \\right \\}  \\\\\n  &= E \\left \\{ A(\\lambda) \\; dZ_X ( \\lambda ) \\; dZ_X ( \\lambda )^* \\; A(\\lambda)^* \\right \\}  \\\\\n  &= \\left | A(\\lambda) \\right |^2 \\; E \\left \\{ dZ_X ( \\lambda ) \\; dZ_X ( \\lambda )^* \\right \\}  \\\\\n  &= \\left | A(\\lambda) \\right |^2 \\; f_{X, X} (\\lambda) \\; d \\lambda \\\\ \\\\\n  \\text{so that } \\\\ \\\\\n  f_{Y, Y} (\\lambda) &= \\left | A(\\lambda) \\right |^2 \\; f_{X, X} (\\lambda)\n\\end{align}\n\\qquad(13.27)\\]\n\n\n13.2.8 White noise\nLet \\(W(\\cdot)\\) denote a univariate “white noise” process, that is, a process having zero mean and having the following auto-covariance function.\n\\[\n\\begin{align}\n  \\gamma_{W, W} (u) &=\n  \\begin{cases}\n    \\sigma_W^2 & \\text{for } u = 0 \\\\\n    0 & \\text{for } u \\ne 0\n  \\end{cases}\n\\end{align}\n\\qquad(13.28)\\]\nWe use the following notation to specify a white noise process.\n\\[\n\\begin{align}\n  W &\\sim wn(0, \\sigma_W^2)\n\\end{align}\n\\qquad(13.29)\\]\nThe power spectrum density function, \\(f_{W, W} (\\lambda)\\) is readily seen to evaluate to the constant \\(\\sigma_W^2\\).\n\\[\n\\begin{align}\n  f_{W, W} (\\lambda) &= \\sum_{u = -\\infty}^\\infty \\gamma_W (u) \\times e^{- i \\lambda u} \\\\\n  &= \\gamma_W (0) \\\\\n  &= \\sigma_W^2\n\\end{align}\n\\qquad(13.30)\\]\n\n\n13.2.9 White noise filtered\nNow suppose that \\(X(\\cdot)\\) can be represented as the following filtered version of \\(W(\\cdot)\\).\n\\[\n\\begin{align}\n  X(t) &= \\sum_{\\nu = 0}^\\infty \\psi_\\nu \\times W(t - \\nu) \\\\ \\\\\n  \\text{where } \\\\ \\\\\n  \\sum_{\\nu = 0}^\\infty | \\; \\psi_\\nu \\; | &&lt; \\infty\n\\end{align}\n\\qquad(13.31)\\]\nFrom Equation 13.27 we see that\n\\[\n\\begin{align}\n  f_{X, X} (\\lambda) &= \\left | \\psi(e^{-i \\lambda}) \\right |^2 \\; \\sigma_W^2\n\\end{align}\n\\qquad(13.32)\\]\nwhere \\(\\psi(\\cdot)\\) denotes the power series having coefficients \\(\\psi (\\nu)\\).\n\\[\n\\begin{align}\n  \\psi (z) &= \\sum_{\\nu = 0}^\\infty \\psi_\\nu \\times z^\\nu \\quad \\text{for } |z| \\le 1\n\\end{align}\n\\qquad(13.33)\\]\n\n\n13.2.10 ARMA models\nTo elaborate, let’s define auto-regressive moving average (ARMA) models to develop such filtered versions of the white noise process \\(W(\\cdot)\\). These models are defined by polynomials \\(\\phi(\\cdot)\\) and \\(\\theta(\\cdot)\\) applied to the back-shift operator \\(\\mathcal{B}\\) as follows. (We set the mean of each process to zero in order to simplify the notation.)\n\\[\n\\begin{align}\n  \\phi(\\mathcal{B}) \\; X(\\cdot) &= \\theta(\\mathcal{B}) \\; W(\\cdot) \\\\ \\\\\n  \\text{with } \\\\ \\\\\n  \\phi(z) &= 1 - \\sum_{j = 1}^p \\phi_j \\; z^j \\\\\n  \\theta(z) &= 1 + \\sum_{k = 1}^q \\theta_k \\; z^k \\\\ \\\\\n  \\text{so that } \\\\ \\\\\n  X(t) &= \\sum_{j = 1}^p \\phi_j \\; X(t - j) \\; + \\; W(t) \\; + \\; \\sum_{k = 1}^q \\theta_k \\; W(t - k)\n\\end{align}\n\\qquad(13.34)\\]\nTaking the multiplicative inverse of \\(\\phi(\\cdot)\\) we can re-express this model as the following filtering of the white noise process \\(W(\\cdot)\\).\n\\[\n\\begin{align}\n  X(\\cdot) &= \\phi(\\mathcal{B})^{-1} \\;  \\theta(\\mathcal{B}) \\; W(\\cdot) \\\\ \\\\\n  &= \\psi(\\mathcal{B}) \\; W(\\cdot) \\\\ \\\\\n  \\text{where } \\\\ \\\\\n  \\psi (z) &= \\frac{\\theta(z)}{\\phi(z)} = \\sum_{\\nu = 0}^\\infty \\psi_\\nu \\times z^\\nu\n\\end{align}\n\\qquad(13.35)\\]\nThe power-series expansion of the rational function \\(\\psi(\\cdot)\\) requires the roots of polynomial \\(\\phi(\\cdot)\\) to all have magnitude greater than unity, which we assume to be the case. Now let us specify some simple examples.\n\n\n13.2.11 AR(1)\nFirst we define \\(X(\\cdot)\\) to be the following simple auto-regressive model, abbreviated \\(AR(1)\\).\n\\[\n\\begin{align}\n  X(t) &= \\phi_1 \\; X(t - 1) \\; + \\; W(t) \\quad \\text{with } | \\phi_1 | &lt; 1\n\\end{align}\n\\qquad(13.36)\\]\nso that\n\\[\n\\begin{align}\n  \\psi (z) &= \\frac{1}{1 - \\phi_1 z} \\\\\n  &= \\sum_{\\nu = 0}^\\infty \\phi_1^\\nu \\times z^\\nu\n\\end{align}\n\\qquad(13.37)\\]\nand\n\\[\n\\begin{align}\n  f_{X, X} (\\lambda) &= \\left | \\psi(e^{-i \\lambda}) \\right |^2 \\; \\sigma_W^2 \\\\\n  &= \\left | \\frac{1}{1 - \\phi_1 e^{-i \\lambda}} \\right |^2 \\; \\sigma_W^2 \\\\\n  &= \\frac{\\sigma_W^2}{1 - 2 \\phi_1 \\cos(\\lambda) + \\phi_1^2}\n\\end{align}\n\\qquad(13.38)\\]\n\n\n13.2.12 MA(1)\nNext, let \\(Y(\\cdot)\\) denote the following simple moving average model \\((MA(1))\\).\n\\[\n\\begin{align}\n  Y(t) &= W(t) \\; + \\; \\theta_1 W(t - 1) \\quad \\text{with } | \\theta_1 | &lt; 1\n\\end{align}\n\\qquad(13.39)\\]\nso that\n\\[\n\\begin{align}\n  \\psi (z) &= 1 + \\theta_1 z\n\\end{align}\n\\qquad(13.40)\\]\nand\n\\[\n\\begin{align}\n  f_{Y, Y} (\\lambda) &= \\left | \\psi(e^{-i \\lambda}) \\right |^2 \\; \\sigma_W^2 \\\\\n  &= \\left | 1 + \\theta_1 e^{-i \\lambda} \\right |^2 \\; \\sigma_W^2 \\\\\n  &= \\left ( 1 + 2 \\theta_1 \\cos(\\lambda) + \\theta_1^2 \\right ) \\; \\sigma_W^2 \\\\\n\\end{align}\n\\qquad(13.41)\\]\n\n\n13.2.13 ARMA(1, 1)\nFinally we let \\(Z(\\cdot)\\) denote the following \\((ARMA(1, 1))\\) model.\n\\[\n\\begin{align}\n  Z(t) &= \\phi_1 Z(t - 1) \\; + \\; W(t) \\; + \\; \\theta_1 W(t - 1)\n\\end{align}\n\\qquad(13.42)\\]\nso that\n\\[\n\\begin{align}\n  \\psi (z) &= \\frac{1 + \\theta_1 z}{1 - \\phi_1 z} \\\\\n  &= (1 + \\theta_1 z) \\; \\sum_{\\nu = 0}^\\infty \\phi_1^\\nu \\times z^\\nu \\\\\n  &= 1 \\; + \\; \\sum_{k = 1}^\\infty (\\phi_1^k + \\theta_1 \\; \\phi_1^{k - 1}) \\times z^k\n\\end{align}\n\\qquad(13.43)\\]\nand\n\\[\n\\begin{align}\n  f_{Z, Z} (\\lambda) &= \\left | \\psi(e^{-i \\lambda}) \\right |^2 \\; \\sigma_W^2 \\\\\n  &= \\left | \\frac{1 + \\theta_1 e^{-i \\lambda}}{1 - \\phi_1 e^{-i \\lambda}} \\right |^2 \\; \\sigma_W^2 \\\\\n  &= \\frac{1 + 2 \\theta_1 \\cos(\\lambda) + \\theta_1^2}{1 - 2 \\phi_1 \\cos(\\lambda) + \\phi_1^2} \\; \\sigma_W^2\n\\end{align}\n\\qquad(13.44)\\]\n\n\n13.2.14 Comparison of ARMA models\nFigure 13.1 shows the respective time series simulated from these simple ARMA models, with the following parameter settings: \\(\\phi_1 =\\) 0.9; \\(\\theta_1 =\\) 0.9; \\(\\sigma_W =\\) 1.\n\n\n\n\n\n\n\n\nFigure 13.1: Simulation from simple ARMA models\n\n\n\n\n\nThe trajectory of the white noise process \\(W(\\cdot)\\) is indeed noisy; it would remain qualitatively unchanged by any permutation of its values. The MA(1) model generating \\(Y(\\cdot)\\) also displays a rather noisy trajectory. The remaining models, the AR(1) model generating \\(X(\\cdot)\\) and the ARMA(1, 1) model generating \\(Z(\\cdot)\\) show greater continuity.\nFigure 13.2 shows the respective spectrum density functions of these ARMA models. The spectrum densities of the AR(1) model and the ARMA(1, 1) model are concentrated at lower frequencies, correponding to greater wavelengths. Note that the horizontal axis labeled “freq” is a scaled version of \\(\\lambda\\), namely \\(\\mathcal{f} = \\frac{\\lambda}{2 \\pi}\\), which is the inverse of wavelength.\n\n\n\n\n\n\n\n\nFigure 13.2: Spectrum density function of each ARMA model",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#sec-finite-FT",
    "href": "ts-fourier.html#sec-finite-FT",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.3 Finite Fourier transform",
    "text": "13.3 Finite Fourier transform\n\n13.3.1 Definition\nLet \\(X_\\bullet (\\cdot)\\) be a random process satisfying Equation 13.12. Assume for the moment that \\(X_\\bullet (\\cdot)\\) has mean zero. Suppose that we have data generated by this process for \\(|t| \\le n\\).12 Then we define \\(d_\\bullet (\\lambda)\\), the finite Fourier transform of time series \\(\\{ X(t) \\}_{|t| \\le n}\\), as follows.\n\\[\n\\begin{align}\n  d_\\bullet (\\lambda) &= \\sum_{t = -n}^n X_\\bullet (t) \\times e^{- i \\lambda t}\n\\end{align}\n\\qquad(13.45)\\]\n\n\n13.3.2 Cramér representation\nFrom the Cramér representation (Equation 13.21) we have\n\\[\n\\begin{align}\n  d_\\bullet (\\lambda) &= \\sum_{t = -n}^n X_\\bullet (t) \\times e^{- i \\lambda t} \\\\\n  &= \\sum_{t = -n}^n \\left \\{ \\int_{0}^{2 \\pi} e^{i \\alpha t} \\; dZ_\\bullet (\\alpha) \\right \\} \\times e^{- i \\lambda t} \\\\\n  &= \\sum_{t = -n}^n \\left \\{ \\int_{0}^{2 \\pi} e^{i (\\alpha - \\lambda) t} \\; dZ_\\bullet (\\alpha) \\right \\} \\\\\n  &= \\int_{0}^{2 \\pi} \\left \\{ \\sum_{t = -n}^n e^{i (\\alpha - \\lambda) t}  \\right \\} \\; dZ_\\bullet (\\alpha) \\\\\n  &= \\int_{0}^{2 \\pi} \\mathcal{D}_n (\\alpha - \\lambda) \\; dZ_\\bullet (\\alpha)\n\\end{align}\n\\qquad(13.46)\\]\nHere \\(\\mathcal{D}_n (\\cdot)\\) denotes the Dirichlet kernel function of order \\(n\\).\n\\[\n\\begin{align}\n   \\mathcal{D}_n (\\theta) &= \\sum_{\\nu = -n}^n e^{i \\theta \\nu} \\\\\n   &=\n   \\begin{cases}\n     2 n + 1 & \\text{for } \\theta \\equiv 0 \\bmod (2 \\pi) \\\\\n     \\frac{\\sin ((2n + 1) \\theta / 2)}{\\sin(\\theta / 2)} & \\text{for } \\theta \\not \\equiv 0 \\bmod (2 \\pi)\n   \\end{cases}\n\\end{align}\n\\qquad(13.47)\\]\n\n\n13.3.3 Data windows and their frequency kernels\n\\(\\mathcal{D}_n (\\cdot)\\) is thus the Fourier transform of the indicator function \\(1_{|t| \\le n} (\\cdot)\\). In the early 1900’s Fejér noted that the approximation of a function by a finite Fourier series could be improved by replacing this indicator function by its Cesàro average (the average of partial sums). This has led to the applicaton of data windows of the form \\(h(t/n)\\) and their Fourier transforms \\(\\mathcal{H}_n(\\theta)\\) as follows.\n\\[\n\\begin{align}\n  d_\\bullet (\\lambda) &= \\sum_{t = -n}^n h(\\frac{t}{n}) \\times X_\\bullet (t) \\times e^{- i \\lambda t} \\\\\n  &= \\int_{0}^{2 \\pi} \\mathcal{H}_n (\\alpha - \\lambda) \\; dZ_\\bullet (\\alpha) \\\\ \\\\\n  &\\text{where } \\\\ \\\\\n  \\mathcal{H}_n (\\theta) &= \\sum_{\\nu = -n}^n h(\\frac{\\nu}{n}) \\times e^{i \\theta \\nu}\n\\end{align}\n\\qquad(13.48)\\]\nFunction \\(h(x)\\) (called a convergence factor or data taper) is typically chosen to be positive, even, and piecewise continuous, having a maximum of unity at \\(x = 0\\), and vanishing for \\(|x| &gt; 1\\). This ensures that the \\(2 \\pi\\)-periodic function \\(\\mathcal{H}_n (\\theta)\\) (called a frequency kernel) integrates to unity over any interval of length \\(2 \\pi\\) and is increasingly concentrated near \\(\\theta \\equiv 0 \\bmod (2 \\pi)\\) as \\(n \\rightarrow \\infty\\).13\nIn time series applications, the data taper \\(h(x)\\) is designed to produce a corresponding frequency kernel \\(\\mathcal{H}_n (\\theta)\\) highly concentrated at frequency zero, to the degree allowed by other considerations, such as computational complexity.\n\n\n13.3.4 Asymptotic normality\nIf \\(X_\\bullet\\) satisfies further regularity conditions14 then \\(d_\\bullet (\\lambda)\\) defined in Equation 13.48 has an asymptotic multivariate normal distribution. This asymptotic distribution is a standard (real-valued) multivariate normal distribution if \\(\\lambda \\equiv 0 \\bmod \\pi\\). Otherwise, it is a complex multivariate normal distribution (which doubles the degrees of freedom).15\nFor \\(\\lambda \\not \\equiv 0 \\bmod (2 \\pi)\\), the expected value of \\(d_\\bullet (\\lambda)\\) is zero. At \\(\\lambda \\equiv 0 \\bmod (2 \\pi)\\), the expected value of \\(d_\\bullet (\\lambda)\\) is proportionate to the mean of \\(X_\\bullet (\\cdot)\\) (assumed in this section to be zero to simplify notation). The asymptotic variance of \\(d_\\bullet (\\lambda)\\) is as follows.\n\\[\n\\begin{align}\n  Var \\{ d_\\bullet (\\lambda) \\} &\\sim (2n + 1) \\; \\pi \\; \\eta^2 f_{\\bullet, \\bullet} (\\lambda) \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  \\eta^2 &= \\int_{-1}^1 h(x)^2 \\; dx\n\\end{align}\n\\qquad(13.49)\\]\nMoreover if \\(0 \\le \\lambda_0 &lt; \\lambda_1 &lt; \\ldots &lt; \\lambda_K \\le \\pi\\), then \\(\\{ d_\\bullet (\\lambda_k) \\}_k\\) are asymptotically independent.16",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#sec-spec-est",
    "href": "ts-fourier.html#sec-spec-est",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.4 Spectrum Estimation",
    "text": "13.4 Spectrum Estimation\nIn this section we continue to assume that \\(X_\\bullet (\\cdot)\\) has mean zero in order to simplify notation and focus on key ideas.\n\n13.4.1 Periodogram\nThe term “periodogram” was introduced by Schuster17 in 1898, who sought to detect hidden periodicities in natural phenomena. It is defined as a scaled version of \\(d_\\bullet (\\lambda) \\times d_\\bullet (\\lambda)^*\\).\nRecall that for \\(\\lambda \\not \\equiv 0 \\bmod (2 \\pi)\\), the expected value of \\(d_\\bullet (\\lambda)\\) is zero. Moreover, we assume that \\(X_\\bullet (\\cdot)\\), and thus \\(d_\\bullet (0)\\), has mean zero in order to simplify notation. Consequently, the asymptotic expected value of \\(d_\\bullet (\\lambda) \\times d_\\bullet (\\lambda)^*\\) is the asymptotic variance-covariance matrix given by Equation 13.49.\n\\[\n\\begin{align}\n  E \\{ d_\\bullet (\\lambda) \\times d_\\bullet (\\lambda)^* \\} &\\sim (2n + 1) \\; \\pi \\; \\eta^2 f_{\\bullet, \\bullet} (\\lambda) \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  \\eta^2 &= \\int_{-1}^1 h(x)^2 \\; dx\n\\end{align}\n\\qquad(13.50)\\]\nFollowing conventional use of the symbol \\(I\\) to denote “intensity”, we define the periodogram as the following complex-valued, non-negative definite matrix\n\\[\n\\begin{align}\n  I_{\\bullet, \\bullet} (\\lambda) &= c_n \\; d_\\bullet (\\lambda) \\times d_\\bullet (\\lambda)^* \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  c_n &= \\left \\{ (2n + 1) \\; \\pi \\; \\eta^2 \\right \\}^{-1}\n\\end{align}\n\\qquad(13.51)\\]\nso that\n\\[\n\\begin{align}\n  E \\{ I_{\\bullet, \\bullet} (\\lambda) \\} &\\sim f_{\\bullet, \\bullet} (\\lambda)\n\\end{align}\n\\qquad(13.52)\\]\nThus the periodogram \\(I_{\\bullet, \\bullet} (\\lambda)\\) is an asymptotically unbiased estimate of the spectrum density \\(f_{\\bullet, \\bullet} (\\lambda)\\). The problem is that the variance of \\(I_{\\bullet, \\bullet} (\\lambda)\\) elements remains large as \\(n \\rightarrow \\infty\\).\nRecall that under regularity conditions on \\(X(\\cdot)\\), \\(d_\\bullet(\\lambda)\\) follows an asymptotically normal distribution. Under those same conditions the asymptotic distribution of \\(I_{\\bullet, \\bullet} (\\lambda)\\) is a Wishart distribution, real if \\(\\lambda \\equiv 0 \\bmod \\pi\\) and complex otherwise.\nA diagonal element of \\(I_{\\bullet, \\bullet} (\\lambda)\\), say \\(I_{a, a} (\\lambda)\\), is an estimate of \\(f_{a, a} (\\lambda)\\), the power spectrum of \\(X_a (\\cdot)\\). For \\(\\lambda \\equiv 0 \\bmod \\pi\\), the estimate is asymptotically distributed as \\(f_{a, a} (\\lambda) \\times \\chi_1^2\\), where \\(\\chi_\\nu^2\\) denotes a random variable having a chi-squared distribution with \\(\\nu\\) degrees of freedom. For \\(\\lambda \\not \\equiv 0 \\bmod \\pi\\), the estimate is asymptotically distributed as \\(f_{a, a} (\\lambda) \\times \\chi_2^2 / 2\\).18\n\\[\n\\begin{align}\n  I_{a, a} (\\lambda) &\\sim\n  \\begin{cases}\n    f_{a, a} (\\lambda) \\times \\chi_1^2 & \\text{if } \\lambda \\equiv 0 \\bmod \\pi \\\\\n    f_{a, a} (\\lambda) \\times \\chi_2^2 / 2 & \\text{if } \\lambda \\not \\equiv 0 \\bmod \\pi\n  \\end{cases}\n\\end{align}\n\\qquad(13.53)\\]\n\\[\n\\begin{align}\n  Var \\{ I_{a, a} (\\lambda) \\} &\\sim\n  \\begin{cases}\n    2 f_{a, a} (\\lambda)^2 & \\text{if } \\lambda \\equiv 0 \\bmod \\pi \\\\\n    f_{a, a} (\\lambda)^2 & \\text{if } \\lambda \\not \\equiv 0 \\bmod \\pi\n  \\end{cases}\n\\end{align}\n\\qquad(13.54)\\]\nThus \\(I_{\\bullet, \\bullet} (\\lambda)\\), although asymptotically unbiased, is not a consistent estimator of \\(f_{\\bullet, \\bullet} (\\lambda)\\) since the variance of its diagonal elements remains bounded above zero as \\(n \\rightarrow \\infty\\).\n\n\n13.4.2 DFT: Discrete Fourier transform\nThe discrete Fourier transform (DFT) is the restriction of the finite Fourier transform to a discrete set of equi-spaced frequencies. To simplify notation we consider a univariate process \\(X(\\cdot)\\). We also index observed values as \\(\\{ X(0), X(1), \\ldots, X(T-1) \\}\\). Now we define the DFT of this time series as follows.\n\\[\n\\begin{align}\n  d_X (\\lambda_j) &= \\sum_{t = 0}^{T - 1} h \\left (\\frac{2(t - t_m)}{T} \\right ) \\times X(t) \\times e^{- i \\lambda_j t} \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  t_m &= \\frac{T - 1}{2}\n\\end{align}\n\\qquad(13.55)\\]\nThe fast Fourier transform (FFT19) enables efficient computation of the DFT at a set of \\(S\\) frequencies, where \\(S \\ge T\\) is chosen to be a highly composite number.\n\\[\n\\begin{align}\n  \\lambda_j &= \\frac{2 \\pi j}{S} \\quad \\text{for } j \\in \\{0, \\ldots, S - 1 \\}\n\\end{align}\n\\qquad(13.56)\\]\nThe computational steps are:\n\napply taper \\(h(\\cdot)\\) to \\(\\{ X(t) \\}_{t = 0}^{T-1}\\)\n“pad” the tapered data by appending \\(S - T\\) zeros\nsubmit the padded, tapered data to the FFT algorithm\n\nIn R the DFT can be computed with the function stats:fft().\n\n\n13.4.3 Smoothed periodogram\nWe noted above that the periodogram \\(I_{\\bullet, \\bullet} (\\lambda)\\), although an asymptotically unbiased estimate of \\(f_{\\bullet, \\bullet} (\\lambda)\\), suffers from high variance (which is evident from plots of the periodogram). Averaging periodogram values at adjacent frequencies yields a revised estimate, say \\(\\hat{f}_{\\bullet, \\bullet} (\\lambda)\\), having lower variance, but if too many periodogram values are averaged \\(\\hat{f}_{\\bullet, \\bullet} (\\lambda)\\) may be a biased estimate that blurs features of interest.\nHere is a framework for constructing a consistent20 estimator, that is, a sequence of estimates that converge to \\(f_{\\bullet, \\bullet} (\\lambda)\\) as \\(T \\rightarrow \\infty\\). Let \\(\\beta_T &gt; 0\\) denote the bandwidth of adjacent frequencies to be averaged. To drive the variance of successive estimates to zero as \\(T \\rightarrow \\infty\\) we want the number of adjacent frequencies, proportionate to \\(\\beta_T \\; T\\), to increase without bound. To drive the bias to zero we want the neighborhood of averaged frequencies to shrink to \\(\\lambda\\), thus requiring \\(\\beta_T \\rightarrow 0\\). For example we might set \\(\\beta_T = 1/\\sqrt{T}\\), or more generally \\(\\beta_T = c \\; T^{- \\alpha}\\) for some choice of \\(c &gt; 0\\) and \\(\\alpha \\in (0, 1)\\).\nThis framework also requires a frequency window function \\(W(\\cdot)\\) that integrates to unity and is even, absolutely integrable, and of bounded variation.\n\\[\n\\begin{align}\n  & W(-\\alpha) = W(\\alpha) \\\\\n  & \\int_{-\\infty}^\\infty | W(\\alpha) | \\; d \\alpha &lt; \\infty \\\\\n  & \\int_{-\\infty}^\\infty W(\\alpha) \\; d \\alpha = 1\n\\end{align}\n\\qquad(13.57)\\]\nNow we define \\(\\hat{f}_{\\bullet, \\bullet} (\\lambda)\\) as follows.21\n\\[\n\\begin{align}\n  \\hat{f}_{\\bullet, \\bullet} (\\lambda) &= \\sum_{\\lambda_j \\not \\equiv 0 \\bmod (2 \\pi)} \\frac{1}{\\beta_T} \\; W \\left ( \\frac{\\lambda - \\lambda_j}{\\beta_T} \\right ) \\times I_{\\bullet, \\bullet} ( \\lambda_j ) \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  \\lambda_j &= \\frac{2 \\pi j}{S} \\quad \\forall j \\in \\mathbb{Z}\n\\end{align}\n\\qquad(13.58)\\]\nUnder the previously mentioned regularity conditions for \\(X_{\\bullet, \\bullet} (\\cdot)\\), \\(\\hat{f}_{\\bullet, \\bullet} (\\lambda)\\) is a consistent estimator and is asymptotically normal.\nThe asymptotic variance of \\(\\hat{f}_{a, a} (\\lambda)\\), the estimated power spectrum of \\(X_a (\\cdot)\\), is as follows.\n\\[\n\\begin{align}\n  Var\\{ \\hat{f}_{a, a} (\\lambda) \\} &\\sim\n  \\begin{cases}\n    4 \\pi \\omega^2 \\; \\frac{f_{a, a} (\\lambda)^2}{\\beta_T \\; T} & \\text{for } \\lambda \\equiv 0 \\bmod \\pi \\\\\n    2 \\pi \\omega^2 \\; \\frac{f_{a, a} (\\lambda)^2}{\\beta_T \\; T} & \\text{for } \\lambda \\not \\equiv 0 \\bmod \\pi\n  \\end{cases} \\\\ \\\\\n  & \\text{where } \\\\ \\\\\n  \\omega^2 &= \\int_{- \\infty}^\\infty W(\\alpha)^2 d \\alpha\n\\end{align}\n\\qquad(13.59)\\]\nTaking the natural logarithm, and assuming \\(f_{a, a} (\\lambda) &gt; 0\\), we have that \\(\\ln \\hat{f}_{a, a} (\\lambda)\\) is a consistent and asymptotically normal estimate of \\(\\ln f_{a, a} (\\lambda)\\) with the following asymptotic variance.22\n\\[\n\\begin{align}\n  Var\\{ \\ln \\hat{f}_{a, a} (\\lambda) \\} &\\sim\n  \\begin{cases}\n    \\frac{4 \\pi \\omega^2}{\\beta_T \\; T} & \\text{for } \\lambda \\equiv 0 \\bmod \\pi \\\\\n    \\frac{2 \\pi \\omega^2}{\\beta_T \\; T} & \\text{for } \\lambda \\not \\equiv 0 \\bmod \\pi\n  \\end{cases}\n\\end{align}\n\\qquad(13.60)\\]",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#sec-spec-examples",
    "href": "ts-fourier.html#sec-spec-examples",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.5 Examples of Spectrum Analysis",
    "text": "13.5 Examples of Spectrum Analysis\nIn Section 13.1 we introduced a few sets of time series data and considered questions that we now attempt to address.\nThe R function stats::spec.pgram() is used to estimate spectra in this section. A raised cosine bell-shaped function is used to taper a user-specified portion of the head and tail of data. The tapered data are padded with zeros for efficient use of the FFT algorithm. The periodogram is calculated and then a modified Daniell frequency kernel averages across adjacent frequencies within a user-specified bandwidth. The function documentation gives further details.\n\n13.5.1 Sunspots\nFigure 13.3 below shows the estimated spectrum density function of monthly sunspot numbers. The first and final quarter of the data were tapered to better resolve the spectrum at adjacent frequencies. For the same reason the density estimate is a running weighed average of only 17 adjacent periodogram values, amounting to a fairly narrow bandwidth (0.012).\n\n\n\n\n\n\n\n\nFigure 13.3: Monthly sunspot numbers: estimated spectrum density\n\n\n\n\n\nFigure 13.4 gives a closer view of the spectrum density function at the lowest observable frequencies. The peak in the spectrum occurs at frequency 0.008 cycles per month corresponding to a wavelength of 10.8 years. As previously mentioned, the gaps between peak values of sunspot numbers vary somewhat, which is part of the reason for the breadth of this peak in the spectrum density (the other part being the unavoidable periodogram-averaging required to lower the variance of the estimated spectrum density function.)\n\n\n\n\n\n\n\n\nFigure 13.4: Sunspot spectrum density: zoomed-in view\n\n\n\n\n\n\n\n13.5.2 City Temperatures\nFigure 13.5 below shows (on a log-10 scale) estimated spectrum densities of daily temperatures in Honolulu and New York City.23\n\n\n\n\n\n\n\n\nFigure 13.5: City temperatures: estimated spectrum densities\n\n\n\n\n\nFigure 13.6 gives a zoomed-in view of the lowest observable frequencies. For both cities the peak in the spectrum occurs at frequency 0.003 cycles per day corresponding to a wavelength of 360.6 days, a bit below the expected 365.25 days due to imperfect frequency resolution.\nPreviously we noted that Honolulu temperatures fluctuate much less than New York City temperatures, and we raised the question of the degree to which the annual cycle contributes to overall variability in each city. The respective ratios of max-divided-by-min (or log-max minus log-min) of the spectrum density estimates appear similar in the two cities, with New York exhibiting a somewhat greater range. This suggests that annual variability in Honolulu contributes nearly as much to the overall variance of temperatures as it does in New York City.\n\n\n\n\n\n\n\n\nFigure 13.6: City temperature spectra: zoomed-in view\n\n\n\n\n\nFigure 13.7 shows the two cities to be nearly perfectly coherent and in phase at frequenccies near the annual cycle, as expected.\n\n\n\n\n\n\n\n\nFigure 13.7: HNL-NYC coherence and phase: zoomed-in view",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#closing-remarks",
    "href": "ts-fourier.html#closing-remarks",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.6 Closing Remarks",
    "text": "13.6 Closing Remarks\nThis note is intended to illustrate some uses of spectrum analysis, and to introduce the corresponding mathematical framework and methods of estimation.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#sec-glossary",
    "href": "ts-fourier.html#sec-glossary",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.7 A: Glossary of Notation",
    "text": "13.7 A: Glossary of Notation\n\\[\n\\begin{align}\n  {a_{\\bullet, \\bullet} (u)}\n    \\quad & \\quad \\text{matrix of filter coefficients at time-shift } u \\\\\n  \\\\\n  \\mathcal{B}\n     \\quad & \\quad \\text{back-shift operator } \\\\\n  \\\\\n  \\beta_T\n     \\quad & \\quad \\text{frequency bandwidth of averaged periodogram values } \\\\\n  \\\\\n  DFT\n     \\quad & \\quad \\text{discrete Fourier transform } \\\\\n  \\\\\n  d_\\bullet (\\lambda)\n     \\quad & \\quad \\text{finite Fourier transform of } X_\\bullet (t) \\\\\n  \\\\\n  FFT\n     \\quad & \\quad \\text{fast Fourier transform algorithm } \\\\\n  \\\\\n  f_{\\bullet, \\bullet} (\\lambda)\n     \\quad & \\quad \\text{spectrum density matrix at frequency } \\lambda \\\\\n  \\\\\n  \\hat{f}_{\\bullet, \\bullet} (\\lambda)\n     \\quad & \\quad \\text{estimate of } f_{\\bullet, \\bullet} (\\lambda) \\\\\n  \\\\\n  \\phi(\\cdot)\n     \\quad & \\quad \\text{AR polynomial, } 1 - \\sum_{\\nu = 1}^p \\phi_\\nu \\; z^\\nu \\\\\n  \\\\\n  \\gamma_{\\bullet, \\bullet} (u)\n     \\quad & \\quad \\text{auto-covariance matrix at time-shift } u \\\\\n  \\\\\n  h (x)\n     \\quad & \\quad \\text{data-taper or convergence factor } \\\\\n  \\\\\n  \\mathcal{H}_n (\\lambda)\n     \\quad & \\quad \\text{finite Fourier transform of } h(\\frac{t}{n}) \\\\\n  \\\\\n  I_{\\bullet, \\bullet} (\\lambda)\n     \\quad & \\quad \\text{periodogram at frequency } \\lambda \\\\\n  \\\\\n  \\mathcal{I}\n     \\quad & \\quad \\text{identity operator } \\\\\n  \\\\\n  m_\\bullet (t)\n     \\quad & \\quad E \\{ X_\\bullet (t) \\} \\text{, expected value of } X_\\bullet (t) \\\\\n  \\\\\n  \\mu_X\n     \\quad & \\quad E\\{ X (t_0) \\} \\text{, expected value of } X (t_0) \\\\\n  \\\\   \n  \\hat{\\mu}_X\n     \\quad & \\quad \\text{sample mean, } S_{X, T} / T \\\\\n  \\\\   \n  \\nu_T\n     \\quad & \\quad \\{ 0, 1, \\dots, T-1 \\} \\\\\n  \\\\\n  R_{a, b} (\\lambda)\n     \\quad & \\quad \\text{coherency: } \\frac{f_{a, b}(\\lambda)}{\\sqrt{f_{a, a}(\\lambda) f_{b, b}(\\lambda)}} \\\\\n  \\\\\n  | R_{a, b} (\\lambda) |^2\n     \\quad & \\quad \\text{coherence } \\\\\n  \\\\\n  Arg(\\; R_{a, b} (\\lambda) \\;)\n     \\quad & \\quad \\text{phase } \\\\\n  \\\\\n  \\rho_{\\bullet, \\bullet} (u)\n     \\quad & \\quad \\text{autocorrelation matrix at time-shift } u \\\\\n  \\\\\n  S_{X, T}\n     \\quad & \\quad \\text{the sample sum of T observations of } X (\\cdot) \\\\\n  \\\\\n  \\sigma_X^2\n     \\quad & \\quad Var \\{ X (t_0) \\} \\text{, variance of } X (t_0) \\\\\n  \\\\\n  T\n     \\quad & \\quad \\text{number of time series observations } \\\\\n  \\\\\n  t_0\n     \\quad & \\quad \\text{initial time index of time series observations } \\\\\n  \\\\\n  t_f\n     \\quad & \\quad \\text{final time index of time series observations } \\\\\n  \\\\\n  \\tau_T\n     \\quad & \\quad t_0 + \\{ 0, 1, \\ldots, T-1  \\} \\\\\n  \\\\\n  \\theta(\\cdot)\n     \\quad & \\quad \\text{MA polynomial, } 1 + \\sum_{\\nu = 1}^q \\theta_\\nu \\; z^\\nu \\\\\n  \\\\\n  W(\\cdot)\n     \\quad & \\quad \\text{white noise process, } W(\\cdot) \\sim wn(0, \\sigma_W^2)  \\\\\n  \\\\\n  W(\\lambda)\n     \\quad & \\quad \\text{frequency window or kernel function}  \\\\\n  \\\\\n  X_\\bullet (t)\n     \\quad & \\quad (X_1 (t), \\ldots, X_d (t)) \\text{, a multivariate random process }  \\\\\n  \\\\\n  dZ_\\bullet (\\lambda)\n     \\quad & \\quad \\text{Cramér representation of } X_\\bullet (t)\n\\end{align}\n\\]",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#sec-data-examples",
    "href": "ts-fourier.html#sec-data-examples",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.8 B: Data Examples",
    "text": "13.8 B: Data Examples\n\n13.8.1 Sunspot Numbers: Monthly Averages, 1749 - Present\n\n13.8.1.1 Description\nFigure 13.8 below shows sunspot.month, monthly average sunspot numbers, in R package datasets. Source: Solar Influences Data Analysis Center (SIDC), Royal Observatory of Belgium. The data shown here span the years 1749-2025, for a total of 3310 months (275 years and 10 months).\n\n\n\n\n\n\n\n\nFigure 13.8: Sunspot numbers: monthly average\n\n\n\n\n\n\n\n13.8.1.2 Context: Questions and Objectives\nAs described in Recalibration of the Sunspot-Number: Status Report | Solar Physics, the record of sunspot numbers links past and present solar behavior, and is the primary input for reconstructions of total solar irradiance (TSI) for years before 1978.\n\n\n\n13.8.2 Daily City Temperatures: 1995-2020\n\n13.8.2.1 Description\nFigure 13.9 below shows the daily average temperature in Honolulu and New York City from 1995-01-01 to 2020-05-13 (for a total or 9265 days).24\n\n\n\n\n\n\n\n\nFigure 13.9: Daily Average Temperature (F), HNL and NYC, 1995-2020\n\n\n\n\n\n\n\n13.8.2.2 Context: Questions and Objectives\nThe temperatures shown are available from Kaggle (obtained from the University of Dayton) as part of a collection of temperatures at 157 cities in the US and 167 cities across the globe. These surface temperatures are one of several variables that are used to model weather systems worldwide.\nAlso see the following R packages:\n\nGSOD, Global Surface Summary of the Day\nclimate",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#references",
    "href": "ts-fourier.html#references",
    "title": "13  Time Series Spectrum Analysis",
    "section": "13.9 References",
    "text": "13.9 References",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-fourier.html#footnotes",
    "href": "ts-fourier.html#footnotes",
    "title": "13  Time Series Spectrum Analysis",
    "section": "",
    "text": "See The Historical Sunspot Record.↩︎\nOccasionally “time series” methods are applied to data indexed not by time but rather by length, or some other linear dimension. For example see (thomson1977?).↩︎\nWe assume the components of \\(X_\\bullet(t)\\) to be real-valued, unless stated otherwise.↩︎\nFor a univariate time series \\(X(t)\\) we denote distributional parameters either omitting a subscript or else using the subscript \\(X\\).↩︎\nThe assumption of second-order (or wide-sense) stationarity suffices for most time series models used in practice, but some cases may call for the assumption of strict stationarity. This means that for any finite set of times \\((t_1, \\ldots, t_K)\\) and any time-shift \\(s\\), the joint probability distribution of \\((X_\\bullet (t_1), \\ldots, X_\\bullet (t_K))\\) is identical to that of \\((X_\\bullet (t_1 - s), \\ldots, X_\\bullet (t_K - s))\\).↩︎\nSee (Newton1672spectrum?).↩︎\nAlso see (brillinger1993digitalrainbow?).↩︎\nIn other contexts the term “intensity” is replaced by “power”. (wiener1930GHA?) introduced the term “power spectrum” of a signal.↩︎\nSee (brillinger2001?).↩︎\nGiven a complex-valued matrix or vector \\(\\zeta\\) we denote by \\(\\zeta^*\\) its conjugate transpose.↩︎\nSee (brillinger2001?)↩︎\nTo simplify notation in this section we change the range of integer index \\(t\\) from \\(t \\in [0, T)\\) to \\(t \\in [-n, n]\\).↩︎\nSee (brillinger2001?).↩︎\nSee (brillinger2001?). Asymptotic normality of the discrete Fourier transform is ensured if \\(X_\\bullet (\\cdot)\\) is strictly stationary and has summable cumulants of all orders.↩︎\nSee (brillinger2001?). If \\(U_\\bullet\\) and \\(V_\\bullet\\) are random, real-valued vectors, each having \\(d\\) elements, then \\(W = U + i V\\) is said to have a complex normal distribution provided that the \\(2d\\)-vector \\((U, V)\\) has a multivariate normal distribution whose variance-covariance matrix is such that \\(\\Sigma_{V, V} = \\Sigma_{U, U}\\) and \\(\\Sigma_{V, U}\\) is skew-symmetric, so that \\(\\Sigma_{V, U} = -\\Sigma_{U, V}\\).↩︎\nSee (brillinger2001?, Theorem 4.4.2).↩︎\nSee (schuster1898?).↩︎\nSee (brillinger2001?, Theorem 5.2.7).↩︎\nSee (FFT_Wikipedia_2025?).↩︎\nThat is, mean-square consistent: the expected squared estimation error tends to zero as \\(T \\rightarrow \\infty\\).↩︎\nAlthough this summation extends over an infinite number of discrete frequencies \\(\\lambda_j\\), there are just \\(S\\) frequencies modulo \\(2 \\pi\\). Recall that the periodogram is \\(2 \\pi\\)-periodic. In addition, we may define the frequency window \\(W(\\cdot)\\) equal to zero outside some finite interval.↩︎\nSee (brillinger2001?) for further details.↩︎\nWe have used the same parameter settings for tapering and periodogram-averaging as in the sunspot example.↩︎\nFor convenience missing values have been imputed (estimated) with a weighted running average spanning 7 days. This was done with R function imputeTS::na.ma(). Of the 9265 days shown, there were 18 missing values for Honolulu and 20 missing values for NYC.↩︎",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Time Series Spectrum Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html",
    "href": "ts-intro.html",
    "title": "11  Time Series Data Analysis",
    "section": "",
    "text": "11.1 Introduction\nThis technical note gives examples of data in which successive observations are statistically dependent and are typically indexed by time.1 Time series data sampled at regular intervals (daily stock prices, for example) are the most common case, followed by point process data representing the occurrence of events (earthquakes, for example) at random times.\nNearly all of the time series examples presented here are from the selected textbook (shumway2025?) supported by R package astsa (astsa?).\nStatistical models of such data include parametric models expressed in the time domain and non-parametric models expressed in the Fourier frequency domain (number of cycles per unit of time).\nThis note also comments on the detection of statistical dependence among successive observations and the modification of statistical tests and estimators for such data.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#natural-science-examples",
    "href": "ts-intro.html#natural-science-examples",
    "title": "11  Time Series Data Analysis",
    "section": "11.2 Natural Science Examples",
    "text": "11.2 Natural Science Examples\n\n11.2.1 Sunspots: Monthly Counts, 1749 - Present\n\n11.2.1.1 Description\nFigure 13.8 below shows sunspot.month, the number of sunspots per month in R package datasets. Source: Solar Influences Data Analysis Center (SIDC), Royal Observatory of Belgium.\n\n\n\n\n\n\n\n\nFigure 11.1: Sunspots: monthly count\n\n\n\n\n\n\n\n11.2.1.2 Context: Questions and Objectives\nAs described in Recalibration of the Sunspot-Number: Status Report | Solar Physics, the record of sunspot numbers links past and present solar behavior, and is the primary input for reconstructions of total solar irradiance (TSI) for years before 1978.\n\n\n\n11.2.2 Old Faithful eruptions\n\n11.2.2.1 Description\nFigure 11.2 below represents eruptions of the “Old Faithful” geyser in Yellowstone National Park, Wyoming from August 1 to August 15, 1985.2 The delay variable denotes the number of minutes since the previous eruption and the duration variable denotes the duration of the eruption, again in minutes.\nThe data represent a point process, where each observation records the time of occurrence of a certain type of event, and may include other information about the occurrence. Here the type of event is an eruption of the Old Faithful geyser. In addition to the time of the eruption we have the duration of the eruption.\n\n\n\n\n\n\n\n\nFigure 11.2: Old Faithful eruptions: delay and duration\n\n\n\n\n\n\n\n11.2.2.2 Context: Questions and Objectives\nThe figure shows two clusters of data points. Old Faithful eruptions are conjectured to occur in 2 distinct temporal patterns due to the presence of an upper and lower chamber beneath the vertical column (tube) that forms the geyser.\nIn this example, we have the goal of scientific understanding of a natural phenomenon, supported by statistical modeling.\n\n\n\n11.2.3 Climate Change\nHere are some of the data sets related to climate change from the R package astsa.\n\n\n\nClimate: selected data sets\n\n\n\n\n\n\n\npkg\nds\ntitle\n\n\n\n\nastsa\nENSO\nEl Nino - Southern Oscillation Index\n\n\nastsa\ncardox\nMonthly Carbon Dioxide Levels at Mauna Loa\n\n\nastsa\ngtemp.month\nMonthly global average surface temperatures by year\n\n\nastsa\ngtemp_both\nGlobal mean land and open ocean temperature deviations, 1850-2023\n\n\nastsa\ngtemp_land\nGlobal mean land temperature deviations, 1850-2023\n\n\nastsa\ngtemp_ocean\nGlobal mean ocean temperature deviations, 1850-2023\n\n\nastsa\nsoi\nSouthern Oscillation Index\n\n\n\n\n\n\n11.2.3.1 Description\nFigure 12.11 below shows the time series gtemp_land and gtemp_ocean, annual temperature deviations (in ◦C) from averages for the period 1991-2020.3 The temperatures are based on averages over the Earth’s land area and over the part of the ocean that is free of ice at all times (open ocean). The time period is from 1850 to 2023. Note that the trend is not linear, with periods of leveling off followed by sharp increases.\n\n\n\n\n\n\n\n\nFigure 11.3: Deviations in global surface temperatures\n\n\n\n\n\n\n\n11.2.3.2 Context: Questions and Objectives\nThe upward trend in the two temperature series during the latter part of the 20th century has been used as an argument for the climate change hypothesis. Further data and information would be needed, of course, to estimate the contributions of natural and anthropogenic sources to the observed rise in temperature.\n\n\n\n11.2.4 El Niño and Fish Population\n\n11.2.4.1 Description\nFigure 12.14 shows the Southern Oscillation Index (SOI, astsa::soi) and associated Recruitment (astsa::rec), an index of the number of viable new fish). Both series consist of 453 monthly values ranging over the years 1950–1987.\nThe two time series show two types of oscillation: an annual cycle (warm in the summer, cool in the winter), and a slower cycle that seems to repeat about every 4 years.\n\n\n\n\n\n\n\n\nFigure 11.4: Southern Oscillation Index and fish recruitment\n\n\n\n\n\n\n\n11.2.4.2 Context: Questions and Objectives\nThe processes that drive the periodicity of the two time series are of scientific interest. Frequency decomposition of the time series may help to identify those underlying processes.\n\n\n\n11.2.5 Predator–Prey Interactions\n\n11.2.5.1 Description\nOne of the classic studies of predator–prey interactions is based on the record of lynx (astsa::Lynx) and snowshoe hare (astsa::Hare) pelts purchased by the Hudson’s Bay Company of Canada from 1845 to 1935. Assuming pelt purchases are proportional to animals in the wild, the data are an indirect measure of predation.\nThese predator–prey interactions often lead to cyclical patterns of predator and prey abundance. The units of the data shown in Figure 12.9 are thousands of pelts per year.\n\n\n\n\n\n\n\n\nFigure 11.5: Hare and lynx: purchased pelts\n\n\n\n\n\n\n\n11.2.5.2 Context: Questions and Objectives\nThe scientific understanding of predator-prey interactions, starting with the Lotka-Volterra model (1910-1926), marked the beginnings of mathematical biology, and has been extended to the analysis of economic competition.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#business-and-economic-examples",
    "href": "ts-intro.html#business-and-economic-examples",
    "title": "11  Time Series Data Analysis",
    "section": "11.3 Business and Economic Examples",
    "text": "11.3 Business and Economic Examples\n\n11.3.1 JJ Quarterly Earnings\n\n11.3.1.1 Description\nFigure 12.10 below shows Johnson & Johnson quarterly earnings per share in US dollars from 1960 through 1980 (astsa::jj). The bottom panel shows the same data on a \\(\\log_e\\) scale. Superimposed on the upward trend is an annual pattern, including a sharp rise to first quarter earnings from those of the previous quarter.\n\n\n\n\n\n\n\n\nFigure 11.6: J&J: earnings per share at linear and log scales\n\n\n\n\n\n\n\n11.3.1.2 Context: Questions and Objectives\nDeveloping a statistical model of the time series can be regarded as a first step toward an economic understanding. With that understanding, various interested parties can set company performance goals, make investment decisions, or use this company as an example in a larger study.\n\n\n\n11.3.2 Dow Jones Industrial Average\n\n11.3.2.1 Description\nFigure 12.12 shows the trading day closings and returns (percent change)4 of the Dow Jones Industrial Average (DJIA, astsa::djia) from 2006 to 2016. It is easy to spot the financial crisis of 2008.\n\n\n\n\n\n\n\n\nFigure 11.7: Dow Jones Industrial Average\n\n\n\n\n\n\n\n11.3.2.2 Context: Questions and Objectives\nThe returns of the DJIA are typical of other assets. The mean function of the series appears to be stable with an average return of approximately zero. Of equal interest are highly volatile (variable) periods. That variability is both a symptom and cause of market uncertainty. A challenge in the analysis of financial data is to forecast the volatility of future returns.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#signal-processing-examples",
    "href": "ts-intro.html#signal-processing-examples",
    "title": "11  Time Series Data Analysis",
    "section": "11.4 Signal Processing Examples",
    "text": "11.4 Signal Processing Examples\n\n11.4.1 fMRI Imaging\n\n11.4.1.1 Description\nFigure 11.8 shows fMRI data (astsa::fmri1) from 2 locations in each of the cortex, thalamus, and cerebellum regions of the brain; n = 128 points, with one observation taken every 2 seconds. The square wave shows the stimulus signal (on or off).\nThe data are from a 1997 study that used fMRI to examine pain perception in humans. Here we focus on five subjects whose hands were periodically brushed. The stimulus (represented as a square wave in the figure) was applied for 32 seconds and then stopped for 32 seconds so that the signal period is 64 seconds. The sampling rate was one observation every 2 seconds for 256 seconds (n = 128).\nThe data are consecutive measures of blood oxygenation-level dependent (BOLD) signal intensity, which measures areas of activation in the brain. The series shown are from two locations each in the cortex, thalamus, and cerebellum and the values are averaged over subjects. These were evoked responses and all subjects were in phase.\n\n\n\n\n\n\n\n\nFigure 11.8: Evoked response per brain region\n\n\n\n\n\n\n\n11.4.1.2 Context: Questions and Objectives\nNote that the periodicities appear strongly in the motor cortex series and less so in the thalamus and cerebellum. This experiment has been designed to test whether these distinct areas of the brain respond differently to the stimulus.\n\n\n\n11.4.2 Speech Recording\n\n11.4.2.1 Description\nFigure 11.9 below shows the speech data set from the astsa package, a recording of the utterance aaahhh. Note the repetitive nature of the signal and the rather regular periodicities. Also note the repetition of small wavelets. The separation between wavelets is known as the pitch period and represents the response of the vocal tract filter to a periodic sequence of pulses stimulated by the opening and closing of the glottis.\n\n\n\n\n\n\n\n\nFigure 11.9: Utterance ‘aaahhh’ sampled at 10 Hz\n\n\n\n\n\n\n\n11.4.2.2 Context: Questions and Objectives\nComputer recognition of speech is an active area of research and development. In the current case one might seek to transcribe the signal above into the text string “aaahhh”. One approach would be a frequency based decomposition (spectral analysis) of the signal yielding a signature of the utterance that could be matched to one or more entries in a library of such signatures.\n\n\n\n11.4.3 Earthquakes and Explosions\n\n11.4.3.1 Description\nFigure 11.10 shows recordings (40 per second) at a Scandinavian seismic station of an earthquake (astsa::EQ5) and of a mining explosion (astsa::EXP6).\n\n\n\n\n\n\n\n\nFigure 11.10: Earthquake and explosion sampled at 40 Hz\n\n\n\n\n\n\n\n11.4.3.2 Context: Questions and Objectives\nThe general problem of interest is to distinguish between wave-forms generated by earthquakes versus those generated by explosions.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#mathematical-framework",
    "href": "ts-intro.html#mathematical-framework",
    "title": "11  Time Series Data Analysis",
    "section": "11.5 Mathematical Framework",
    "text": "11.5 Mathematical Framework\nStatistical applications are often based on one or more data frames in which each row represents an observation and each column represents a variable of interest. Consider for example the predator-prey data previously shown.\n\n\n# A tibble: 91 × 3\n     yr  hare  lynx\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1845  19.6  30.1\n2  1846  19.6  45.2\n3  1847  19.6  49.2\n4  1848  12.0  39.5\n5  1849  28.0  21.2\n# ℹ 86 more rows\n\n\nThe distinction of time series analysis is that observations are indexed by time, and are not assumed to be statistically independent. In time series analysis, we typically consider the data to be a realization of a random process of the following form.\n\\[\n\\begin{align}\n  X_\\bullet (t) = (X_1 (t), \\ldots, X_d (t))\n\\end{align}\n\\qquad(11.1)\\]\nIn our example the number \\(d\\) of data columns is two (hare, lynx), the unit of time is one year, and the sampling frequency is once per year. The random process is idealized to span all time \\((t \\in \\mathbb{Z})\\), but the data of course span some finite period.\n\\[\n\\begin{align}\n  t &= t_0 + u & \\text{ with } u \\in \\{0, 1, \\ldots, T-1 \\}\n\\end{align}\n\\]\nThe expected value of the random process may be modeled by various functions of time: a constant, a linear trend, a seasonal component (periodic function), etc.\n\\[\n\\begin{align}\n  m_\\bullet (t) &= E \\{ X_\\bullet (t) \\}\n\\end{align}\n\\qquad(11.2)\\]\nIn an additive model (which is most common), the residual random process, \\(X_\\bullet (t) - m_\\bullet (t)\\) , is assumed to be second-order stationary.5 That is, if we shift \\(X_\\bullet (\\cdot)\\) by any number of time units \\(s\\) to obtain a new proces \\(Y_\\bullet (\\cdot)\\), we assume that the respective covariance structures of \\(X_\\bullet (\\cdot)\\) and \\(Y_\\bullet (\\cdot)\\) are the same.\n\\[\n\\begin{align}\n  Y_\\bullet (t) &= \\mathcal{B}^s \\{ X_\\bullet (\\cdot) \\} (t)  \\\\\n  &= X_\\bullet (t - s)\n\\end{align}\n\\qquad(11.3)\\]\nHere \\(\\mathcal{B}\\) denotes the back-shift operator that shifts time by one unit, so that \\(\\mathcal{B}^s\\) (that is, \\(s\\) repeated applications of \\(\\mathcal{B}\\)) shifts time by \\(s\\) units. Then second-order stationarity can be expressed as follows.\n\\[\n\\begin{align}\n  Cov \\{ X_a (t + u), X_b (t) \\}\n  &= Cov \\{ Y_a (t + u), Y_b (t) \\} \\\\\n  &= Cov \\{ X_a (t + u - s), X_b (t - s) \\} \\\\\n  \\\\\n  & \\text{for all } s \\in \\mathbb{Z} \\text{ and } a, b \\in \\{ 1, \\ldots, d \\}\n\\end{align}\n\\]\nSetting \\(s = t\\) we have\n\\[\n\\begin{align}\n  Cov \\{ X_a (t + u), X_b (t) \\}\n  &= Cov \\{ X_a (u), X_b (0) \\}\n\\end{align}\n\\qquad(11.4)\\]\nConsequently, second-order stationarity enables one to define and estimate the following auto-covariance function.\n\\[\n\\begin{align}\n  \\gamma_{\\bullet, \\bullet} (u) &= \\left \\{ \\gamma_{a, b} (u) \\right \\}_{a, b = 1}^d \\\\\n  \\\\\n  \\text{where} \\\\\n  \\gamma_{a, b} (u) &= Cov \\{ X_a (t + u), X_b (t) \\} \\\\\n  &= Cov \\{ X_a (u), X_b (0) \\}\n\\end{align}\n\\qquad(11.5)\\]\nThe auto-covariance function is often rescaled to yield the autocorrelation function \\(\\rho_{\\bullet, \\bullet}(\\cdot)\\).\n\\[\n\\begin{align}\n\\rho_{\\bullet, \\bullet} (u) &= \\left \\{ \\rho_{a, b} (u) \\right \\}_{a, b = 1}^d \\\\\n  \\\\\n  \\text{where} \\\\\n  \\rho_{a, b} &= cor \\{ X_a (u), X_b (0) \\} \\\\\n  &= \\frac{\\gamma_{a, b} (u)}{\\sqrt{\\gamma_{a, a} (0) \\; \\gamma_{b, b} (0)}}\n\\end{align}\n\\qquad(11.6)\\]",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#operations-on-time-series",
    "href": "ts-intro.html#operations-on-time-series",
    "title": "11  Time Series Data Analysis",
    "section": "11.6 Operations on Time Series",
    "text": "11.6 Operations on Time Series\nThe data transformations used on independent observations are applicable to time series data. For example, in the case of Johnson and Johnson quarterly earnings, we saw that the transformed time series, \\(Y(t) = \\log_e (X(t))\\), could be more closely approximated by a linear trend.\nIn addition to the data transformations used on independent observations, we often want to average time series data over time.\nAs an example, Figure 12.1 shows a 5-year moving average of the global temperature data previously shown. This operation smooths out local fluctuations to show the overall trend more clearly.\n\n\n\n\n\n\n\n\nFigure 11.11: Global temperatures: 5-year moving average\n\n\n\n\n\nIn this case the 5-year moving average operation can be expressed as follows.6\n\\[\n\\begin{align}\n  Y_\\bullet (t)\n  &= \\frac{1}{5} \\sum_{u = -2}^2 X_\\bullet (t - u)\n\\end{align}\n\\qquad(11.7)\\]\nHere \\(X_\\bullet (\\cdot)\\) is a two-component random process representing annual global averages of land and ocean temperatures. This 5-year moving average is an example of a linear, time-invariant filter, having the following general form.\n\\[\n\\begin{align}\n  Y_\\bullet (t)\n  &= a_{\\bullet, \\bullet} (\\cdot) \\; * \\; X_\\bullet (\\cdot)  \\\\\n  &= \\sum_u a_{\\bullet, \\bullet} (u) \\; \\times \\; X_\\bullet (t - u)\n\\end{align}\n\\qquad(11.8)\\]\nwhere \\(a_{\\bullet, \\bullet} (\\cdot)\\) is a sequence of \\(d \\times d\\) matrices and \\(*\\) denotes discrete convolution.\nFor example, here’s the 5-year moving average above expressed in this format.\n\\[\n\\begin{align}\n  a_{\\bullet, \\bullet} (u) &=\n    \\begin{cases}\n      \\frac{1}{5} I & \\text{ for } |u| \\le 2 \\\\\n      0 & \\text{ for } |u| &gt; 2\n    \\end{cases}\n\\end{align}\n\\qquad(11.9)\\]\nAs noted above, the moving average operation can help to reveal and thus model the trend in the process, \\(m_\\bullet (t)\\). Forming the residual series \\(X_\\bullet (t) - m_\\bullet (t)\\) is called “de-trending”.\nAlternatively, if the trend is linear it may also be removed by forming differences of the current observation minus its predecessor. This is an application of the “differencing” operator \\(\\nabla = \\mathcal{I} - \\mathcal{B}\\).\n\\[\n\\begin{align}\n  Y_\\bullet (t)\n  &= X_\\bullet (t)  - X_\\bullet (t - 1) \\\\\n  &= (\\mathcal{I} - \\mathcal{B}) \\{ X_\\bullet (\\cdot) \\} (t) \\\\\n  &= \\nabla \\{ X_\\bullet (\\cdot) \\} (t)\n\\end{align}\n\\qquad(11.10)\\]\nIn the DJIA stock index example, returns were defined as the log-ratio of successive closing values, calculated as the difference in successive values of the logarithm of the closing values. Here’s the differencing operator \\(\\nabla\\) in the filtering format.\n\\[\n\\begin{align}\n  a_{\\bullet, \\bullet} (u) &=\n    \\begin{cases}\n      I  & \\text{ for } u = 0 \\\\\n      -I & \\text{ for } u = 1 \\\\\n      0  & \\text{ otherwise }\n    \\end{cases}\n\\end{align}\n\\qquad(11.11)\\]\nA moving average operation smooths the time series on which it operates. That is, it allows low-frequency components to pass, while diminishing high-frequency components. Therefore the moving average operation is categorized as a low-pass filter. The differencing operator can be categorized as a high-pass filter.\nIn signal-processing applications a linear, time-invariant filter \\(a_{\\bullet, \\bullet} (\\cdot)\\) may be designed to extract certain types of signals that are contaminated by noise.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#data-analysis-objectives-methods",
    "href": "ts-intro.html#data-analysis-objectives-methods",
    "title": "11  Time Series Data Analysis",
    "section": "11.7 Data Analysis Objectives & Methods",
    "text": "11.7 Data Analysis Objectives & Methods\nThere are a number of ways to categorize types of data analysis. For example, the distinction between confirmatory and exploratory data analysis has been widely discussed over several decades. But for present purposes we consider the complementary aims of decision support and scientific understanding.\n\n11.7.1 Decision Support\nDecisions are often made via complex human interactions that are difficult to represent formally. Financial transactions, however, are easier to record and model. Financial management operates at markedly different time scales. At one extreme, consider algorithms that respond in real time to changing market valuations. At the other extreme we have decisions pertaining to multi-year projects. The financial stakes warrant substantial investment, which has fueled steady advancement in all aspects of the technology, including time series modeling. To be useful the models must be as precise and as reliable as possible (with precision and reliability balanced as deemed appropriate in each situation).\nTime series models that support decisions (in any setting) are typically time-domain parametric models whose components may include data transformations along with filtering algorithms. We leave the description of such models for a separate technical note.\n\n\n11.7.2 Scientific Understanding\nEnhancing human understanding of natural and social phenomena can be gratifying in itself, but may also pertain directly or indirectly to human welfare. The natural sciences offer many examples of phenomena that are usefully decomposed by frequency-domain time series models. Again, description of these methods is left to a separate technical note.\n\n\n11.7.3 Overlap\nOf course societal decisions often require both action-oriented recommendations and scientific understanding of what is known and unknown pertaining to the decision. Decisions related to climate change are a leading example of this overlap.\nTo illustrate, Figure 11.12 below shows monthly \\(CO_2\\) levels (ppm) taken at the Mauna Loa, Hawaii observatory (top) and a second-order differencing of the data to remove trends (bottom).\n\n\n\n\n\n\n\n\nFigure 11.12: \\(CO_2\\): Mauna Loa, monthly readings\n\n\n\n\n\nFigure 12.8 shows a 5-year forecast based on a time-domain model (a seasonal auto-regressive, moving-average model).\n\n\n\n\n\n\n\n\nFigure 11.13: \\(CO_2\\): Mauna Loa, monthly forecast\n\n\n\n\n\nWe leave the development of such models to a separate technical note. The point here is that such forecasts, when presented to support high-stakes societal decisions, are both valued for their precision and scrutinized as to their scientific reliability.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#detecting-serial-correlation",
    "href": "ts-intro.html#detecting-serial-correlation",
    "title": "11  Time Series Data Analysis",
    "section": "11.8 Detecting Serial Correlation",
    "text": "11.8 Detecting Serial Correlation\nThe process of building a statistical model (time series or not) is typically cyclic: examining data (or model residuals); conjecturing a statistical model (or model refinement); fitting the conjectured model; and examining the model residuals.\nIn time series analysis, an initial review of the data might include a test for serial correlation (autocorrelation). That is, one can check for correlation of \\(X_\\bullet (t)\\) with \\(X_\\bullet (t - s)\\) at several lags \\(s\\). If the data do appear to be auto-correlated, one might proceed to build a time series model. If the residuals of that model also appear to be auto-correlated or to exhibit some trend, one might seek to refine the model until the model residuals appear to be free of trend and autocorrelation.\n\n11.8.1 Statistical Tests\nThere are several statistical tests for autocorrelation. One of the most widely used is the Ljung-Box test(ljung1978?) available in R as the function stats::Box.test() in which one specifies the maximal lag to be checked.\nIn the frequency domain, the variation exhibited in the data is decomposed into the variation from groups of sinusoidal functions, each group corresponding to a frequency band. From this perspective the absence of autocorrelation corresponds to each band contributing the same proportion of variance, a phenomenon called “white noise”. A test for such white noise is available in R as the function hwwntest::bartlettB()(hwwntest?).\n\n\n11.8.2 Autocorrelation and Estimator Variance\nThe presence of autocorrelation in the data requires adjustment of the usual variance calculations designed for data in which observations are statistically independent. Here’s an example.\nSuppose that \\(W(t)\\) is a white noise process of independent and identically distributed (“iid”) random variables, and that \\(X(\\cdot)\\) is the following filtered version of \\(W(\\cdot)\\).\n\\[\n\\begin{align}\n  X(t) &= W(t) + \\theta \\; W(t-1) \\\\\n  \\\\\n  & \\text{where } -1 &lt; \\theta &lt; 1\n\\end{align}\n\\qquad(11.12)\\]\nThen \\(X(\\cdot)\\) is an example of a moving average random process. In the theory of such processes one usually assumes that \\(X(\\cdot)\\) is observed for some interval of time \\((t_0, \\ldots, t_0 + T - 1)\\) and that \\(W(\\cdot)\\) is not observed. But for present purposes suppose that this is a simulation study in which both \\(X(t)\\) and \\(W(\\cdot)\\) are generated over the same time interval and that the values of all parameters are known, including the population means, the population variances, and \\(\\theta\\).\nLet’s focus on the calculation of the sample mean and its variance. For \\(W(\\cdot)\\) we have the following familiar formulas for the sum of observed values.\n\\[\n\\begin{align}\n  S_{W, T} &= \\sum_{t = t_0}^{t_0 + T -1} W(t) \\\\\n  E \\left \\{ S_{W, T} \\right \\} &= T \\times E \\left \\{ W(t_0) \\right \\}  \\\\\n  &= T \\times \\mu_W \\\\\n  Var \\left \\{ S_{W, T} \\right \\} &= T \\times Var \\left \\{ W(t_0) \\right \\}  \\\\\n  &= T \\times \\sigma_W^2\n\\end{align}\n\\qquad(11.13)\\]\nThe sample mean is derived from the sample sum.\n\\[\n\\begin{align}\n  \\hat{\\mu}_W &= \\frac{S_{W, T}}{T} \\\\\n  \\\\\n  E \\left \\{ \\hat{\\mu}_W \\right \\}\n  &= \\frac{E \\left \\{ S_{W, T} \\right \\}}{T} \\\\\n  &= \\mu_W \\\\\n  \\\\\n  Var \\left \\{ \\hat{\\mu}_W \\right \\}\n  &= \\frac{Var \\left \\{ S_{W, T} \\right \\}}{T^2} \\\\\n  &= \\frac{ \\sigma_W^2 }{T}\n\\end{align}\n\\qquad(11.14)\\]\nThe mean and variance of \\(X(t)\\) can be expressed as respective functions of the mean and variance of \\(Z(t)\\).\n\\[\n\\begin{align}\n  \\mu_X &= (1 +\\theta) \\; \\mu_Z \\\\\n  \\sigma_X^2 &= (1 +\\theta^2) \\; \\sigma_Z^2\n\\end{align}\n\\qquad(11.15)\\]\nThe variance formula for the sample sum, \\(S_{X, T}\\), differs from that of \\(S_{Z, T}\\), because \\(X(\\cdot)\\) is auto-correlated. The auto-covariance function is\n\\[\n\\begin{align}\n  \\gamma_X (u) &= Cov \\left ( X(t + u), X(t) \\right ) \\\\\n  &=\n  \\begin{cases}\n    (1 +\\theta^2) \\; \\sigma_Z^2 & \\text{ for } u = 0 \\\\\n    \\theta \\; \\sigma_Z^2 & \\text{ for } u = \\pm 1 \\\\\n    0 & \\text{ for } |u| &gt; 1\n  \\end{cases}\n\\end{align}\n\\qquad(11.16)\\]\nThe auto-correlation function is thus7\n\\[\n\\begin{align}\n  \\rho_X (u) &= cor \\left ( X(t + u), X(t) \\right ) \\\\\n  &=\n  \\begin{cases}\n    1 & \\text{ for } u = 0 \\\\\n    \\frac{\\theta}{1 +\\theta^2} & \\text{ for } u = \\pm 1 \\\\\n    0 & \\text{ for } |u| &gt; 1\n  \\end{cases}\n\\end{align}\n\\qquad(11.17)\\]\nTherefore\n\\[\n\\begin{align}\n  Var \\left \\{ S_{X, T} \\right \\}\n  &= Cov \\left ( \\sum_{s = t_0}^{t_0 + T - 1} X(s), \\; \\sum_{t = t_0}^{t_0 + T - 1} X(t)  \\right ) \\\\\n  &= \\sum_{s = t_0}^{t_0 + T - 1} \\sum_{y = t_0}^{t_0 + T - 1}\n  Cov \\left ( X(s), X(t) \\right ) \\\\\n  &= \\sum_{s = t_0}^{t_0 + T - 1} \\sum_{y = t_0}^{t_0 + T - 1}\n  \\gamma_X (s - t) \\\\\n  &= \\sum_{u = -T + 1}^{T - 1} (T - |u|) \\; \\gamma_X (u) \\\\\n  &= T \\; \\gamma_X (0) + 2 (T - 1) \\; \\gamma_X (1) \\\\\n  &= T \\; \\sigma_X^2 \\; \\left ( 1 + 2 \\; (1 - \\frac{1}{T}) \\; \\rho_X (1) \\right ) \\\\\n  &= T \\; \\sigma_X^2 \\; \\left ( 1 + 2 \\; (1 - \\frac{1}{T}) \\; \\frac{\\theta}{1 +\\theta^2} \\right ) \\\\\n  &= T \\; \\sigma_X^2 \\; \\frac{(1 + \\theta)^2}{1 +\\theta^2}\n  - \\sigma_X^2 \\frac{2 \\theta}{1 +\\theta^2}\n\\end{align}\n\\qquad(11.18)\\]\nIn short, ignoring the autocorrelation of \\(X(\\cdot)\\) would lead to erroneous calculations of the variance of \\(S_{X, T}\\) and \\(\\hat{\\mu}_X\\).\n\n\n11.8.3 Effective Sample Size (ESS)\nThe magnitude of such erroneous calculations can be represented by a quantity called the effective sample size (ESS). The calculation of ESS varies with application8, but here’s the basic idea in the present context. Consider a finite sequence of independent draws from the distribution of the random variable \\(X(t_0)\\), thereby producing a sample of independent observations. Define the effective sample size, \\(N_d\\), to be the number of draws whose sample mean has the same variance as that of the auto-correlated case.\n\\[\n\\begin{align}\n  \\frac{\\sigma_X^2}{N_d} &= Var \\left \\{ \\hat{\\mu}_X \\right \\} \\\\\n  &= \\frac{ Var \\left \\{ S_{X, T} \\right \\} }{T^2}\n\\end{align}\n\\qquad(11.19)\\]\nEquivalently\n\\[\n\\begin{align}\n  N_d &= \\frac{T^2 \\sigma_X^2}{ Var \\left \\{ S_{X, T} \\right \\} }\n\\end{align}\n\\qquad(11.20)\\]\nIn our moving average example this yields\n\\[\n\\begin{align}\n  N_d &= \\frac{T^2 \\sigma_X^2}{ Var \\left \\{ S_{X, T} \\right \\} } \\\\\n  &\\sim\n  \\begin{cases}\n    T \\; \\frac{1 +\\theta^2}{(1 + \\theta)^2}\n      & \\text{ as } T \\rightarrow \\infty \\\\  \n    T^2  & \\text{ as } \\theta \\rightarrow -1\n  \\end{cases}\n\\end{align}\n\\qquad(11.21)\\]\nAs \\(\\theta\\) increases from \\(0\\) towards \\(1\\) the matching number of independent draws \\(N_d\\) decreases from \\(T\\) towards \\(T/2\\). In the other direction, as \\(\\theta\\) decreases from \\(0\\) towards \\(-1\\) the matching number of independent draws \\(N_d\\) increases from \\(T\\) towards \\(T^2\\).\nThe point here is that care must be taken when calculating the variance of estimators derived from time series data. This is a known issue addressed by available software.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#closing-remarks",
    "href": "ts-intro.html#closing-remarks",
    "title": "11  Time Series Data Analysis",
    "section": "11.9 Closing Remarks",
    "text": "11.9 Closing Remarks\nThis technical note is intended as an overview of time series analysis based on the textbook (shumway2025?) cited below along with the supporting R package astsa (astsa?). The reader should feel free to select the presented examples of greatest interest, skipping others to save time.\nIn this note, time-domain and frequency-domain methods are mentioned only in the broadest terms, leaving details for other presentations.\nFor a more comprehensive listing of relevant R packages see the time series task view available from CRAN, the Comprehensive R Archive Network.\nSee Regression and Time Series Primer for a more detailed introduction to the topics presented here. And see Fun with asta for an introduction to R package astsa.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#glossary-of-math-symbols",
    "href": "ts-intro.html#glossary-of-math-symbols",
    "title": "11  Time Series Data Analysis",
    "section": "11.10 Glossary of Math Symbols",
    "text": "11.10 Glossary of Math Symbols\n$$ \\[\\begin{align}\n  {a_{\\bullet, \\bullet} (u)}\n    \\quad & \\quad \\text{matrix of filter coefficients at time-shift } u \\\\\n    \\\\\n  \\mathcal{B}\n     \\quad & \\quad \\text{back-shift operator } \\\\\n    \\\\\n  \\nabla\n     \\quad & \\quad \\text{difference operator } \\mathcal{I - B} \\\\\n    \\\\\n  \\gamma_{\\bullet, \\bullet} (u)\n     \\quad & \\quad \\text{auto-covariance matrix at time-shift } u \\\\\n    \\\\\n  \\mathcal{I}\n     \\quad & \\quad \\text{identity operator } \\\\\n    \\\\\n  m_\\bullet (t)\n     \\quad & \\quad E \\{ X_\\bullet (t) \\} \\text{, expected value of } X_\\bullet (t) \\\\\n    \\\\\n  \\mu_X\n     \\quad & \\quad E\\{ X (t_0) \\} \\text{, expected value of } X (t_0) \\\\\n    \\\\   \n  \\hat{\\mu} _X\n     \\quad & \\quad \\text{sample mean, } S_{X, T} / T \\\\\n    \\\\\n  N_d\n     \\quad & \\quad \\text{number of independent draws (effective sample size) } \\\\\n    \\\\\n  \\rho_{\\bullet, \\bullet} (u)\n     \\quad & \\quad \\text{auto-correlation matrix at time-shift } u \\\\\n     \n  S_{X, T}\n     \\quad & \\quad \\text{the sample sum of T observations of } X (\\cdot) \\\\\n    \\\\\n  \\sigma_X^2\n     \\quad & \\quad Var \\{ X (t_0) \\} \\text{, variance of } X (t_0) \\\\\n    \\\\\n  T\n     \\quad & \\quad \\text{number of time series observations } \\\\\n    \\\\\n  t_0\n     \\quad & \\quad \\text{initial time index of time series observations } \\\\\n    \\\\\n  X_\\bullet (t)\n     \\quad & \\quad (X_1 (t), \\ldots, X_d (t)) \\text{, a multivariate random process }\n\\end{align}\\] $$",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#references",
    "href": "ts-intro.html#references",
    "title": "11  Time Series Data Analysis",
    "section": "11.11 References",
    "text": "11.11 References\n\n\n\n\nVenables, W. N., and B. D. Ripley. 2002. “Modern Applied Statistics with s.” https://www.stats.ox.ac.uk/pub/MASS4/.",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "ts-intro.html#footnotes",
    "href": "ts-intro.html#footnotes",
    "title": "11  Time Series Data Analysis",
    "section": "",
    "text": "In rare cases time series methods are applied to data indexed not by time but rather by some other measure, e.g. distance. See for example (thomson1977?).↩︎\nThere are two prominent R packages representing the Old Faithful measurements during August, 1985. The MASS::geyser data set of 299 observations (Venables and Ripley 2002) includes nocturnal measurements whose duration was coded as 2, 3 or 4 minutes, having originally been described as ‘short’, ‘medium’ or ‘long’. The datasets::faithful data set of 272 measurements, shown in the figure, excludes 27 of these nocturnal measurements.↩︎\nThe data source is NOAA, who use the term “anomaly” rather than “deviation”. Further clarifications are pending.↩︎\nThe return is here calculated as \\(log_e\\) of the ratio: current day’s closing price divided by that of the preceding day. The median value is about 6 basis points.↩︎\nThe assumption of second-order (or wide-sense) stationarity suffices for most time series models used in practice, but some cases may call for the assumption of strict stationarity. This means that for any finite set of times \\((t_1, \\ldots, t_K)\\) and any time-shift \\(s\\), the joint probability distribution of \\((X_\\bullet (t_1), \\ldots, X_\\bullet (t_K))\\) is identical to that of \\((X_\\bullet (t_1 - s), \\ldots, X_\\bullet (t_K - s))\\).↩︎\nThe example illustrates a simple moving average. A weighted moving average may have non-negative coefficients that sum to unity. The term “moving average” is also used more broadly to mean a component of a time series model that filters an uncorrelated noise sequence.↩︎\nNote that \\(\\rho_X (1)\\) is restricted to the interval \\((-0.5, 0.5)\\), since we have restricted \\(\\theta\\) to the interval \\((-1, 1)\\).↩︎\nThe R package astsa provides function ESS().↩︎",
    "crumbs": [
      "Time Series Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series Data Analysis</span>"
    ]
  },
  {
    "objectID": "la-intro.html#footnotes",
    "href": "la-intro.html#footnotes",
    "title": "6  Linear Regression",
    "section": "",
    "text": "To be more precise, every data variable qualifies as a feature vector, but a feature vector may also be some function of the data and other information.↩︎\n“Best” in the sense of minimizing the sum of squared residuals of actual minus predicted sons’ heights.↩︎\nThe OECD (Organisation for Economic Co-operation and Development) works with 100+ countries to collect and analyze data in order to promote public policy. The OECD’s 38 Member countries span the world, from North America and South America to Europe and Asia-Pacific.↩︎\nSee LeCun, Cortes, and Burges (2005) and “MNIST Database | Wikipedia” (2025).↩︎\nSee “Multinomial Logistic Regression | Wikipedia” (2025).↩︎\nThe conversion of a matrix of pixels to a vector of pixels is known as raster-to-vector (R2V) conversion, usually in row-major format, whereby the elements of the vector are taken from the top row and then from each succeeding row. See “Raster Graphics | Wikipedia” (2025).↩︎\nSee “One-Hot | Wikipedia” (2025) and “Categorical Variable | Wikipedia” (2025).↩︎\nThe inner product of vectors \\(x, y \\in \\mathcal{V}\\) has alternative notations, including \\(x \\boldsymbol\\cdot y\\), \\(\\left &lt; x, y \\right &gt;\\), and \\(x^\\top y\\).↩︎\nWe assume in this chapter that feature matrix \\(X\\) has only numeric columns, possibly as the result of selecting from and transforming a larger collection of numeric and non-numeric features.↩︎\nMatrix \\(X\\) on the right side of Equation 6.19 is called a feature matrix that may contain original data columns (other than the response or labeling variable) and may also contain columns that are functions of the data or of other information. A data matrix is model-agnostic, whereas a feature matrix is constructed to support a model of some form.↩︎\nThere are various notations for the subspace spanned by the columns of matrix \\(X_{\\bullet, \\bullet}\\) including \\(\\mathcal{C}(X_{\\bullet, \\bullet})\\), \\(\\mathcal{Im}(X_{\\bullet, \\bullet})\\) (for the image of \\(X_{\\bullet, \\bullet}\\)), and \\(\\mathcal{R}(X_{\\bullet, \\bullet})\\) (for the range of \\(X_{\\bullet, \\bullet}\\)).↩︎\nSee “Hamming Distance | Wikipedia” (2025) and “Levenshtein Distance | Wikipedia” (2025).↩︎\nThe residual son’s height is the error term in the regression formula. In the figure, residuals are color-coded according to their sign: black if positive and red otherwise.↩︎\nThe regression plane qualifies as a linear manifold, mathematically speaking.↩︎\nThe cited value of the correlation coefficient \\(r_{m, s}\\) pertains to the random sample of size 20 created to facilitate a detailed view of regression residuals. Among all 179 families whose oldest child was a son, we have \\(r_{m, s} \\approx 0.3\\).↩︎",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "la-intro.html#geometry-of-fitting-models-to-data",
    "href": "la-intro.html#geometry-of-fitting-models-to-data",
    "title": "6  Linear Algebra for Fitting Models to Data",
    "section": "6.2 Geometry of Fitting Models to Data",
    "text": "6.2 Geometry of Fitting Models to Data\nThe preceding section introduced example data sets along with corresponding models of the following form.\n\\[\n\\begin{align}\n  y &= X \\; \\beta \\; + \\; \\epsilon\n\\end{align}\n\\qquad(6.19)\\]\nEach of the elements of Equation 6.19 has alternative names, including the following. 8\n\\[\n\\begin{align}\n  y &= \\text{a } \\textit{response, target,} \\text{ or } \\textit{labeling}  \\text{ variable} \\\\\n  X &= \\text{columns of } \\textit{explanatory, predictor,} \\text{ or } \\textit{feature}  \\text{ variables} \\\\\n  \\beta &= \\text{a vector of model } \\textit{coefficients} \\text{ or } \\textit{parameters} \\\\\n  \\epsilon &= \\text{an } \\textit{error} \\text{ or } \\textit{residual} \\text{ term}\n\\end{align}\n\\qquad(6.20)\\]\nThis linear regression format follows the more general mathematical notation \\(y = f(x)\\). In data science and machine learning, however, the response variable \\(y\\) and the feature matrix \\(X\\) have known values, whereas \\(\\beta\\) and \\(\\epsilon\\) are determined or evaluated over the course of the modeling process.\nIn the data examples of the preceding section, the response variable took the following form.\n\nFamily heights: \\(y =\\) oldest child’s height\nBetter Life Index: \\(y =\\) the Life Satisfaction indicator\nMNIST: \\(y =\\) a probability vector \\(\\{ P(D = \\nu) \\}_{\\nu = 0}^9\\) assigned to each image\n\nThe MNIST example illustrates a vector-valued rather than scalar-valued response variable.\nIf the data include a labeling or response variable, \\(y\\), then the problem is said to be supervised. In unsupervised problems (that lack a \\(y\\) variable), we may need to find patterns in the given data. For example we may seek those feature variables (columns of the feature matrix \\(X\\)), or linear combinations of feature variables, that account for most of the variability in the entire set of feature variables. Or we may need to find observations (rows of the feature matrix \\(X\\)) that are similar and thereby form groups (or clusters) of observations. In these unsupervised situations we may model the data matrix (or covariance matrix) as the product of other matrices of special form (to be discussed later in this chapter).\nIn the remainder of this chapter we will focus on ideas and methods that help us to solve Equation 6.19, or rather, that help us to determine the value of \\(\\beta\\) that minimizes (in some sense) the residual term \\(\\epsilon\\).\n\n\n\n\n“Categorical Variable | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Categorical_variable.\n\n\nLeCun, Yann, Corinna Cortes, and Christopher J. C. Burges. 2005. “The MNIST Database of Handwritten Digits.” https://web.archive.org/web/20200430193701/http://yann.lecun.com/exdb/mnist/.\n\n\n“MNIST Database | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/MNIST_database.\n\n\n“Multinomial Logistic Regression | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Multinomial_logistic_regression.\n\n\n“One-Hot | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/One-hot.\n\n\n“Raster Graphics | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Raster_graphics.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Algebra for Fitting Models to Data</span>"
    ]
  },
  {
    "objectID": "la-intro.html#data-examples",
    "href": "la-intro.html#data-examples",
    "title": "6  Linear Regression",
    "section": "",
    "text": "6.1.1 Heights of Parents and Oldest Child\nIn 1885 Sir Francis Galton examined the heights (in inches) of parents and their adult children to determine the strength of evidence to support height as a hereditary trait. The corresponding R data set HistData::GaltonFamilies consists of 934 adult children from a total of 205 families. Restricting attention to the oldest child in each family, there were 26 daughters and 179 sons.\nThe table below shows a portion of this data matrix. Each row represents a family and consists of: a family identifier, the father’s height, the mother’s height, the oldest child’s height, and the oldest child’s gender.\n\n\n\n\nTable 6.1: Family heights in inches: father, mother, oldest child\n\n\n\n\nFamily heights: father, mother, oldest child\n\n\nfamily\nfather\nmother\nchild\ngender\n\n\n\n\n001\n78.5\n67.0\n73.2\nmale\n\n\n002\n75.5\n66.5\n73.5\nmale\n\n\n003\n75.0\n64.0\n71.0\nmale\n\n\n004\n75.0\n64.0\n70.5\nmale\n\n\n005\n75.0\n58.5\n72.0\nmale\n\n\n006\n74.0\n68.0\n69.5\nfemale\n\n\n\n\n\n\n\n\nThe figure below represents all the families, with the gender of the oldest child distinguished by color: red for daughters and blue for sons.\n\n\n\n\n\n\n\n\nFigure 6.1: Height of oldest child: daughters (red), sons (blue)\n\n\n\n\n\nIn Chapter 2 we regressed the son’s height on the father’s height. We obtained the regression line, which approximates the graph of averages: the average son’s height per father’s height. The linear regression can be interpreted as a linear prediction of the height of a son whose father is of some given height.\nWe can now expand on this idea by regressing the son’s height on the heights of both the mother and the father. This is a model in which the predicted son’s height, \\(\\hat{s}\\), is some constant plus some linear combination of the parents’ heights.\n\\[\n\\begin{align}   \n  \\hat{s} & = \\mathcal{l}_{R}(m, f) \\\\   \n  &= \\beta_0 \\; + \\; \\beta_m \\times m \\; + \\; \\beta_f \\times f\n\\end{align}\n\\qquad(6.1)\\]\nwhere\n\\[\n\\begin{align}\n  \\hat{s} &= \\text{predicted height of son} \\\\\n  m &= \\text{height of mother}  \\\\\n  f &= \\text{height of father}\n\\end{align}\n\\qquad(6.2)\\]\nEach set of coefficient values determines some plane in the 3-dimensional space of (mother, father, son) heights. The coefficients \\((\\hat{\\beta}_0, \\hat{\\beta}_m, \\hat{\\beta}_f)\\) obtained by linear regression determine the regression plane (Figure 6.2) that gives the best linear approximation \\((\\hat{s})\\) to the son’s height for a given pair of parent heights \\((m, f)\\). 2\n\n\n\n\n\n\n\n\nFigure 6.2: Son’s height given (mother, father) heights: predicted (plane) and observed (point)\n\n\n\n\n\nIn vector-matrix notation we are seeking a vector \\((\\hat{\\beta}_0, \\hat{\\beta}_m, \\hat{\\beta}_f)\\) of coefficient values that yields the least-squares solution to the following linear approximation problem.\n\\[\n\\begin{align}\n  s_\\bullet &\\approx (1_\\bullet, m_\\bullet, f_\\bullet) \\times\n    \\begin{pmatrix}\n      \\hat{\\beta}_0 \\\\ \\hat{\\beta}_m \\\\ \\hat{\\beta}_f\n    \\end{pmatrix}\n\\end{align}\n\\qquad(6.3)\\]\nwhere\n\\[\n\\begin{align}\n  s_\\bullet &= \\text{data column vector: heights of sons} \\\\\n  1_\\bullet &= \\text{column vector } (1, \\ldots, 1) \\\\\n  m_\\bullet &= \\text{data column vector: heights of mothers}  \\\\\n  f_\\bullet &= \\text{data column vector: heights of fathers}\n\\end{align}\n\\qquad(6.4)\\]\nThis is a statistical estimation problem that corresponds to the following linear algebra problem and notation.\n\\[\n\\begin{align}\n  b_\\bullet &\\approx A_{\\bullet, \\bullet} \\times x_\\bullet\n\\end{align}\n\\qquad(6.5)\\]\nwhere\n\\[\n\\begin{align}\n  b_\\bullet &= s_\\bullet \\\\\n  A_{\\bullet, \\bullet} &= (1_\\bullet, m_\\bullet, f_\\bullet) \\\\\n  x_\\bullet &= (\\hat{\\beta}_0, \\hat{\\beta}_m, \\hat{\\beta}_f)\n\\end{align}\n\\qquad(6.6)\\]\nIt turns out that the least squares solution \\((\\hat{\\beta}_0, \\hat{\\beta}_m, \\hat{\\beta}_f)\\) can be obtained as the vector of coefficients of an orthogonal projection of vector \\(s_\\bullet\\) onto the 3-dimensional subspace spanned by vectors \\((1_\\bullet, m_\\bullet, f_\\bullet)\\). More on this later.\n\n\n6.1.2 Survey Data: Better Life Index\nWe now turn to a data set having several data columns, namely the OECD’s Better Life Index (BLI). 3 The following table shows a portion of the data.\n\n\n\nTable 6.2: Better Life Index (BLI)\n\n\n\n# A tibble: 42 × 26\n  code  country     CG_SENG CG_VOTO EQ_AIRP EQ_WATER ES_EDUA ES_EDUEX ES_STCS\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 AUS   Australia       2.7      92     6.7       92      84       20     499\n2 AUT   Austria         1.3      76    12.2       92      86       17     491\n3 BEL   Belgium         2        88    12.8       79      80       19     500\n4 BRA   Brazil          2.2      80    11.7       70      57       16     400\n5 CAN   Canada          2.9      68     7.1       90      92       17     517\n6 CHE   Switzerland     2.3      45    10.1       96      89       17     498\n# ℹ 36 more rows\n# ℹ 17 more variables: HO_HISH &lt;dbl&gt;, HS_LEB &lt;dbl&gt;, HS_SFRH &lt;dbl&gt;,\n#   IW_HADI &lt;dbl&gt;, IW_HNFW &lt;dbl&gt;, JE_EMPL &lt;dbl&gt;, JE_LMIS &lt;dbl&gt;, JE_LTUR &lt;dbl&gt;,\n#   JE_PEARN &lt;dbl&gt;, PS_FSAFEN &lt;dbl&gt;, PS_REPH &lt;dbl&gt;, SC_SNTWS &lt;dbl&gt;,\n#   SW_LIFS &lt;dbl&gt;, WL_EWLH &lt;dbl&gt;, WL_TNOW &lt;dbl&gt;, HO_BASE &lt;dbl&gt;, HO_NUMR &lt;dbl&gt;\n\n\n\n\nEach row of this data matrix gives specified measurements of an identified country. The first two columns give, respectively, each country’s OECD code and name. The remaining 24 columns are measures pertaining to the well-being of the populace.\nThe column name of each measures consists of a two-letter prefix followed by a suffix. The prefix is associated with a broad indicator of social well-being. The suffix pertains to a particular component of this indicator. Here is an expansion of these prefixes.\n\n\n\n\nTable 6.3: BLI Indicators and Sub-Components\n\n\n\n\nBLI Indicators and Sub-Components\n\n\n\n\n\n\n\n\nprefix\nname\nn_comps\ncomponents\n\n\n\n\nCG\nCivic Engagement\n2\nCG_SENG, CG_VOTO\n\n\nEQ\nEnvironmental Quality\n2\nEQ_AIRP, EQ_WATER\n\n\nES\nEducation System\n3\nES_EDUA, ES_EDUEX, ES_STCS\n\n\nHO\nHousing\n3\nHO_BASE, HO_HISH, HO_NUMR\n\n\nHS\nHealth Status\n2\nHS_LEB, HS_SFRH\n\n\nIW\nIncome and Wealth\n2\nIW_HADI, IW_HNFW\n\n\nJE\nJobs Employment\n4\nJE_EMPL, JE_LMIS, JE_LTUR, JE_PEARN\n\n\nPS\nPersonal Safety\n2\nPS_FSAFEN, PS_REPH\n\n\nSC\nSocial Connections\n1\nSC_SNTWS\n\n\nSW\nSubjective Well-Being\n1\nSW_LIFS\n\n\nWL\nWork Life Balance\n2\nWL_EWLH, WL_TNOW\n\n\n\n\n\n\n\n\nThe component indicators (corresponding to the suffix of the column name) are elaborated in the following table.\n\n\n\n\nTable 6.4: BLI Component Indicators\n\n\n\n\nBLI Component Indicators\n\n\n\n\n\n\n\n\n\nprefix\nsuffix\nunit\nname\ndescription\n\n\n\n\nCG\nSENG\nAVSCORE\nStakeholder Engagement\nExtent to which people can engage with government in rule-making\n\n\nCG\nVOTO\nPC\nVoter Turnout\nPercent of registered voters who voted in recent elections\n\n\nEQ\nAIRP\nMICRO_M3\nAir Pollution\nConcentration of PM2.5 particulate matter (micrograms per cubic meter)\n\n\nEQ\nWATER\nPC\nWater Quality\nPercent satisfied with water quality\n\n\nES\nEDUA\nPC\nEducational Attainment\nPercent aged 25-64 with at least upper-secondary education\n\n\nES\nEDUEX\nYR\nExpected Years of Education\nExpected years of schooling\n\n\nES\nSTCS\nAVSCORE\nStudent Cognitive Skills\nPISA scores in reading, mathematics, and science\n\n\nHO\nBASE\nPC\nDwellings w/o Basic Facilities\nPercentage of dwellings that lack basic sanitary facilities\n\n\nHO\nHISH\nPC\nHousing Expenditure\nPercentage of household gross adjusted disposable income spent on housing\n\n\nHO\nNUMR\nRATIO\nRooms per Person\nNumber of rooms per person in dwelling\n\n\nHS\nLEB\nYR\nLife Expectancy at Birth\nAverage number of years a person can expect to live\n\n\nHS\nSFRH\nPC\nSelf-Reported Health\nPercentage who report being in good or very good health\n\n\nIW\nHADI\nUSD\nHousehold Adjusted Disposable Income\nAverage household income after taxes\n\n\nIW\nHNFW\nUSD\nHousehold Net Financial Wealth\nHousehold net financial wealth (financial assets minus liabilities)\n\n\nJE\nEMPL\nPC\nEmployment Rate\nPercentage of people aged 15-64 in paid employment\n\n\nJE\nLMIS\nPC\nLabour Market Insecurity\nExpected loss of earnings if someone becomes unemployed\n\n\nJE\nLTUR\nPC\nLong-Term Unemployment Rate\nPercentage unemployed for 12+ months\n\n\nJE\nPEARN\nUSD\nPersonal Earnings\nAverage annual earnings per full-time employee\n\n\nPS\nFSAFEN\nPC\nFeeling Safe Walking Alone at Night\nPercentage who feel safe\n\n\nPS\nREPH\nRATIO\nHomicide Rate\nDeaths per 100,000 people\n\n\nSC\nSNTWS\nPC\nSupport Network Quality\nPercentage who believe they have someone to rely on in times of need\n\n\nSW\nLIFS\nAVSCORE\nLife Satisfaction\nAverage self-evaluation on a scale from 0 to 10\n\n\nWL\nEWLH\nPC\nEmployees Working Long Hours\nPercentage of employees working 50+ hours per week\n\n\nWL\nTNOW\nHOUR\nTime Devoted to Leisure and Personal Care\nHours per day spent on leisure, personal care, eating, and sleeping\n\n\n\n\n\n\n\n\nThe unit column in the above table gives the unit of measure, with PC meaning percent, YR meaning number of years, and so on.\nWe now turn to a statistical and algebraic treatment of the BLI data matrix of Table 6.2. Consider the indicator component SW_LIFS (Life Satisfaction) as a response variable, with the remaining 23 indicator components serving as explanatory variables. As with the previous data example, we want to approximate or predict the response variable by a constant \\(\\beta_0\\) plus a linear combination of the explantory variables, as follows.\n\\[\n\\begin{align}\n  L_\\bullet &\\approx (1_\\bullet, C_{\\bullet, 1}, \\ldots, C_{\\bullet, d}) \\times\n    \\begin{pmatrix}\n      \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\vdots \\\\ \\hat{\\beta}_d\n    \\end{pmatrix}\n\\end{align}\n\\qquad(6.7)\\]\nwhere\n\\[\n\\begin{align}\n  L_\\bullet &= \\text{life satisfaction indicator per country} \\\\\n  1_\\bullet &= \\text{column vector } (1, \\ldots, 1) \\\\\n  C_{\\bullet, k} &= k^{th} \\text{ indicator component per country} \\\\\n  d &= \\text{number of explanatory indicators}\n\\end{align}\n\\qquad(6.8)\\]\nWe now have more explanatory variables than in the previous example, a fact that merits some comment.\nOn the one hand, the approach to determining least-squares regression coefficients \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_d\\) is unchanged. We project the response vector, now \\(L_\\bullet\\), onto the space spanned by the constant vector \\(1_\\bullet\\) along with the explanatory variables, that is onto the space spanned by \\((1_\\bullet, C_{\\bullet, 1}, \\ldots, C_{\\bullet, d})\\). The fitted coefficients yield a function of the explanatory variables that forms a regression hyperplane of dimension 23 that passes through a cloud of data points, \\((C_{\\bullet, 1}, \\ldots, C_{\\bullet, d}, L_\\bullet)\\), in a space of dimension 24.\nOn the other hand, we are now estimating 24 regression coefficients based on observations from just 42 countries. From a statistical perspective, this paucity of observations relative to the number of estimates leads to large standard errors for the set of estimated coefficients. From the perspective of numerical linear algebra, the vector of fitted coefficients \\((\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_d)\\) is less stable (more sensitive to error in the data) than it was in the previous example.\n\n\n6.1.3 MNIST: Images of Handwritten Digits\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten decimal digits consisting of 60,000 training images and 10,000 testing images. 4\nThe history of this database goes back to 1988, when the US Postal Service constructed images of digits appearing on handwritten zip codes. Around the same time the US Census Bureau requested NIST to evaluate optical character recognition (OCR) systems. In 1992, NIST and the Census Bureau sponsored a competition in which participating teams were given images of Handwriting Sample Forms (HSFs), including handwritten decimal digits. The initial version of MNIST was constructed sometime before summer 1994.\nHere’s an example of each handwritten digit from the training set of images.\n\n\n\n\n\n\n\n\nFigure 6.3: Example images of handwritten digits from the MNIST dataset\n\n\n\n\n\nEach image is represented by a \\(28 \\times 28\\) matrix of pixels, with each pixel represented as a grayscale integer value from 0 through 255. That is, each image represents a single vector in a space of dimension 784 (since \\(28 \\times 28 = 784\\)).\nThe 1992 competition prompted the development of algorithms to determine the decimal digit represented by any such image. This is a classification problem: to label each case of data (image) as belonging to one of several possible categories (decimal digits).\nOne such method, multinomial logistic regression, assigns a probability that a given image represents a specified digit, resulting in a 10-element probability vector per image. 5\n\n6.1.3.1 Multinomial Logistic Regression\nTo formulate the model, we convert the representation of an image from a \\(28 \\times 28\\) matrix of pixels into a vector of pixels of length 784. 6 We’ll denote such a vector as \\((P_1, \\ldots, P_d)\\), where \\(d = 784\\).\nLet \\(D\\) denote the digit represented by the image. The ordering of the digits from 0 through 9 is not directly relevant to the image-recognition problem, so let us regard \\(D\\) as a categorical variable having the set \\(\\{ 0, 1, \\ldots, 9 \\}\\) as possible values. An alternative representation is the set of indicator vectors \\(e_0 = (1, 0, \\ldots, 0)\\) through \\(e_9 = (0, 0, \\ldots, 1)\\), called “one-hot encoding” in machine learning. 7\nThen the multinomial logistic regression model can be formulated as follows.\n\\[\n\\begin{align}\n  \\log_e{ \\frac{P(D = \\nu)}{P(D = 0)} } &= (1, P_1, \\ldots, P_d) \\times\n    \\begin{pmatrix}\n      \\beta_0^{(\\nu)} \\\\ \\beta_1^{(\\nu)} \\\\ \\vdots \\\\ \\beta_d^{(\\nu)}\n    \\end{pmatrix} & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.9)\\]\nwith\n\\[\n\\begin{align}\n  P(D = 0) &= 1 - \\sum_{\\nu = 1}^9 P(D = \\nu)\n\\end{align}\n\\qquad(6.10)\\]\nFor a more compact notation let \\(X_{\\bullet} = (1, P_1, \\ldots, P_d)\\) and let \\(\\beta_{\\bullet}^{(\\nu)} = (\\beta_0^{(\\nu)}, \\beta_1^{(\\nu)}, \\ldots, \\beta_d^{(\\nu)})\\), with the inner product 8 of these two vectors denoted as \\(X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}\\). Then we have\n\\[\n\\begin{align}\n  \\log_e{ \\frac{P(D = \\nu)}{P(D = 0)} } &= X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)} & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.11)\\]\nExponentiation of Equation 6.11 gives:\n\\[\n\\begin{align}\n  \\{ P(D = \\nu) \\} &= \\{ P(D = 0) \\} \\times e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}} & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.12)\\]\nTaking the sum over \\(\\nu\\) we have:\n\\[\n\\begin{align}\n  \\sum_{\\nu = 1}^9 {P(D = \\nu)} &= \\{ P(D = 0) \\} \\times \\sum_{\\nu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}}\n\\end{align}\n\\qquad(6.13)\\]\nNow applying Equation 6.10 we have\n\\[\n\\begin{align}\n  \\left \\{ 1 - P(D = 0) \\right \\} &= \\{ P(D = 0) \\} \\times \\sum_{\\nu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}}\n\\end{align}\n\\qquad(6.14)\\]\nwhich yields:\n\\[\n\\begin{align}\n  P(D = 0) &= \\frac{1} { 1 + \\sum_{\\nu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}} }\n\\end{align}\n\\qquad(6.15)\\]\nApplying Equation 6.12 gives:\n\\[\n\\begin{align}\n  P(D = \\nu) &= \\frac{ e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}} } { 1 +  \\sum_{\\mu = 1}^9 e^{X_{\\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\mu)}} } & \\text{ for } \\nu \\in \\{ 1, \\ldots, 9 \\}\n\\end{align}\n\\qquad(6.16)\\]\n\n\n6.1.3.2 Matrix Representation\nEquation 6.11 pertains to the probability that a single image represents a single digit \\(\\nu \\in \\{1, \\ldots, 9 \\}\\). Therefore, in a data set of \\(n\\) images, with \\(i\\) denoting the index of a particular image, we have:\n\\[\n\\begin{align}\n  \\log_e{ \\frac{P(D_i = \\nu)}{P(D_i = 0)} } &= X_{i, \\bullet} \\boldsymbol\\cdot \\beta_{\\bullet}^{(\\nu)}\n\\end{align}\n\\qquad(6.17)\\]\nExpanding the last equation to matrix notation, with \\(i\\) as the row index and \\(\\nu\\) as a column index, we have\n\\[\n\\begin{align}\n&\n\\begin{pmatrix}\n  \\log_e{ \\frac{P(D_1 = 1)}{P(D_1 = 0)} }, & \\ldots, & \\log_e{ \\frac{P(D_1 = 9)}{P(D_1 = 0)} } \\\\\n  \\vdots & \\vdots & \\vdots \\\\\n  \\log_e{ \\frac{P(D_n = 1)}{P(D_n = 0)} }, & \\ldots, & \\log_e{ \\frac{P(D_n = 9)}{P(D_n = 0)} }\n\\end{pmatrix}  \\\\ \\\\\n&=\n\\begin{pmatrix}\n  X_{1, \\bullet} \\\\\n  \\vdots \\\\\n  X_{n, \\bullet}\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\beta_{\\bullet}^{(1)}, & \\ldots, & \\beta_{\\bullet}^{(9)}\n\\end{pmatrix}\n\\end{align}\n\\qquad(6.18)\\]\nThe matrix on the left side of Equation 6.18 has dimensions \\(n \\times 9\\). On the right side, the first matrix factor has dimensions \\(n \\times 785\\), and the second matrix factor has dimensions \\(785 \\times 9\\).",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "la-intro.html#notation",
    "href": "la-intro.html#notation",
    "title": "6  Linear Regression",
    "section": "6.2 Notation",
    "text": "6.2 Notation\nThe preceding section introduced example data sets along with corresponding linear regression models of the following form (the generic linear model). 9\n\\[\n\\begin{align}\n  y &= X \\; \\beta \\; + \\; \\epsilon\n\\end{align}\n\\qquad(6.19)\\]\nEach of the elements of Equation 6.19 has alternative names, including the following. 10\n\\[\n\\begin{align}\n  y &= \\text{a } \\textit{response, target,} \\text{ or } \\textit{labeling}  \\text{ variable} \\\\\n  X &= \\text{feature matrix of } \\textit{explanatory, predictor,} \\text{ or } \\textit{feature}  \\text{ variables} \\\\\n  \\beta &= \\text{a vector of model } \\textit{coefficients} \\text{ or } \\textit{parameters} \\\\\n  \\epsilon &= \\text{an } \\textit{error} \\text{ or } \\textit{residual} \\text{ term}\n\\end{align}\n\\qquad(6.20)\\]\nHere’s the linear regression model in matrix format for a given feature matrix of specified dimensions.\n\\[\n\\begin{align}\n  y_\\bullet &= X_{\\bullet, \\bullet} \\; \\beta_\\bullet \\; + \\; \\epsilon_\\bullet\n\\end{align}\n\\qquad(6.21)\\]\nLet the dimensions of \\(X_{\\bullet, \\bullet}\\) be given as \\(n\\) rows by \\(d\\) columns. Then the column vectors \\((y_\\bullet, \\epsilon_\\bullet)\\) are each of length \\(n\\), and the number of rows in column-vector \\(\\beta_\\bullet\\) is \\(d\\). Let us also delineate the columns of \\(X_{\\bullet, \\bullet}\\), the “feature vectors”, as follows.\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet} &= \\left ( x_{\\bullet, 1}, \\ldots, x_{\\bullet, d} \\right )\n\\end{align}\n\\qquad(6.22)\\]\nThe \\(n-\\)dimensional feature vectors \\(\\{ x_{\\bullet, k} \\}_{k = 1}^d\\) span a subspace, “feature space”, within \\(n-\\)space, denoted \\(span(x_{\\bullet, 1}, \\ldots, x_{\\bullet, d})\\). Since this is the subspace generated by the columns of matrix \\(X_{\\bullet, \\bullet}\\), the subspace is also denoted as \\(col(X_{\\bullet, \\bullet})\\). 11\nThe mapping \\(\\beta_\\bullet \\mapsto X_{\\bullet, \\bullet} \\; \\beta_\\bullet\\) sends a vector of coefficients \\(\\beta_\\bullet\\) to the following linear combination within the feature subspace.\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet} \\; \\beta_\\bullet &=\\beta_1 \\; x_{\\bullet, 1} \\; + \\; \\cdots \\; + \\; \\beta_d \\; x_{\\bullet, d}\n\\end{align}\n\\qquad(6.23)\\]\nThe dimension of the feature subspace is called the rank of \\(X_{\\bullet, \\bullet}\\), denoted \\(rank(X_{\\bullet, \\bullet})\\). The rank can be no greater than \\(n\\), the dimension of the space containing each feature vector, nor can it be greater than \\(d\\), the number of feature vectors. If the feature vectors are linearly independent then this subspace is of dimension \\(d\\). If the feature vectors are not linearly independent (for example, if \\(d &gt; n\\)) then the feature subspace is of some smaller dimension.\nThis linear regression format follows the more general mathematical notation \\(y = f(x)\\). In data science and machine learning, however, the response variable \\(y\\) and the feature matrix \\(X\\) have known values, whereas \\(\\beta\\) and \\(\\epsilon\\) are fit (determined or evaluated based on \\(y\\) and \\(X\\)) over the course of the modeling process.\nIn the data examples of the preceding section, the response variable took the following form.\n\nFamily heights: \\(y =\\) oldest child’s height\nBetter Life Index: \\(y =\\) the Life Satisfaction indicator\nMNIST: \\(y =\\) a probability vector \\(\\{ P(D = \\nu) \\}_{\\nu = 0}^9\\) assigned to each image\n\nThe MNIST example illustrates a vector-valued rather than scalar-valued response variable.\nIf the data include a labeling or response variable, \\(y\\), then the problem is said to be supervised. In unsupervised problems (that lack a \\(y\\) variable), we may need to find patterns in the given data. For example we may seek those feature variables (columns of the feature matrix \\(X\\)), or linear combinations of feature variables, that account for most of the variability in the entire set of feature variables. Or we may need to find observations (rows of the feature matrix \\(X\\)) that are similar and thereby form groups (or clusters) of observations. In these unsupervised situations we may model the feature matrix (or its covariance matrix) as the product of other matrices of special form (to be discussed later in this chapter).\nIn the remainder of this chapter we will focus on ideas and methods that help us to solve Equation 6.19, or rather, that help us to determine the value of \\(\\beta\\) that minimizes (in some sense) the residual term \\(\\epsilon\\). We refer to this minimization as the linear regression problem, which is made precise once we specify the measure of \\(\\epsilon\\) to be minimized.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "la-intro.html#geometry",
    "href": "la-intro.html#geometry",
    "title": "6  Linear Regression",
    "section": "6.3 Geometry",
    "text": "6.3 Geometry\nWe now consider the geometry of the least-squares solution of the linear regression problem, using the example of family heights. We begin by defining this measure of the residual term \\(\\epsilon\\).\n\n6.3.1 Distance Measures\nThe sum-of-squares measure of the residual vector \\(\\epsilon_\\bullet = (\\epsilon_1, \\ldots, \\epsilon_n)\\) is simply the sum of the squares of the components \\(\\epsilon_\\nu\\).\n\\[\n\\begin{align}\n  \\sum_{\\nu = 1}^n | \\epsilon_\\nu |^2\n\\end{align}\n\\qquad(6.24)\\]\n\n6.3.1.1 Vector norms\nEquation 6.24 defines the following norm on \\(n-\\)dimensional Euclidean space. For any vector \\(v_\\bullet = (v_1, \\ldots, v_n) \\in \\mathbb{R}^n\\) we define \\(\\Vert v_\\bullet \\rVert_2\\) as follows.\n\\[\n\\begin{align}\n  \\Vert v_\\bullet \\rVert_2\n  &= \\left ( \\sum_{\\nu = 1}^n | v_\\nu |^2 \\right )^{\\frac{1}{2}}\n\\end{align}\n\\qquad(6.25)\\]\nThis norm can be derived from (or used to define) the inner-product of a pair of vectors \\(v_\\bullet, w_\\bullet \\in \\mathbb{R}^n\\), defined as follows with the following alternative notations.\n\\[\n\\begin{align}\n  \\left &lt;  v_\\bullet, w_\\bullet \\right &gt;\n  &= v_\\bullet \\boldsymbol\\cdot w_\\bullet \\\\\n  &= v_\\bullet^\\top w_\\bullet \\\\\n  &= \\sum_{\\nu = 1}^n v_\\nu \\; w_\\nu\n\\end{align}\n\\qquad(6.26)\\]\nThen we have\n\\[\n\\begin{align}\n  \\Vert v_\\bullet \\rVert_2^2\n  &= \\left &lt;  v_\\bullet, v_\\bullet \\right &gt;\n\\end{align}\n\\qquad(6.27)\\]\nMore generally, for any real number \\(p \\ge 1\\), the so-called \\(p-\\)norm (or Minkowski norm of order \\(p\\)) is defined as\n\\[\n\\begin{align}\n  \\Vert v_\\bullet \\rVert_p\n  &= \\left ( \\sum_{\\nu = 1}^n | v_\\nu |^p \\right )^{\\frac{1}{p}} & \\text{ for } 1 \\le p &lt; \\infty\n\\end{align}\n\\qquad(6.28)\\]\nThis definition can be extended to the case \\(p = \\infty\\) as follows.\n\\[\n\\begin{align}\n  \\Vert v_\\bullet \\rVert_\\infty\n  &= \\max \\left \\{ |v_\\nu | \\right \\}_{\\nu = 1}^n\n\\end{align}\n\\qquad(6.29)\\]\nMore generally, for \\(v, w \\in \\mathcal{V}\\), a real-valued or complex-valued vector space, a norm \\(\\Vert \\cdot \\rVert\\) is defined to have the following properties.\n\\[\n\\begin{align}\n  \\Vert v \\rVert &\\ge 0 \\\\\n  \\Vert v \\rVert &= 0 & \\text{ if and only if } v = 0 \\\\\n  \\Vert v + w \\rVert &\\le \\Vert v \\rVert + \\Vert w \\rVert \\\\\n  \\Vert \\lambda \\; v \\rVert &= | \\lambda | \\; \\Vert v \\rVert & \\text{ for any scalar } \\lambda\n\\end{align}\n\\qquad(6.30)\\]\n\n\n6.3.1.2 Matrix norms\nIf \\(\\lVert v_\\bullet \\rVert\\) denotes some defined norm for vectors \\(v_\\bullet \\in \\mathbb{R}^n\\) and if \\(M_{\\bullet, \\bullet}\\) is an \\(n \\times n\\) numeric matrix, then the vector norm defines a corresponding matrix norm \\(\\lVert M_{\\bullet, \\bullet} \\rVert\\) as follows.\n\\[\n\\begin{align}\n  \\lVert M_{\\bullet, \\bullet} \\rVert &=\n  \\sup_{ \\lVert v_\\bullet \\rVert = 1 }\n  \\left \\{ \\lVert  M_{\\bullet, \\bullet} \\; v_\\bullet \\rVert \\right \\}\n\\end{align}\n\\qquad(6.31)\\]\nNote that some matrix norms are defined otherwise. For example, the Frobenius norm, \\(\\lVert M_{\\bullet, \\bullet} \\rVert_F\\), is defined as \\(\\lVert vec(M_{\\bullet, \\bullet}) \\rVert_2\\), where \\(vec(M_{\\bullet, \\bullet})\\) converts an \\(n \\times n\\) matrix into a vector of length \\(n^2\\) by concatenating matrix columns.\n\n\n6.3.1.3 Metric spaces\nA vector norm \\(\\Vert \\cdot \\rVert\\) defines a corresponding distance measure \\(\\delta_{\\Vert \\cdot \\rVert}\\)\n\\[\n\\begin{align}\n  \\delta_{\\Vert \\cdot \\rVert} (v_\\bullet, w_\\bullet)\n  &= \\lVert v_\\bullet - w_\\bullet \\rVert\n\\end{align}\n\\qquad(6.32)\\]\nA general distance measure or metric, \\(\\delta (\\cdot, \\cdot)\\), together with the set of points \\(\\mathcal{M}\\) over which it is defined constitutes a metric space with the following properties, for any \\(m_1, m_2, m_3 \\in \\mathcal{M}\\).\n\\[\n\\begin{align}\n  \\delta (m_1, m_1) &= 0 \\\\\n  \\delta (m_1, m_2) &&gt; 0 & \\text{ whenever } m_1 \\ne m_2 \\\\\n  \\delta (m_1, m_2) &= \\delta (m_2, m_1) \\\\\n  \\delta (m_1, m_3) &\\le \\delta (m_1, m_2) \\; + \\; \\delta (m_2, m_3)\n\\end{align}\n\\qquad(6.33)\\]\nHamming distance and Levenshtein distance are important examples of metrics used in natural language processing (NLP) that are not based on a vector norm. 12\n\n\n\n6.3.2 Family heights\nApplying vector-matrix format of Equation 6.21 to the family heights data we have:\n\\[\n\\begin{align}\n  s_\\bullet &= (1_\\bullet, m_\\bullet, f_\\bullet) \\;\n  \\begin{pmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\beta_2\n  \\end{pmatrix} \\; + \\; \\epsilon_\\bullet\n\\end{align}\n\\qquad(6.34)\\]\nwhere\n\\[\n\\begin{align}\n  s_\\bullet &= \\text{heights of sons } \\\\\n  m_\\bullet &= \\text{heights of mothers } \\\\\n  f_\\bullet &= \\text{heights of fathers } \\\\\n  (1_\\bullet, m_\\bullet, f_\\bullet) &= \\text{feature matrix} \\\\\n  \\beta_\\bullet &= \\text{coefficient vector} = (\\beta_0, \\beta_1, \\beta_2)^\\top \\\\\n  \\epsilon_\\bullet &= \\text{residual vector}\n\\end{align}\n\\qquad(6.35)\\]\nConsider the least-squares estimate \\(\\hat{\\beta}_\\bullet\\) and the consequent predicted height \\(\\hat{y}_\\bullet = X_{_\\bullet, _\\bullet} \\hat{\\beta}_\\bullet\\) of the son. Figure 6.4 is based on a random sample of 20 families and shows the heights of sons on the vertical axis, along with their vertical displacement (residual) from the predicted value lying on the regression plane. 13\n\n\n\n\n\n\n\n\nFigure 6.4: Sampled son heights: residual = observed - predicted\n\n\n\n\n\nFigure 6.4 represents individual rows of data, \\(\\{ (m_i, f_i, s_i) \\}_{i = 1}^n\\) along with model predictions \\((\\hat{s} = \\hat{\\beta}_0 + \\hat{\\beta}_1 m + \\hat{\\beta}_2 f)\\) and residuals \\((\\hat{\\epsilon} = s - \\hat{s})\\) in three-dimensional \\((m, f, s)\\) space.\nTo gain more insight into linear regression we’ll first reduce the regression problem to the simple case in which the response variable and the predictor variables have all been coerced to have an average value of zero, a process called centering. This will eliminate the need for the intercept coefficient, \\(\\beta_0\\), and consequently eliminate the need to include the constant vector \\(1_\\bullet\\) in the feature matrix \\(X_{_\\bullet, _\\bullet}\\).\n\n\n6.3.3 Centering Data Vectors\nThe regression plane that we glimpse in Figure 6.4 actually spans all the \\((m, f)\\) combinations that are mathematically possible. If we imagine infinitesimally short parents with \\((m, f) = (0, 0)\\), the predicted height of their son would be \\(\\hat{\\beta}_0\\), which is not zero. That is, the plane does not pass through the origin \\((0, 0, 0)\\) and therefore does not qualify as a subspace of \\((m, f, s)\\) space. 14 But the regression plane determines a parallel subspace (that does pass through the origin).\nThe concept of a subspace is central to linear algebra. Therefore determining the subspace parallel to the regression plane will enable us to apply linear algebra methods to better understand linear regression.\nOne way to generate this subspace is to center each of the \\((m_i, f_i, s_i)\\) data values, that is, to replace data value \\(v_i\\) with its centered version \\(\\dot{v}_i = v_i - \\bar{v}\\), where \\(\\bar{v}\\) denotes the average value (arithmetic mean) of vector \\(v_\\bullet\\).\nIn vector-matrix notation we have\n\\[\n\\begin{align}\n  \\bar{v} &= \\frac{1}{n} \\sum_{\\nu = 1}^n v_\\nu \\\\\n  &= \\frac{1}{n} \\;  1_\\bullet^\\top \\; v_\\bullet \\\\ \\\\\n  & \\text{so that} \\\\ \\\\\n  \\dot{v}_\\bullet &= v_\\bullet - ( \\bar{v} \\; 1_\\bullet ) \\\\\n  &= v_\\bullet - \\frac{1}{n} \\;  1_\\bullet \\;  1_\\bullet^\\top \\; v_\\bullet \\\\\n  &= \\left ( I - \\frac{1}{n} \\;  1_\\bullet \\;  1_\\bullet^\\top \\right ) \\; v_\\bullet\n\\end{align}\n\\qquad(6.36)\\]\nLet \\(C_{\\bullet, \\bullet}\\) denote the matrix factor on the right side of the last equality, and define vector \\(\\tilde{1}_\\bullet\\) as follows.\n\\[\n\\begin{align}\n  \\tilde{1}_\\bullet &= \\frac{1}{\\sqrt{n}} \\;  1_\\bullet \\\\ \\\\\n  & \\text{so that} \\\\ \\\\\n  \\lVert \\tilde{1}_\\bullet \\rVert &= 1 \\\\ \\\\\n  & \\text{and} \\\\ \\\\\n  C_{\\bullet, \\bullet} &= I \\; - \\; \\tilde{1}_\\bullet \\; \\tilde{1}_\\bullet^\\top\n\\end{align}\n\\qquad(6.37)\\]\nThen we have\n\\[\n\\begin{align}\n  \\tilde{1}_\\bullet \\; \\tilde{1}_\\bullet^\\top \\;  v_\\bullet\n  &= \\bar{v} \\; 1_\\bullet \\\\ \\\\\n  C_{\\bullet, \\bullet} \\; v_\\bullet &= v_\\bullet \\; - \\; \\bar{v} \\; 1_\\bullet \\\\\n  &= \\dot{v}_\\bullet\n\\end{align}\n\\qquad(6.38)\\]\nSetting \\(v_\\bullet = 1_\\bullet\\) gives\n\\[\n\\begin{align}\n  \\left ( \\tilde{1}_\\bullet \\; \\tilde{1}_\\bullet^\\top \\right ) \\;  1_\\bullet\n  &= 1_\\bullet \\\\ \\\\\n  C_{\\bullet, \\bullet} \\; 1_\\bullet &= 0_\\bullet\n\\end{align}\n\\qquad(6.39)\\]\nWe now multiply both sides of Equation 6.21, the generic regression equation, by matrix \\(C_{\\bullet, \\bullet}\\) to obtain\n\\[\n\\begin{align}\n  C_{\\bullet, \\bullet} \\; y_\\bullet\n  &= C_{\\bullet, \\bullet} \\; X_{\\bullet, \\bullet} \\; \\beta_\\bullet\n  \\; + \\; C_{\\bullet, \\bullet} \\; \\epsilon_\\bullet\n\\end{align}\n\\qquad(6.40)\\]\nNow for the family heights data (Equation 6.34) we have\n\\[\n\\begin{align}\n  C_{\\bullet, \\bullet} \\; X_{\\bullet, \\bullet} \\; \\beta_\\bullet\n  &= C_{\\bullet, \\bullet} \\; (1_\\bullet, m_\\bullet, f_\\bullet) \\;\n  \\begin{pmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\beta_2\n  \\end{pmatrix} \\\\\n  &= (0_\\bullet, \\dot{m}_\\bullet, \\dot{f}_\\bullet) \\;\n  \\begin{pmatrix}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\beta_2\n  \\end{pmatrix} \\\\\n  &= \\beta_1 \\; \\dot{m}_\\bullet \\; + \\; \\beta_2 \\; \\dot{f}_\\bullet \\\\\n  &= (\\dot{m}_\\bullet, \\dot{f}_\\bullet) \\;\n  \\begin{pmatrix}\n    \\beta_1 \\\\\n    \\beta_2\n  \\end{pmatrix}\n\\end{align}\n\\qquad(6.41)\\]\nThen the centered version of Equation 6.34 is\n\\[\n\\begin{align}\n  \\dot{s}_\\bullet &= (\\dot{m}_\\bullet, \\dot{f}_\\bullet) \\;\n  \\begin{pmatrix}\n    \\beta_1 \\\\\n    \\beta_2\n  \\end{pmatrix} \\; + \\; \\dot{\\epsilon}_\\bullet\n\\end{align}\n\\qquad(6.42)\\]\nwhere\n\\[\n\\begin{align}\n  \\dot{s}_\\bullet &= \\text{centered heights of sons } \\\\\n  \\dot{m}_\\bullet &= \\text{centered heights of mothers } \\\\\n  \\dot{f}_\\bullet &= \\text{centered heights of fathers } \\\\\n  (\\dot{m}_\\bullet, \\dot{f}_\\bullet) &= \\text{centered feature matrix} \\\\\n  \\beta_\\bullet &= \\text{coefficient vector} = (\\beta_1, \\beta_2)^\\top \\\\\n  \\dot{\\epsilon}_\\bullet &= \\text{centered residual vector}\n\\end{align}\n\\qquad(6.43)\\]\nThat is, we can eliminate the intercept coefficient from the centered linear model, and we can also eliminate the constant vector \\(1_\\bullet\\) from the feature matrix \\(X_{_\\bullet, _\\bullet}\\). Figure 6.5 is a version of Figure 6.4 corresponding to Equation 6.42. Geometrically it’s the same figure, the difference being that each of the three axes has been shifted, now with 0 as the central value.\n\n\n\n\n\n\n\n\nFigure 6.5: Centered family heights\n\n\n\n\n\nThe advantage of centering the family heights data is that Figure 6.5 above represents all the dimensions of the centered linear model: the vector of responses \\(\\dot{s}_\\bullet\\) along with the feature vectors \\((\\dot{m}_\\bullet, \\dot{f}_\\bullet)\\). The disadvantage is that we have replaced heights with deviations from average heights, and those average heights can no longer be discerned from the scatter diagram.\nOn the other hand, prior to data-centering our feature matrix included the constant vector \\(1_\\bullet\\) (a vector neither interesting nor visible in our scatter diagrams) in order for the model to include a constant coefficient \\(\\beta_0\\) that accounted for the distinct average heights of (mother, father, son).\nThese are two representations of essentially the same linear model, as can be seen by reconstructing the original variables from their centered versions:\n\\[\n\\begin{align}\n  \\dot{s}_\\bullet &= (\\dot{m}_\\bullet, \\dot{f}_\\bullet) \\;\n  \\begin{pmatrix}\n    \\beta_1 \\\\\n    \\beta_2\n  \\end{pmatrix} \\; + \\; \\dot{\\epsilon}_\\bullet \\\\\n  &= \\beta_1 \\; \\dot{m}_\\bullet \\; + \\;\n     \\beta_2 \\; \\dot{f}_\\bullet \\; + \\;\n     \\dot{\\epsilon}_\\bullet \\\\ \\\\\n     &\\text{ or equivalently } \\\\ \\\\\n  s_\\bullet - \\bar{s} 1_\\bullet\n  &= \\beta_1 \\; (m_\\bullet - \\bar{m} 1_\\bullet) \\; + \\;\n     \\beta_2 \\; (f_\\bullet - \\bar{f} 1_\\bullet) \\; + \\;\n     \\dot{\\epsilon}_\\bullet \\\\ \\\\\n     &\\text{ so that } \\\\ \\\\\n  s_\\bullet\n  &= (\\bar{s} \\; - \\beta_1 \\;\\bar{m} \\; - \\beta_2 \\;\\bar{f}) \\; 1_\\bullet \\; + \\;\n     \\beta_1 \\; m_\\bullet \\; + \\;\n     \\beta_2 \\; f_\\bullet \\; + \\;\n     \\dot{\\epsilon}_\\bullet \\\\\n  &= \\tilde{\\beta}_0 \\; 1_\\bullet \\; + \\;\n     \\beta_1 \\; m_\\bullet \\; + \\;\n     \\beta_2 \\; f_\\bullet \\; + \\;\n     \\dot{\\epsilon}_\\bullet\n\\end{align}\n\\qquad(6.44)\\]\nIn words, the centered model, having only two free coefficients \\((\\beta_1, \\beta_2)\\), is equivalent to an uncentered model subject to the following constraints:\n\nthe constant coefficient \\(\\tilde{\\beta}_0\\) is a certain linear combination of the average heights of (mother, father, son) that uses coefficients \\((\\beta_1, \\beta_2)\\) and thereby forces the regression plane to pass through the (mathematical) point of averages \\((\\bar{m}, \\bar{f}, \\bar{s})\\); and\nthe residual vector is constrained to have an average value of zero.\n\n\n\n6.3.4 Least Squares Solutions\nWe now discuss the least-squares solution to the generic linear regression problem (Equation 6.21), which determines coefficient values \\(\\hat{\\beta}_\\bullet\\) that minimize the sum of squared residuals.\n\\[\n\\begin{align}\n  \\sum_{i = 1}^n \\epsilon_i^2 &= \\lVert \\epsilon_\\bullet \\rVert^2 \\\\\n  &= \\epsilon_\\bullet^\\top\\epsilon_\\bullet \\\\\n  &= (y_\\bullet - X_{\\bullet, \\bullet} \\beta_\\bullet)^\\top (y_\\bullet - X_{\\bullet, \\bullet} \\beta_\\bullet)\n\\end{align}\n\\qquad(6.45)\\]\nTo find coefficient values that minimize this sum of squares, one can take derivatives of the above expression with respect to \\(\\beta_\\bullet\\) and set that result to zero, which yields the following normal equations:\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet} \\; \\hat{\\beta}_\\bullet\n  &= X_{\\bullet, \\bullet}^\\top \\; y_\\bullet\n\\end{align}\n\\qquad(6.46)\\]\nOn the left side of the normal equations we have the matrix factor \\(\\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )\\). For the centered heights data this matrix factor is proportional to the (mother, father) covariance matrix, a \\(2 \\times 2\\) positive-definite matrix, and thus an invertible matrix. If this matrix factor is invertible, one can solve for \\(\\hat{\\beta}_\\bullet\\) as follows.\n\\[\n\\begin{align}\n  \\hat{\\beta}_\\bullet\n  &= \\left ( X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\; y_\\bullet\n\\end{align}\n\\qquad(6.47)\\]\nThe predicted vector \\(\\hat{y}_\\bullet\\) is thus:\n\\[\n\\begin{align}\n  \\hat{y}_\\bullet\n  &= X_{\\bullet, \\bullet} \\hat{\\beta}_\\bullet \\\\\n  &= X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top y_\\bullet\n\\end{align}\n\\qquad(6.48)\\]\n\n\n6.3.5 Orthogonal Projections\nAs previously noted (Equation 6.23), the mapping \\(\\beta_\\bullet \\mapsto X_{\\bullet, \\bullet} \\beta_\\bullet\\) sends coefficient vector \\(\\beta_\\bullet\\) to a linear combination of the feature vectors, which is therefore a vector within the feature subspace. The particular coefficient vector \\(\\hat{\\beta}_\\bullet\\) obtained by least squares linear regression produces the linear mapping of Equation 6.48:\n\\[\n\\begin{align}\n  \\hat{y}_\\bullet &= P \\; y_\\bullet\n\\end{align}\n\\qquad(6.49)\\]\nwhere \\(P\\) is the following matrix.\n\\[\n\\begin{align}\n  P\n  &= X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top\n\\end{align}\n\\qquad(6.50)\\]\nMatrix \\(P\\) is idempotent and symmetric, that is, both its square \\(P^2\\) and its transpose \\(P^\\top\\) equal \\(P\\) itself.\n\\[\n\\begin{align}\n  P^2\n  &= \\left \\{ X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\right \\}\n  \\left \\{ X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\right \\} \\\\\n  &= X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\\\\n  &= P \\\\ \\\\\n  P^\\top\n  &= \\left \\{ X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\right \\}^\\top \\\\\n  &=  X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\\\\n  &= P\n\\end{align}\n\\qquad(6.51)\\]\nIf a square matrix \\(M\\) is idempotent, that is, if \\(M^2 = M\\), then \\(M\\) represents a projection. Repeated applications of \\(M\\) to vector \\(v\\) return the initial application, i.e., \\(M^k v = Mv\\) for any positive integer \\(k\\).\nIf in addition matrix \\(M\\) is symmetric, that is, if \\(M^\\top = M\\), then \\(M\\) represents an orthogonal projection. In this case the complement of \\(M\\), \\(I - M\\), also qualifies as an orthogonal projection and the product of the two matrices is the zero matrix.\nConsequently, any vector \\(v\\) can be expressed as the sum of two vectors \\(v = x + y\\), with \\(x = M v\\) and \\(y = (I-M) v\\). Vector \\(x\\) belongs to the subspace spanned by the columns of \\(M\\), which is called the image or column-space of \\(M\\), denoted as \\(col (M)\\). Similarly \\(y \\in col (I - M)\\). Moreover, these two vectors are orthogonal: \\((x^\\top y = y^\\top x = 0)\\). That is, subspace \\(col (I - M)\\) is the orthogonal complement of subspace \\(col (M)\\).\nLet’s apply these ideas to matrix \\(P\\). First, we have shown that matrix \\(P\\) represents an orthogonal projection. On closer inspection, we can show that the subspace generated by \\(P\\), \\(col (P)\\), is the feature subspace. That is, for any vector \\(v_\\bullet\\) we have:\n\\[\n\\begin{align}\n  P \\; v_\\bullet &= X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\; v_\\bullet \\\\\n  &= X_{\\bullet, \\bullet} \\left \\{ \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\; v_\\bullet \\right \\} \\\\\n  &= X_{\\bullet, \\bullet} \\left \\{ \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1}\n  \\begin{pmatrix}\n    \\dot{m}_{\\bullet}^\\top \\; v_\\bullet \\\\\n    \\dot{f}_{\\bullet}^\\top \\; v_\\bullet\n  \\end{pmatrix}\n  \\right \\} \\\\\n  &= X_{\\bullet, \\bullet} \\; \\gamma_\\bullet (v_\\bullet)\n\\end{align}\n\\qquad(6.52)\\]\nIn words, for any vector \\(v_\\bullet\\) in \\(n-\\)space, \\(P\\) sends \\(v_\\bullet\\) to an \\(n-\\)vector of the form \\(X_{\\bullet, \\bullet} \\; \\gamma_\\bullet\\), which belongs to the feature subspace, \\(col(X_{\\bullet, \\bullet})\\).\nThis means that the respective vectors of predicted response values \\(\\hat{y_\\bullet}\\) and their residuals \\(\\hat{\\epsilon}_\\bullet\\) are orthogonal.\n\\[\n\\begin{align}\n  \\hat{y}_\\bullet &= P \\; y_\\bullet \\\\ \\\\\n  \\hat{\\epsilon}_\\bullet &= y_\\bullet - \\hat{y}_\\bullet \\\\\n  &= (I - P) \\; y_\\bullet \\\\ \\\\\n  \\hat{\\epsilon}_\\bullet^\\top \\; y_\\bullet &= y_\\bullet^\\top \\; (I - P)^\\top P \\; y_\\bullet \\\\\n  &= y_\\bullet^\\top \\; (I - P) \\; P \\; y_\\bullet \\\\\n  &= y_\\bullet^\\top \\; (P - P^2) \\; y_\\bullet \\\\\n  &= y_\\bullet^\\top \\; 0_{\\bullet, \\bullet} \\; y_\\bullet \\\\\n  &= 0\n\\end{align}\n\\qquad(6.53)\\]\nNow let \\(\\phi_\\bullet\\) be any vector in feature space. Then \\(\\phi_\\bullet\\) is some linear combination of the feature vectors and therefore can be represented as \\(\\phi_\\bullet = X_{\\bullet, \\bullet} \\gamma_\\bullet\\) for some coefficient vector \\(\\gamma_\\bullet\\). It now follows the \\(P \\; \\phi_\\bullet = \\phi_\\bullet\\):\n\\[\n\\begin{align}\n  P \\; \\phi_\\bullet &= P \\; (X_{\\bullet, \\bullet} \\gamma_\\bullet) \\\\\n  &= X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\;  (X_{\\bullet, \\bullet} \\gamma_\\bullet) \\\\\n  &= X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} \\left ( X_{\\bullet, \\bullet}^\\top \\;  X_{\\bullet, \\bullet} \\right ) \\gamma_\\bullet \\\\\n  &= X_{\\bullet, \\bullet} \\gamma_\\bullet \\\\\n  &= \\phi_\\bullet\n\\end{align}\n\\qquad(6.54)\\]\nConsequently, the residual vector \\(\\hat{\\epsilon}_\\bullet\\) is orthogonal to any vector \\(\\phi_\\bullet = X_{\\bullet, \\bullet} \\gamma_\\bullet\\) in the feature subspace:\n\\[\n\\begin{align}\n  \\hat{\\epsilon}_\\bullet^\\top \\; \\phi_\\bullet &= \\dot{y}_\\bullet^\\top \\; (I - P)^\\top \\phi_\\bullet \\\\\n  &= \\dot{y}_\\bullet^\\top \\; (I - P) \\; \\phi_\\bullet \\\\\n  &= \\dot{y}_\\bullet^\\top \\; 0_\\bullet \\\\\n  &= 0\n\\end{align}\n\\qquad(6.55)\\]\nIt now follows that of all vectors \\(\\phi_\\bullet = X_{\\bullet, \\bullet} \\gamma_\\bullet\\) in the feature subspace, the predicted vector \\(\\hat{y}_\\bullet\\) is closest to the given vector \\(y_\\bullet\\):\n\\[\n\\begin{align}\n  \\lVert y_\\bullet - \\phi_\\bullet \\rVert^2\n  &= \\lVert (y_\\bullet - \\hat{y}_\\bullet) + (\\hat{y}_\\bullet - \\phi_\\bullet) \\rVert^2  \\\\\n  &= \\lVert \\hat{\\epsilon}_\\bullet + (\\hat{y}_\\bullet - \\phi_\\bullet) \\rVert^2 \\\\\n  &= \\left ( \\hat{\\epsilon}_\\bullet + (\\hat{y}_\\bullet - \\phi_\\bullet) \\right )^\\top\n  \\left ( \\hat{\\epsilon}_\\bullet + (\\hat{y}_\\bullet - \\phi_\\bullet) \\right ) \\\\\n  &= \\hat{\\epsilon}_\\bullet^\\top \\hat{\\epsilon}_\\bullet \\; + \\; 0 \\; + \\; 0 \\; + \\; (\\hat{y}_\\bullet - \\phi_\\bullet)^\\top (\\hat{y}_\\bullet - \\phi_\\bullet) \\\\\n  &= \\lVert \\hat{\\epsilon}_\\bullet \\rVert^2 \\; + \\; \\lVert \\hat{y}_\\bullet - \\phi_\\bullet \\rVert^2 \\\\\n  &\\ge \\lVert \\hat{\\epsilon}_\\bullet \\rVert^2 \\\\\n  &= \\lVert y_\\bullet - \\hat{y}_\\bullet \\rVert^2\n\\end{align}\n\\qquad(6.56)\\]\nThere is one more point worth noting here. Suppose \\(X_{\\bullet, \\bullet}\\) consisted of just a single column, say \\(\\dot{m}_\\bullet\\), the centered heights of mothers.\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet} &= \\dot{m}_\\bullet \\\\ \\\\\n  X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}\n  &= \\dot{m}_\\bullet^\\top \\dot{m}_\\bullet \\\\\n  &= \\lVert \\dot{m}_\\bullet \\rVert^2\n\\end{align}\n\\qquad(6.57)\\]\nThen we would have:\n\\[\n\\begin{align}\n  P\n  &= X_{\\bullet, \\bullet} \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\\\\n  &= \\dot{m}_\\bullet\n    \\frac{1}{\\lVert \\dot{m}_\\bullet \\rVert^2} \\; \\dot{m}_\\bullet^\\top \\\\\n  &= \\left ( \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\right )\n     \\left ( \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\right )^\\top \\\\\n  &= u_\\bullet \\; u_\\bullet^\\top \\\\ \\\\\n  \\text{where} \\\\ \\\\\n  u_\\bullet &= \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\\\ \\\\\n  \\text{so that} \\\\ \\\\\n  \\lVert u_\\bullet \\rVert &= 1\n\\end{align}\n\\qquad(6.58)\\]\nThat is, projection matrix \\(P\\) can take the form of a 1-dimensional projection \\(u \\; u^\\top\\) onto multiples of unit vector \\(u\\), and generalizes such 1-dimensional projections when the feature space is of a higher dimension.\nFigure 6.5 above shows the result of projecting the centered sons’ heights to their predicted values in the parental plane (feature space). Each point in that figure represents an individual family, which corresponds to a single row of the centered feature matrix \\((\\dot{m}_\\bullet, \\dot{f}_\\bullet)\\). In the next section we introduce a different perspective on linear regression, namely a column-based view.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "la-intro.html#row-versus-column-visualization",
    "href": "la-intro.html#row-versus-column-visualization",
    "title": "6  Linear Algebra for Fitting Models to Data",
    "section": "6.4 Row versus Column Visualization",
    "text": "6.4 Row versus Column Visualization\n[TT: Try oblique axes in 2D to prepare for 3D case.]\n\n\nCorrelation: 0.097\n\n\nAngle between axes: 84.4 degrees\n\n\n\n\n\n\n\n\nFigure 6.6: gg-2025-10-25a\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.7: gg-2025-10-25a\n\n\n\n\n\n\n6.4.1 Least Squares: Two Perspectives\n\n\n6.4.2 Data Visualization\n\nThe 3D scatterplot view: each point is (mother_height, father_height, son_height)\nThe fitted regression plane: ŷ = β₀ + β₁·mother + β₂·father\nThis is how we visualize and interpret, but NOT where the projection occurs\nDimension: p = 3 (number of features including intercept)\n\n\n\n6.4.3 Sample Space\n\nEach observation is a vector in ℝⁿ (n = 179)\nThe response vector y ∈ ℝ¹⁷⁹ has components [y₁, y₂, …, y₁₇₉]ᵀ\nEach column of X is also a vector in ℝ¹⁷⁹\nExample: the “mother_heights” column is one vector in 179-dimensional space\n\n\n\n6.4.4 Column Space\n\nC(X) is the 3-dimensional subspace of ℝ¹⁷⁹ spanned by the columns of X\nAll possible linear combinations: Xβ for any β ∈ ℝ³\nKey insight: C(X) contains all predictions our model can make\nDimension: rank(X) ≤ min(n,p) = 3 (assuming full column rank)\n\n\n\n6.4.5 Orthogonal Projection\n\nŷ = Xβ̂ is the orthogonal projection of y onto C(X)\nThe residual vector e = y - ŷ is orthogonal to C(X)\nGeometric interpretation: ŷ is the point in C(X) closest to y (in L₂ distance)\nThe normal equations: XᵀX β̂ = Xᵀy arise from orthogonality condition Xᵀe = 0\nInclude a 2D schematic diagram: y, C(X) as a plane, ŷ as projection, e perpendicular\n\n\n\n6.4.6 Reconciling Perspectives\n\nThe 3D scatterplot shows relationships between variables (p-dimensional)\nThe projection happens in observation space (n-dimensional)\nBoth are valid and useful for different purposes\nThe regression coefficients β connect the two views\n\n\n\n6.4.7 Why This Matters\n\nDegrees of freedom: n - p (observations minus parameters)\nOverfitting: what happens when p approaches n?\nPreview: other norms (L₁) change the geometry but still live in ℝⁿ\n\n\n\n\n\n“Categorical Variable | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Categorical_variable.\n\n\nLeCun, Yann, Corinna Cortes, and Christopher J. C. Burges. 2005. “The MNIST Database of Handwritten Digits.” https://web.archive.org/web/20200430193701/http://yann.lecun.com/exdb/mnist/.\n\n\n“MNIST Database | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/MNIST_database.\n\n\n“Multinomial Logistic Regression | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Multinomial_logistic_regression.\n\n\n“One-Hot | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/One-hot.\n\n\n“Raster Graphics | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Raster_graphics.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Algebra for Fitting Models to Data</span>"
    ]
  },
  {
    "objectID": "la-intro.html#column-versus-row-visualization",
    "href": "la-intro.html#column-versus-row-visualization",
    "title": "6  Linear Regression",
    "section": "6.4 Column versus Row Visualization",
    "text": "6.4 Column versus Row Visualization\nContinuing with the example of centered heights, let’s now take a step back from two explanatory variables to just one, namely the mother’s centered height \\(\\dot{m}_\\bullet\\) as a predictor of the son’s centered height \\(\\dot{s}_\\bullet\\). From Equation 6.58 we have\n\\[\n\\begin{align}\n  P\n  &= \\left ( \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\right )\n     \\left ( \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\right )^\\top \\\\ \\\\\n  \\hat{\\beta}_\\bullet &= \\left ( X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet}  \\right )^{-1} X_{\\bullet, \\bullet}^\\top \\; \\dot{y}_\\bullet \\\\\n  &= \\frac{\\dot{m}_\\bullet^\\top \\; \\dot{y}}{\\lVert \\dot{m}_\\bullet \\rVert^2} \\\\\n  &= \\hat{\\beta}_1 \\\\ \\\\\n  \\text{so that} \\\\ \\\\\n  \\hat{\\dot{s}} &= P \\; \\dot{s} \\\\\n  &= \\left ( \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\right )\n     \\left ( \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\right )^\\top \\; \\dot{s} \\\\\n  &= \\frac{\\dot{m}_\\bullet^\\top \\; \\dot{s}}{\\lVert \\dot{m}_\\bullet \\rVert^2} \\; \\dot{m}_\\bullet \\\\\n  &= \\hat{\\beta}_1 \\; \\dot{m}_\\bullet\n\\end{align}\n\\qquad(6.59)\\]\nFigure 6.6 shows this projection from \\(\\dot{s}_\\bullet\\)) to the one-dimensional space spanned by \\(\\dot{m}_\\bullet\\).\n\n\n\n\n\n\n\n\nFigure 6.6: Centered heights: s-vector (sons) projected to to m-axis (mothers)\n\n\n\n\n\nThe coordinate system of this figure refers to the pair of basis vectors \\(( \\dot{m}_\\bullet, \\dot{s}_\\bullet )\\). If \\(v\\) is a vector in this two-dimensional space then \\(v\\) is some linear combination of the basis vectors.\n\\[\n\\begin{align}\n  v &= \\sigma \\; \\dot{s}_\\bullet \\; + \\; \\mu \\; \\dot{m}_\\bullet\n\\end{align}\n\\qquad(6.60)\\]\nThen \\(v\\) has coordinates \\((\\sigma, \\mu)\\) with respect to the \\(( \\dot{m}_\\bullet, \\dot{s}_\\bullet )\\) basis.\nConsequently the coordinates of vectors \\(\\dot{m}_\\bullet\\), \\(\\dot{s}_\\bullet\\), and \\(\\hat{\\dot{s}}_\\bullet\\) are respectively \\((1, 0)\\), \\((0, 1)\\), and \\((\\hat{\\beta}_1, 0)\\).\nNote that the \\(\\dot{s}_\\bullet\\) axis is not quite perpendicular to the \\(\\dot{m}_\\bullet\\) axis. That is because the two vectors are not orthogonal:\n\\[\n\\begin{align}\n  \\left &lt; \\dot{m}_\\bullet, \\; \\dot{s}_\\bullet \\right &gt; &= \\dot{m}_\\bullet^\\top \\; \\dot{s}_\\bullet \\\\\n  &\\ne 0\n\\end{align}\n\\qquad(6.61)\\]\nInstead we have the following non-zero correlation coefficient, denoted here as \\(r_{m, s}\\). 15\n\\[\n\\begin{align}\n  r_{m, s} &= \\left ( \\frac{\\dot{m}_\\bullet}{\\lVert \\dot{m}_\\bullet \\rVert} \\right )^\\top \\;\n  \\left ( \\frac{\\dot{s}_\\bullet}{\\lVert \\dot{s}_\\bullet \\rVert} \\right ) \\\\\n  &\\approx 0.1\n\\end{align}\n\\qquad(6.62)\\]\nIn linear algebra the expression for \\(r_{m, s}\\) is defined to be the cosine of the angle between vectors \\(\\dot{m}_\\bullet\\) and \\(\\dot{s}_\\bullet\\).\nTherefore the two axes are shown with the angle, say \\(\\theta_{m, s}\\), between the two drawn axes equal to the angle between the two actual vectors, \\(\\dot{m}_\\bullet\\) and \\(\\dot{s}_\\bullet\\). Thus \\(\\cos(\\theta_{m, s}) = r_{m, s}\\). Since the correlation coefficient is positive rather than zero, the cosine is also positive, which implies that \\(\\theta_{m, s} &lt; \\pi / 2\\).\nFigure 6.6 is a column-based view of the linear regression of the centered heights of the sons \\((\\dot{s}_\\bullet)\\) on the centered heights of their mothers \\((\\dot{m}_\\bullet)\\). The figure illustrates the simplicity of linear least-squares regression as, in essence, an orthogonal projection.\nFigure 6.7 below compares the more commonly used (row-based) illustration of the same linear regression.\n\n\n\n\n\n\n\n\n\n\n\n(a) Row\n\n\n\n\n\n\n\n\n\n\n\n(b) Col\n\n\n\n\n\n\n\nFigure 6.7: Centered heights (son ~ mother): Row and Column Views\n\n\n\nThe two perspectives on linear regression are complementary. Figure 6.7 (a) portrays individual rows of data (families here). This is the most common means of visualizing data in two dimensions, and with good reason: it can be very thought-provoking and thus useful for refining models. This view of the regression problem shows model results. On the other hand, the column-based perspective shown in Figure 6.7 (b) can help one to understand the model-fitting process.\nNow let’s see how these ideas carry over from 2D to 3D: we now regress \\(\\dot{s}\\) on \\((\\dot{m}, \\dot{f})\\). Figure 6.8 below shows this regression as an orthogonal projection of the \\(\\dot{s}_\\bullet\\) basis vector to the plane defined by the \\((\\dot{m}_\\bullet, \\dot{f}_\\bullet)\\) basis vectors. In this \\((\\dot{m}, \\dot{f}, \\dot{s})\\) coordinate system, the coordinates of the predicted (that is, projected) vector \\(\\hat{\\dot{s}}_\\bullet\\) are \\((\\hat{\\beta}_1, \\hat{\\beta}_2, 0)\\). The vector of residuals, \\(\\dot{s}_\\bullet - \\hat{\\dot{s}}_\\bullet\\), is represented by the dotted line orthogonal to the \\((\\dot{m}, \\dot{f})\\) plane.\nEach of these vectors represents the 20 families in the sample, but those 20 vector elements are not visible from this column-based perspective. The details of those 20 families, or more generally of individual data cases, are shown in row-based perspectives, like Figure 6.5. Such details are important and of interest, of course. But the column-based perspective also merits our attention. It illustrates the geometry of the model-fitting process, and the angle \\(\\theta\\) between two axes corresponds to the correlation \\(r\\) between the two variables \\((\\cos \\theta = r)\\). Figure 6.9 compares the two perspectives.\n\n\n\n\n\n\n\n\nFigure 6.8: Projection of centered heights: son to (mother, father) plane\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Row\n\n\n\n\n\n\n\n\n\n\n\n(b) Col\n\n\n\n\n\n\n\nFigure 6.9: Centered heights (son ~ mother + father): Row and Column Views",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "la-intro.html#re-centering",
    "href": "la-intro.html#re-centering",
    "title": "6  Linear Algebra for Fitting Models to Data",
    "section": "6.5 Re-Centering",
    "text": "6.5 Re-Centering\nRecall that we centered the family heights data in order to present least-squares linear regression as an orthogonal projection \\(P\\) of the vector \\(\\dot{s}_\\bullet\\) of sons’ centered heights onto the subspace spanned by \\((\\dot{m}_\\bullet, \\dot{f}_\\bullet)\\), the vectors of (mother, father) centered heights. Here are some points to keep in mind:\n\nOrthogonal projection applies to the original uncentered data, so long as the feature matrix \\(X_{\\bullet, \\bullet}\\) includes the constant vector \\(1_\\bullet\\).\nThe coefficient values \\((\\hat{\\beta}_1, \\hat{\\beta}_2)\\) fitted to the centered model \\(\\dot{s} \\approx \\beta_1 \\dot{m} \\; + \\; \\beta_2 \\dot{f}\\) are equal to the fitted coefficients appearing in the uncentered model \\(s \\approx \\beta_0 \\; + \\; \\beta_1 m \\; + \\; \\beta_1 f\\).\n\nTo elaborate, consider the original data along with a linear regression model that includes the constant coefficient \\(\\beta_0\\).\n\\[\n\\begin{align}\n  s & \\approx \\mathcal{l}_{\\beta_\\bullet} (m, f)  \\\\\n  &= \\beta_0 \\; + \\; \\beta_1 m \\; + \\; \\beta_2 f\n\\end{align}\n\\qquad(6.62)\\]\nEach set of coefficient values \\(\\beta_\\bullet = (\\beta_0, \\beta_1, \\beta_2)\\) generates a 2D regression plane, say \\(\\mathcal{L}_{\\beta_\\bullet}\\), within the 3D subspace spanned by the three \\(n-\\)vectors \\((m_\\bullet, f_\\bullet, s_\\bullet)\\).\n\\[\n\\begin{align}\n  \\mathcal{L}_{\\beta_\\bullet} &= \\mathcal{L}_{\\beta_\\bullet} (m, f, s) \\\\\n  &= \\left \\{ (m, f, s): s = \\mathcal{l}_{\\beta_\\bullet} (m, f) \\right \\}\n\\end{align}\n\\qquad(6.63)\\]\nExcluding coefficient \\(\\beta_0\\) from the model is equivalent to the constraint, \\(\\beta_0 = 0\\). This constraint restricts the regression plane to pass through the origin. The role of \\(\\beta_0\\) is to account for differences in the average heights of mothers, fathers, and sons, respectively, which we denote as \\((\\bar{m}, \\bar{f}, \\bar{s})\\). That is, the inclusion of \\(\\beta_0\\) enables us to set\n\\[\n\\begin{align}\n  \\tilde{\\beta}_0 &= \\bar{s} - (\\beta_1 \\bar{m} \\; + \\; \\beta_2 \\bar{f}) \\\\ \\\\\n  &\\text{to ensure} \\\\ \\\\\n  \\mathcal{l}_{\\beta_\\bullet} (\\bar{m}, \\bar{f}) &= \\tilde{\\beta}_0 \\; + \\; \\beta_1 \\bar{m} \\; + \\; \\beta_2 \\bar{f} \\\\\n  &= \\bar{s}\n\\end{align}\n\\qquad(6.64)\\]\nThat is, this formula for \\(\\tilde{\\beta}_0\\) ensures that the model maps parents of average height to a son of average height. A model of this form can be re-expressed as a model of centered heights.\n\\[\n\\begin{align}\n  & \\text{If} & \\tilde{\\beta}_0 &= \\bar{s} - (\\beta_1 \\bar{m} \\; + \\; \\beta_2 \\bar{f}) \\\\\n  & \\text{and} & s & \\approx \\tilde{\\beta}_0 \\; + \\; \\beta_1 m \\; + \\; \\beta_2 f  \\\\\n  & \\text{then} & s - \\bar{s} & \\approx \\beta_1 (m - \\bar{m}) \\; + \\; \\beta_2 (f - \\bar{f})\n\\end{align}\n\\qquad(6.65)\\]\nNow let’s return to the representation of the model using the feature matrix \\(X_{\\bullet, \\bullet}\\), the coefficient vector \\(\\beta_\\bullet\\), and the vector of recorded heights of mothers, fathers, and sons \\((m_\\bullet, f_\\bullet, s_\\bullet)\\). Then we have\n\\[\n\\begin{align}\n  s_\\bullet & \\approx \\mathcal{l}_{\\beta_\\bullet} (m_\\bullet, f_\\bullet)  \\\\\n  &= \\beta_0 1_\\bullet \\; + \\; \\beta_1 m_\\bullet \\; + \\; \\beta_2 f_\\bullet \\\\\n  &= (1_\\bullet, m_\\bullet, f_\\bullet) \\; \\beta_\\bullet \\\\\n  &= X_{\\bullet, \\bullet} \\; \\beta_\\bullet\n\\end{align}\n\\qquad(6.66)\\]",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Algebra for Fitting Models to Data</span>"
    ]
  },
  {
    "objectID": "la-intro.html#summary",
    "href": "la-intro.html#summary",
    "title": "6  Linear Regression",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nThis chapter treats linear regression from the perspective of linear algebra. The key points are:\n\nRow versus Column Views of the Data: Viewing the data is good practice, and is usually done by representing individual cases of data, that is, rows of \\((X, y)\\), where \\(X\\) is the feature matrix and \\(y\\) is the response variable (target or labeling vector). A complementary perspective is to examine the relationship between \\(y\\) and the columns of \\(X\\), the features, as vectors. Feature space is the subspace of \\(n-\\)dimensional vector space that is spanned by the columns of \\(X\\), and is denoted \\(col(X)\\). Linear models are typically formulated from the column perspective, but the results of model-fitting are usually presented in a row perspective.\nLeast-Squares Linear Regression is an Orthogonal Projection: The mapping of the response variable \\(y\\) to its value \\(\\hat{y}\\) predicted by a fitted linear model is the orthogonal projection of vector \\(y\\) to feature space, \\(col(X)\\).",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "la-intro.html#exercises",
    "href": "la-intro.html#exercises",
    "title": "6  Linear Regression",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\n\n6.6.1 Concepts\n\n6.6.1.1 Row vs Column Perspective\nConsider a dataset with \\(n = 5\\) observations and \\(d = 2\\) features plus a response variable.\n\nDescribe what “row space” means in the context of visualizing this data. What dimension is it?\nDescribe what “column space” or “feature space” means. What dimension is it?\nIn which space does the orthogonal projection for least-squares regression actually occur?\n\n\n\n6.6.1.2 \\(col(X_{\\bullet, \\bullet})\\)\nExplain in your own words why the fitted values \\(\\hat{y}_\\bullet\\) must lie in \\(col(X_{\\bullet, \\bullet})\\), the column space of the feature matrix.\n\n\n6.6.1.3 Orthogonality\n\nExplain why the residual vector \\(\\epsilon_\\bullet = y_\\bullet - \\hat{y}_\\bullet\\) is orthogonal to \\(col(X_{\\bullet, \\bullet})\\).\nWhat does this orthogonality imply about the relationship between \\(\\epsilon_\\bullet\\) and each column of \\(X_{\\bullet, \\bullet}\\)?\n\n\n\n6.6.1.4 Centering Data\nExplain why centering the feature vectors (subtracting the mean from each column of \\(X_{\\bullet, \\bullet}\\)) eliminates the need for an intercept term in linear regression. What happens geometrically when data is centered?\n\n\n\n6.6.2 Calculations\n\n6.6.2.1 1D Projection\nLet: \\(y_\\bullet^\\top = (3, 1, 2)\\) and \\(X_{\\bullet, \\bullet}^\\top = (x_{\\bullet, 1})^\\top = (1, 2, 2)\\).\n\nFind the orthogonal projection \\(\\hat{y}_\\bullet\\) of \\(y_\\bullet\\) onto feature space \\(col(X_{\\bullet, \\bullet})\\).\nCalculate the residual vector \\(\\epsilon_\\bullet = y_\\bullet - \\hat{y}_\\bullet\\).\nVerify that the residual vector is orthogonal to \\(col(X_{\\bullet, \\bullet})\\), that is, that \\(\\epsilon_\\bullet \\perp x_{\\bullet, 1}\\), by computing their inner product.\nSketch vectors \\((y_\\bullet, x_{\\bullet, 1}, \\hat{y}_\\bullet, \\epsilon_\\bullet)\\) in 3D space.\n\n\n\n6.6.2.2 Distance Measures\nFor the vectors \\(u_\\bullet = (1,2,3)\\) and \\(v_\\bullet = (4, 1, 2)\\):\n\nCompute the Euclidean \\((\\mathcal{l}_2)\\) distance between \\(u_\\bullet\\) and \\(v_\\bullet\\).\nCompute the Manhattan \\((\\mathcal{l}_1)\\) distance.\nWhich distance measure is affected more by outliers? Why?\n\n\n\n6.6.2.3 2D Projection\nLet:\n\\[\n\\begin{align}\ny_\\bullet &=\n  \\begin{pmatrix}\n    5 \\\\ 2 \\\\ 3 \\\\ 4\n  \\end{pmatrix} \\\\ \\\\\n  & \\text{ and } \\\\ \\\\\nX_{\\bullet, \\bullet} &=\n  \\begin{pmatrix}\n    1 & 0 \\\\\n    0 & 1 \\\\\n    1 & 1 \\\\\n    1 & 0\n  \\end{pmatrix}\n\\end{align}\n\\qquad(6.63)\\]\n\nFind the linear least-squares vector \\(\\hat{\\beta}_\\bullet\\) of regression coefficients by solving the set of normal equations \\(X_{\\bullet, \\bullet}^\\top X_{\\bullet, \\bullet} \\; \\hat{\\beta}_\\bullet = X_{\\bullet, \\bullet}^\\top \\; y_\\bullet\\).\nCalculate \\(\\hat{y} = X_{\\bullet, \\bullet} \\; \\hat{\\beta}_\\bullet\\).\nVerify that \\(\\hat{y}\\) is in \\(col(X_{\\bullet, \\bullet})\\) by expressing it as a linear combination of the columns of \\(X_{\\bullet, \\bullet}\\).\nCalculate \\(\\lVert \\epsilon_\\bullet \\rVert^2\\), where \\(\\epsilon_\\bullet = y_\\bullet - \\hat{y}_\\bullet\\).\n\n\n\n\n6.6.3 Programming\n\n6.6.3.1 Centering Heights\nUsing the Galton family heights data (HistData::GaltonFamilies):\n\nCenter the mother, father, and son height vectors by subtracting their means.\nFit a linear model predicting centered son height from centered mother and father heights (without an intercept term).\nCompare the coefficients from part (b) to those from an uncentered model with intercept. What changed and what stayed the same?\nVerify that the predictions from both models are identical.\n\n\n\n6.6.3.2 Visualizing Residuals\nFor the simple regression of son’s height on father’s height in the Galton data:\n\nCreate a scatter plot with the regression line.\nAdd vertical line segments showing the residuals for each observation.\nCompute the sum of squared residuals.\nPick a different slope coefficient (not the least-squares value) and verify that it gives a larger sum of squared residuals.\n\n\n\n6.6.3.3 Visualizing Feature Space\nUsing centered Galton height data with two predictors (mother and father):\n\nCreate a 3D plot showing the mother, father, and son centered height vectors, \\((\\dot{m}_\\bullet, \\dot{f}_\\bullet, \\dot{s}_\\bullet)\\).\nAdd the vector of fitted values \\(\\hat{\\dot{s}}_\\bullet\\).\nVisually verify that the vector of fitted values lies in the plane spanned by the mother and father vectors.\n\n\n\n\n6.6.4 Advanced\n\n6.6.4.1 Covariance and Inner Product\nConsider the pair of \\(n-\\)dimensional vectors \\((v_\\bullet, w_\\bullet)\\) as a numeric data matrix of dimension \\(n \\times 2\\). Recall the notation \\(\\bar{v}\\) for the arithmetic mean of \\(v_\\bullet\\), and \\(\\dot{v}_\\bullet = v_\\bullet - \\bar{v} \\; 1_\\bullet\\) for the centered version of \\(v_\\bullet\\), so that \\(\\dot{v}_i = v_i - \\bar{v}\\) for each element \\(v_i\\) of \\(v_\\bullet\\). Now the sample covariance \\(cov(v_\\bullet, w_\\bullet)\\) is defined as follows:\n\\[\n\\begin{align}\n  cov(v_\\bullet, w_\\bullet)\n  &= \\frac{1}{n-1} \\sum_{i = 1}^n (v_i - \\bar{v}) \\; (w_i - \\bar{w})\n\\end{align}\n\\qquad(6.64)\\]\n\nExpress the sample covariance using inner-product notation.\nThe sample variance is defined as \\(var(v_\\bullet) = cov(v_\\bullet, v_\\bullet)\\). Express \\(var(v_\\bullet)\\) using norm notation.\nThe sample standard deviation is defined as \\(sd(v_\\bullet) = \\sqrt{var(v_\\bullet)}\\). Express \\(sd(v_\\bullet)\\) using norm notation.\nUse inner-product notation to express the sample correlation \\(cor(v_\\bullet, w_\\bullet)\\), defined as follows.\n\n\\[\n\\begin{align}\n  cor(v_\\bullet, w_\\bullet)\n  &= \\frac{1}{n-1} \\sum_{i = 1}^n \\frac{v_i - \\bar{v}}{sd(v_\\bullet)} \\; \\frac{w_i - \\bar{w}}{sd(w_\\bullet)}\n\\end{align}\n\\qquad(6.65)\\]\n\n\n6.6.4.2 Correlation Geometry\nWhen \\(n-\\)dimensional vectors \\(v_\\bullet, w_\\bullet\\) are centered \\((v_\\bullet \\mapsto \\dot{v}_\\bullet)\\) and normalized to unit length \\((\\tilde{v}_\\bullet = \\dot{v}_\\bullet / \\Vert \\dot{v}_\\bullet \\rVert)\\), show that:\n\nThe inner product \\(\\tilde{v}_\\bullet^\\top \\tilde{w}_\\bullet\\) is equal to the correlation \\(r\\) between features \\(v_\\bullet, w_\\bullet\\) times \\((n - 1)\\).\nThe angle \\(\\theta\\) between \\(v_\\bullet, w_\\bullet\\) satisfies \\(\\cos(\\theta) = r\\).\nApply this to the Galton height data: compute the angles between mother, father, and son height vectors.\n\n\n\n6.6.4.3 Correlated Features\n\nGenerate synthetic data \\((y_\\bullet, x_{\\bullet, 1}, x_{\\bullet, 2})\\) such that the two features \\((x_{\\bullet, 1}, x_{\\bullet, 2})\\) are orthogonal (correlation = 0). Then fit a regression model.\nNow generate similar data, but this time with the two features highly correlated (correlation &gt; 0.9). Once again, fit a regression model.\nCompare the standard errors of the coefficient estimates. Why are they different?\nVisualize the column spaces in both cases. How does the geometry explain the difference in precision?\n\n\n\n6.6.4.4 Sum of Squares Decomposition\nFor any linear regression:\n\nShow algebraically that for centered data \\(\\lVert \\dot{y}_\\bullet \\rVert^2 = \\lVert \\hat{\\dot{y}}_\\bullet \\rVert^2 + \\lVert \\dot{\\epsilon}_\\bullet \\rVert^2\\).\nExplain this geometrically using the Pythagorean theorem in \\(n-\\)dimensional space.\nVerify this numerically with the Galton heights data.\nComment on the following characterization: (total variation) = (explained variation) + (unexplained variation).\n\n\n\n6.6.4.5 Projection Matrix\nThe projection matrix \\(P\\) is defined by Equation 6.50.\n\nShow that \\(P\\) is symmetric: \\(P^\\top = P\\).\nShow that \\(P\\) is idempotent: \\(P^2 = P\\).\nWhat does the idempotent property mean geometrically?\nCompute \\(P\\) for a small \\((n = 5)\\) subset of the Galton heights data and verify these properties numerically.\n\n\n\n6.6.4.6 Feature Basis Vectors\nUsing the Galton heights centered data:\n\nThe regression coefficients \\((\\hat{\\beta}_m, \\hat{\\beta}_f)\\) represent coordinates in the non-orthogonal basis defined by the mother and father vectors \\((\\dot{m}_\\bullet, \\dot{f}_\\bullet)\\). Express the fitted values \\(\\hat{\\dot{s}}_\\bullet\\) in this basis.\nTransform to an orthogonal basis using the Gram-Schmidt process.\nExpress \\(\\hat{\\dot{s}}_\\bullet\\) in the orthogonal basis.\nVerify that the projection is the same in both coordinate systems.\n\n\n\n\n\n“Categorical Variable | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Categorical_variable.\n\n\n“Hamming Distance | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Hamming_distance.\n\n\nLeCun, Yann, Corinna Cortes, and Christopher J. C. Burges. 2005. “The MNIST Database of Handwritten Digits.” https://web.archive.org/web/20200430193701/http://yann.lecun.com/exdb/mnist/.\n\n\n“Levenshtein Distance | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Levenshtein_distance.\n\n\n“MNIST Database | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/MNIST_database.\n\n\n“Multinomial Logistic Regression | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Multinomial_logistic_regression.\n\n\n“One-Hot | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/One-hot.\n\n\n“Raster Graphics | Wikipedia.” 2025. Wikipedia, October. https://en.wikipedia.org/wiki/Raster_graphics.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html",
    "href": "reduce-dim.html",
    "title": "7  Dimension Reduction",
    "section": "",
    "text": "7.1 Data Examples\nThis chapter addresses the problem of finding and visualizing structure in high-dimensional data. We begin the discussion with example data sets.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#data-examples",
    "href": "reduce-dim.html#data-examples",
    "title": "7  Dimension Reduction",
    "section": "",
    "text": "7.1.1 US Arrests\nData analysis often begins with an exploration, using tables and figures to learn more about the data before formulating specific questions or hypotheses. The exploration becomes more difficult as the dimensions of the data set increase. Here’s an example.\nMcNeil (1977) reviewed the relationship among violent crime statistics per US state and the percent of the population living in urban areas, variables described in Table 7.1 below. With just four variables, we can visualize the full data structure (e.g., using a scatter plot matrix as shown in Figure 7.3). However, the initial exploratory data analysis reveals strong correlations among the three crime variables (Murder, Assault, Rape), suggesting that the data may effectively lie in fewer than four dimensions. Dimension reduction techniques can help us identify these underlying patterns, enabling us to (1) visualize state-level crime patterns in 2D or 3D, (2) identify states having similar crime profiles, and (3) understand whether crime variation is driven by one dominant factor or multiple independent factors.\nTable 7.2 below describes related state-level statistics from the same period (the 1970s). The data from both tables are available from the core R package datasets.\n\n\n\n\nTable 7.1: Arrests per US State (1973)\n\n\n\n\nArrests per US State (1973)\n\n\nvariable\nyear\nunit\ndescription\n\n\n\n\nAssault\n1973\n100K\nassault arrests per 100K population\n\n\nRape\n1973\n100K\nrape arrests per 100K population\n\n\nMurder\n1973\n100K\nmurder arrests per 100K population\n\n\nUrbanPop\n1973\nPCT\npercent of population in urban areas\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7.2: Statistics per US State (1970s)\n\n\n\n\nStatistics per US State (1970s)\n\n\nvariable\nyear\nunit\ndescription\n\n\n\n\nst_abb\n1963\nchr\n2-letter abbreviation of state name\n\n\nst_nm\n1959\nchr\nstate name\n\n\nx\n1959\nlat-long\nlongitude of state center\n\n\ny\n1959\nlat-long\nlatitude of state center\n\n\ndivision\n1959\nfct\ngeo grouping into 9 groups\n\n\nregion\n1959\nfct\ngeo grouping into 4 groups\n\n\nPopulation\n1975\n1000\nestimated population\n\n\nIncome\n1974\nUSD\naverage income\n\n\nIlliteracy\n1970\nPCT\nilliterate percent of population\n\n\nLife Exp\n1969\nYR\nlife expetency in years\n\n\nMurder_76\n1976\n100K\nmurder rate per 100K population\n\n\nHS Grad\n1970\nPCT\npercent high-school graduates\n\n\nFrost\n1960\nDAY\navg number of days below freezing\n\n\nArea\n1959\nmi^2\nland area in square miles\n\n\n\n\n\n\n\n\nFigure 7.1 below shows the distribution across the 50 states of each type of violent crime. Rates are calcuated as arrests per 100,000 population. The figure shows these rates on a \\(\\log_{10}\\) scale, since assault arrests are many times more common than rape or murder arrests.\n\n\n\n\n\n\n\n\nFigure 7.1: Violent Crime Rates per US State (1973)\n\n\n\n\n\nThe figure above gives a view of three data variables, that is, three columns of the data matrix. Figure 7.2 below, a 3D scatterplot, gives a complementary view of each of the 50 states as a point whose coordinates are the respective arrest rates for assault, rape, and murder, with each point colored by the value of UrbanPop the percentage of the state’s population living in an urban area. Since individual states correspond to the rows of the data matrix, this figure is a row-based perspective on the data matrix.\n\n\n\n\n\n\n\n\nFigure 7.2: Violent Crime Rates in each US State (1973)\n\n\n\n\nFigure 7.3 below exemplifies a matrix of 2D scatter plots, a display method that accommodates more than 3 variables (but not many more, practically speaking).\n\n\n\n\n\n\n\n\nFigure 7.3: Arrests Variables: Relationships and Correlations\n\n\n\n\n\nThis figure provides both column-based and row-based views of the data matrix, with the scatter diagrams for each pair of variables providing the row-based perspective.\n\n\n7.1.2 Dry Beans\nKoklu and Ozkan (2020) published a dataset of visual characteristics of dried beans “… in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera.” The resulting dataset contains 16 morphological features extracted from each bean image, including measures of area, perimeter, compactness, length, width, and various shape factors.\nThe classification goal is to predict bean variety from these morphological measurements. Successful classification could have many applications, e.g., to improve sorting systems in agricultural processing. However, many of these 16 features are inherently redundant: for example, area and perimeter are strongly correlated, as are various length and width measurements. Dimension reduction can reveal whether bean varieties differ primarily in size, shape, or both, and whether a smaller subset of derived features might achieve comparable classification accuracy with greater interpretability and computational efficiency.\nThe data are available R package beans, containing a data matrix (tibble, more precisely) of dimension \\(13611 \\times 17\\). The last column of the data matrix is response variable class that assigns one of seven types of bean to each bean-image. The remaining 16 columns of the data matrix are morphological measurements (of shape and size).\nKuhn and Silge (2022) develop and evaluate different classification models for these data. 1 They begin by examining the correlation coefficients for each pair of the 16 feature vectors. Figure 7.4 follows their example.\n\n\n\n\n\n\n\n\nFigure 7.4: beans: correlation among features\n\n\n\n\n\nIn this figure, the absolute value of each correlation coefficient is represented by the narrowness of the drawn ellipse and the depth of its color. The sign of the correlation coefficient is represented by both the color and direction of the ellipse.\nThese size and shape features measure similar concepts. Consequently, several pairs of feature vectors are highly correlated, 2 which offers the possibility of transforming the 16 original features into a smaller set without sacrificing classification power.\n\n\n7.1.3 Wine Quality\nP. Cortez et al. (2009) model wine preference as a function of 12 physicochemical properties of wine, which are listed in Table 7.3. The quality score is the median of three expert tastings. The data consist of 6497 wines, 1599 red and 4898 white. 3 4\nThe modeling goal is to understand the chemical properties influencing wine quality ratings and to predict quality from objective laboratory measurements. Apart from wine color (red or white), the remaining 11 physicochemical features include related measurements, e.g., pH, fixed acidity, and citric acid are chemically interrelated, as are free and total sulfur dioxide. Dimension reduction can help us: (1) visualize the chemical space that wines occupy in 2D or 3D, (2) identify whether wine quality varies along a small number of chemical gradients (e.g., acidity vs. alcohol content), (3) understand the chemical differences between red and white wines, and (4) determine whether a smaller set of derived features might predict quality as well as the full set of measurements.\n\n\n\n\nTable 7.3: Wine: physicochemical properties\n\n\n\n\nWine: physicochemical properties\n\n\nvariable\nunit\n\n\n\n\nfixed acidity\ng(tartaric acid)/dm3\n\n\nvolatile acidity\ng(acetic acid)/dm3\n\n\ncitric acid\ng/dm3\n\n\nresidual sugar\ng/dm3\n\n\nchlorides\ng(sodium chloride)/dm3\n\n\nfree sulfur dioxide\nmg/dm3\n\n\ntotal sulfur dioxide\nmg/dm3\n\n\ndensity\ng/cm3\n\n\npH\n\n\n\nsulphates\ng(potassium sulphate)/dm3\n\n\nalcohol\n% volume\n\n\nquality\n0:10\n\n\ncolor\n{red, white}\n\n\n\n\n\n\n\n\nCorrelations among the 11 numeric features are shown in Figure 7.5 below for red and white wines, respectively.\n\n\n\n\n\n\n\n\n\n\n\n(a) Red wines\n\n\n\n\n\n\n\n\n\n\n\n(b) White wines\n\n\n\n\n\n\n\nFigure 7.5: Wine: correlation among numeric features\n\n\n\nThese figures reveal some substantial correlations, with distinct patterns per wine color.\nTable 7.4 below shows feature-quality correlations for red and white wines, respectively.\n\n\n\n\nTable 7.4: Feature-quality correleations among (red, white) wines\n\n\n\n\nFeature-quality correleations\n\n\nfeature\nq_red\nq_white\n\n\n\n\nfixed acidity\n0.12\n-0.11\n\n\nvolatile acidity\n-0.39\n-0.19\n\n\ncitric acid\n0.23\n-0.01\n\n\nresidual sugar\n0.01\n-0.10\n\n\nchlorides\n-0.13\n-0.21\n\n\nfree sulfur dioxide\n-0.05\n0.01\n\n\ntotal sulfur dioxide\n-0.19\n-0.17\n\n\ndensity\n-0.17\n-0.31\n\n\npH\n-0.06\n0.10\n\n\nsulphates\n0.25\n0.05\n\n\nalcohol\n0.48\n0.44\n\n\n\n\n\n\n\n\nWe see that alcohol volume is a prominent indicator of quality for both red and white wines. On the other hand, citric acid and sulphates are strongly correlated with the quality of red wine, but not white.\nAs demonstrated by P. Cortez et al. (2009), these data patterns offer the possibility of developing a smaller set of features as predictors of wine quality.\n\n\n7.1.4 Cancer Genomics (NCI60)\nThe NCI60 data 5 6 consists of gene expression measurements from 64 cancer cell lines. For each cell line, expression levels were measured for 6830 genes using microarray technology. The cell lines represent 14 different cancer types, including leukemia, melanoma, and cancers of the colon, breast, ovary, lung, and central nervous system. This data set is a canonical example of a high-dimensional data matrix where the number of features \\((d)\\) greatly exceeds the number \\(n\\) of data cases: \\(d \\gg n\\).\nThe data set is part of the NCI-60 panel and associated datasets, which are maintained by the Frederick National Laboratory for Cancer Research (FNLCR) and the National Cancer Institute’s (NCI) Developmental Therapeutics Program (DTP). The data serve as a publicly available platform for the global cancer research community to study tumor biology, evaluate new bioinformatics approaches, and select appropriate cell models for specific research questions.\nThese data present challenges that require dimension reduction, that is, the transformation of the set of 6830 genomic features into a smaller set that capture the dominant patterns of variation. We proceed to describe such challenges.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#footnotes",
    "href": "reduce-dim.html#footnotes",
    "title": "7  Dimension Reduction",
    "section": "",
    "text": "To develop and evaluate the classification models, the authors split the observations into three non-overlapping sets: training (\\(n = 10206\\), nearly 75%), testing \\((n = 1703)\\), and validation \\((n = 1702)\\).↩︎\nFor example, the features area and convex_area can differ in principle, but for the beans data the correlation is 0.99994.↩︎\nThe authors contributed the data (now archived) to the UC Irvine Machine Learning Repository. See Paulo Cortez et al. (2009).↩︎\nThe wines in this study are all from the Minho (northwest) region of Portugal. Medium in alcohol, it is appreciated for its freshness (especially in summer). At the time of writing the authors report that Minho wine accounts for 15% of total Portuguese wine production, of which some 10% is exported, mostly white wine.↩︎\nThe data are available from R package ISLR2. The vector of labels (cancer type per observation) can be accessed as ISLR2::NCI60$labs. The data can be accessed as ISLR2::NCI60$data in the form of a \\(64 \\times 6830\\) matrix with column names “1”, “2”, …, “6830” rather than gene identifiers. While this simplification is adequate for illustrating dimension reduction methods, readers requiring gene-level biological interpretation should consult the rcellminer and rcellminerData Bioconductor packages (Luna et al., 2016; Reinhold et al., 2019), which provide the same data with complete gene annotations and represent the current standard for programmatic access to NCI-60 molecular profiling data.↩︎\nThe ISLR2 package cites Ross et al. (2000) as the source of ISLR2::NCI60.↩︎\nSee Bellman (1957), Donoho (2000), and “Curse of Dimensionality | Wikipedia” (2025).↩︎\nAmong the notations for the inner product (scalar product) of vectors \\(x, y\\) belonging to the same Euclidean space are: (1) \\(\\left &lt; x, y \\right&gt;\\); and (2) \\(x^\\top \\; y\\). The former is less ambiguous. The latter conforms to the notation of matrix multiplication; it is useful and perhaps more intuitive. The ambiguity may occur when at least one of \\(x, y\\) is a row or a column of some matrix. If \\(x\\) is a row vector and \\(y\\) is a column vector, then the inner product of \\(x, y\\) should be written as \\(x \\; y\\) according to matrix notation. Nevertheless the matrix multiplication notation is useful. For example we use \\(u \\; u^\\top\\) to denote an orthogonal projection matrix, not a scalar product, when \\(u\\) is a unit vector.↩︎\nNote that \\(x_{i, \\bullet} \\in \\mathbb{R}^d\\) is a row vector, namely the \\(i^{th}\\) row of feature matrix \\(X_{\\bullet, \\bullet}\\). Therefore the inner product \\(\\left &lt; x_{i, \\bullet}, \\; P \\; x_{i, \\bullet} \\right &gt;\\) is expressed as \\(x_{i, \\bullet} \\; P \\; x_{i, \\bullet}^\\top\\) in matrix multiplication notation.↩︎",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#dimensionality-curse-and-blessing",
    "href": "reduce-dim.html#dimensionality-curse-and-blessing",
    "title": "7  Dimension Reduction",
    "section": "7.2 Dimensionality: Curse and Blessing",
    "text": "7.2 Dimensionality: Curse and Blessing\nAs just noted, the NCI60 data set, with \\(d =\\) 6830 and \\(n =\\) 64, presents challenges that make dimension reduction essential. Several issues arise with any “wide” data set in which \\(d \\gg n\\), and include the following.\n\nVisualization: We cannot plot points in \\(d\\) dimensions to explore patterns or detect outliers.\nOver-fitting: With \\(d \\gg n\\), infinitely many coefficient vectors \\(\\beta_\\bullet\\) produce identical predictions that replicate the response variable within the training data. This makes model selection impossible without regularization.\nComputation: Storing and manipulating a \\(d \\times d\\) covariance matrix becomes prohibitively expensive.\n\nIn 1957 Richard Bellman characterized such challenges as “the curse of dimensionality”. 7\nDonoho (2000) points out that: (1) many established statistical methods assume \\(d &lt; n\\); and (2) we can expect \\(d \\gg n\\) to occur more and more often as data-collection is increasingly automated to retrieve all potentially useful details for subsequent screening.\nIn addition, “Curse of Dimensionality | Wikipedia” (2025) notes that our geometric intuition is grounded in \\(d \\le 3\\) and is overturned as \\(d\\) increases. To illustrate, consider a unit hypercube \\([0,1]^d\\) containing the largest possible inscribed ball. The ratio \\(r(d)\\) of their volumes shrinks dramatically: \\(r(3) \\approx 0.52\\), \\(r(10) \\approx 0.0025\\), and \\(r(100) \\approx 2 \\times 10^{-70}\\). In high dimensions, the preponderance of randomly generated points within the cube land far from the center!\nYet Donoho (2000) also sees opportunities. While randomly generated data in high dimensions behaves pathologically, actual data tend to be more coherent. Consider the example data sets:\n\nUS Arrests: The three crime variables (assault, rape, murder) are highly correlated; they don’t independently span 3D space.\nDry_Beans: The 16 shape measurements aren’t independent; they reflect underlying bean geometry.\n\nWine quality: The 11 chemical properties are constrained by fermentation chemistry.\nNCI60: The 6,830 genes participate in shared biological pathways.\n\nIn each case, the data likely occupies a much lower-dimensional structure within the high-dimensional feature space. If we can identify this structure, dimension reduction may actually improve modeling by revealing the true degrees of freedom in the data.\nThe remainder of this chapter develops methods that exploit such opportunities. We begin with Principal Component Analysis (Section 7.3), which finds low-dimensional approximations to high-dimensional data through eigendecomposition. We then explore how supervision (Section 7.4) can guide dimension reduction when prediction is the goal. Finally, we examine computational strategies (Section 7.5) for extreme cases like NCI60 where \\(d \\gg n\\).",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#principal-component-analysis",
    "href": "reduce-dim.html#principal-component-analysis",
    "title": "7  Dimension Reduction",
    "section": "7.3 Principal Component Analysis",
    "text": "7.3 Principal Component Analysis",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#supervised-dimension-reduction",
    "href": "reduce-dim.html#supervised-dimension-reduction",
    "title": "7  Dimension Reduction",
    "section": "7.4 Supervised Dimension Reduction",
    "text": "7.4 Supervised Dimension Reduction",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#computational-considerations",
    "href": "reduce-dim.html#computational-considerations",
    "title": "7  Dimension Reduction",
    "section": "7.5 Computational Considerations",
    "text": "7.5 Computational Considerations",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#choosing-and-evaluating-methods",
    "href": "reduce-dim.html#choosing-and-evaluating-methods",
    "title": "7  Dimension Reduction",
    "section": "7.6 Choosing and Evaluating Methods",
    "text": "7.6 Choosing and Evaluating Methods",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#other-methods",
    "href": "reduce-dim.html#other-methods",
    "title": "7  Dimension Reduction",
    "section": "7.7 Other Methods",
    "text": "7.7 Other Methods",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#summary",
    "href": "reduce-dim.html#summary",
    "title": "7  Dimension Reduction",
    "section": "7.8 Summary",
    "text": "7.8 Summary",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#exercises",
    "href": "reduce-dim.html#exercises",
    "title": "7  Dimension Reduction",
    "section": "7.9 Exercises",
    "text": "7.9 Exercises\n\n\n\n\nBellman, Richard Ernest. 1957. Dynamic Programming. Princeton University Press.\n\n\nCortez, Paulo, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009. “Wine Quality.” https://doi.org/https://doi.org/10.24432/C56S3T.\n\n\nCortez, P., Antonio Luíz Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. 2009. “Modeling Wine Preferences by Data Mining from Physicochemical Properties.” Decis. Support Syst. 47: 547–53. https://api.semanticscholar.org/CorpusID:2996254.\n\n\n“Curse of Dimensionality | Wikipedia.” 2025. Wikipedia, November. https://en.wikipedia.org/wiki/Curse_of_dimensionality.\n\n\nDonoho, David L. 2000. “High-Dimensional Data Analysis: The Curses and Blessings of Dimensionality,” 1–32. https://www.researchgate.net/publication/220049061_High-Dimensional_Data_Analysis_The_Curses_and_Blessings_of_Dimensionality.\n\n\nKoklu, Mehmet, and Ibrahim A Ozkan. 2020. “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture 174: 105507.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. O’Reilly Media. https://www.tmwr.org/.\n\n\nMcNeil, Donald R. 1977. Interactive Data Analysis: A Practical Primer. New York, USA: John Wiley & Sons.\n\n\nRoss, Douglas T, Uwe Scherf, Michael B Eisen, Charles M Perou, Christian Rees, Paul Spellman, Vishwanath Iyer, et al. 2000. “Systematic Variation in Gene Expression Patterns in Human Cancer Cell Lines.” Nature Genetics 24: 227–35. https://doi.org/10.1038/73432.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-data-examples",
    "href": "reduce-dim.html#sec-dim-r-data-examples",
    "title": "7  Dimension Reduction",
    "section": "",
    "text": "7.1.1 US Arrests\nData analysis often begins with an exploration, using tables and figures to learn more about the data before formulating specific questions or hypotheses. The exploration becomes more difficult as the dimensions of the data set increase. Here’s an example.\nMcNeil (1977) reviewed the relationship among violent crime statistics per US state and the percent of the population living in urban areas, variables described in Table 7.1 below. With just four variables, we can visualize the full data structure (e.g., using a scatter plot matrix as shown in Figure 7.3). However, the initial exploratory data analysis reveals strong correlations among the three crime variables (Murder, Assault, Rape), suggesting that the data may effectively lie in fewer than four dimensions. Dimension reduction techniques can help us identify these underlying patterns, enabling us to (1) visualize state-level crime patterns in 2D or 3D, (2) identify states having similar crime profiles, and (3) understand whether crime variation is driven by one dominant factor or multiple independent factors.\nTable 7.2 below describes related state-level statistics from the same period (the 1970s). The data from both tables are available from the core R package datasets.\n\n\n\n\nTable 7.1: Arrests per US State (1973)\n\n\n\n\nArrests per US State (1973)\n\n\nvariable\nyear\nunit\ndescription\n\n\n\n\nAssault\n1973\n100K\nassault arrests per 100K population\n\n\nRape\n1973\n100K\nrape arrests per 100K population\n\n\nMurder\n1973\n100K\nmurder arrests per 100K population\n\n\nUrbanPop\n1973\nPCT\npercent of population in urban areas\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7.2: Statistics per US State (1970s)\n\n\n\n\nStatistics per US State (1970s)\n\n\nvariable\nyear\nunit\ndescription\n\n\n\n\nst_abb\n1963\nchr\n2-letter abbreviation of state name\n\n\nst_nm\n1959\nchr\nstate name\n\n\nx\n1959\nlat-long\nlongitude of state center\n\n\ny\n1959\nlat-long\nlatitude of state center\n\n\ndivision\n1959\nfct\ngeo grouping into 9 groups\n\n\nregion\n1959\nfct\ngeo grouping into 4 groups\n\n\nPopulation\n1975\n1000\nestimated population\n\n\nIncome\n1974\nUSD\naverage income\n\n\nIlliteracy\n1970\nPCT\nilliterate percent of population\n\n\nLife Exp\n1969\nYR\nlife expetency in years\n\n\nMurder_76\n1976\n100K\nmurder rate per 100K population\n\n\nHS Grad\n1970\nPCT\npercent high-school graduates\n\n\nFrost\n1960\nDAY\navg number of days below freezing\n\n\nArea\n1959\nmi^2\nland area in square miles\n\n\n\n\n\n\n\n\nFigure 7.1 below shows the distribution across the 50 states of each type of violent crime. Rates are calcuated as arrests per 100,000 population. The figure shows these rates on a \\(\\log_{10}\\) scale, since assault arrests are many times more common than rape or murder arrests.\n\n\n\n\n\n\n\n\nFigure 7.1: Violent Crime Rates per US State (1973)\n\n\n\n\n\nThe figure above gives a view of three data variables, that is, three columns of the data matrix. Figure 7.2 below, a 3D scatterplot, gives a complementary view of each of the 50 states as a point whose coordinates are the respective arrest rates for assault, rape, and murder, with each point colored by the value of UrbanPop the percentage of the state’s population living in an urban area. Since individual states correspond to the rows of the data matrix, this figure is a row-based perspective on the data matrix.\n\n\n\n\n\n\n\n\nFigure 7.2: Violent Crime Rates in each US State (1973)\n\n\n\n\nFigure 7.3 below exemplifies a matrix of 2D scatter plots, a display method that accommodates more than 3 variables (but not many more, practically speaking).\n\n\n\n\n\n\n\n\nFigure 7.3: Arrests Variables: Relationships and Correlations\n\n\n\n\n\nThis figure provides both column-based and row-based views of the data matrix, with the scatter diagrams for each pair of variables providing the row-based perspective.\n\n\n7.1.2 Dry Beans\nKoklu and Ozkan (2020) published a dataset of visual characteristics of dried beans “… in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera.” The resulting dataset contains 16 morphological features extracted from each bean image, including measures of area, perimeter, compactness, length, width, and various shape factors.\nThe classification goal is to predict bean variety from these morphological measurements. Successful classification could have many applications, e.g., to improve sorting systems in agricultural processing. However, many of these 16 features are inherently redundant: for example, area and perimeter are strongly correlated, as are various length and width measurements. Dimension reduction can reveal whether bean varieties differ primarily in size, shape, or both, and whether a smaller subset of derived features might achieve comparable classification accuracy with greater interpretability and computational efficiency.\nThe data are available R package beans, containing a data matrix (tibble, more precisely) of dimension \\(13611 \\times 17\\). The last column of the data matrix is response variable class that assigns one of seven types of bean to each bean-image. The remaining 16 columns of the data matrix are morphological measurements (of shape and size).\nKuhn and Silge (2022) develop and evaluate different classification models for these data. 1 They begin by examining the correlation coefficients for each pair of the 16 feature vectors. Figure 7.4 follows their example.\n\n\n\n\n\n\n\n\nFigure 7.4: beans: correlation among features\n\n\n\n\n\nIn this figure, the absolute value of each correlation coefficient is represented by the narrowness of the drawn ellipse and the depth of its color. The sign of the correlation coefficient is represented by both the color and direction of the ellipse.\nThese size and shape features measure similar concepts. Consequently, several pairs of feature vectors are highly correlated, 2 which offers the possibility of transforming the 16 original features into a smaller set without sacrificing classification power.\n\n\n7.1.3 Wine Quality\nP. Cortez et al. (2009) model wine preference as a function of 12 physicochemical properties of wine, which are listed in Table 7.3. The quality score is the median of three expert tastings. The data consist of 6497 wines, 1599 red and 4898 white. 3 4\nThe modeling goal is to understand the chemical properties influencing wine quality ratings and to predict quality from objective laboratory measurements. Apart from wine color (red or white), the remaining 11 physicochemical features include related measurements, e.g., pH, fixed acidity, and citric acid are chemically interrelated, as are free and total sulfur dioxide. Dimension reduction can help us: (1) visualize the chemical space that wines occupy in 2D or 3D, (2) identify whether wine quality varies along a small number of chemical gradients (e.g., acidity vs. alcohol content), (3) understand the chemical differences between red and white wines, and (4) determine whether a smaller set of derived features might predict quality as well as the full set of measurements.\n\n\n\n\nTable 7.3: Wine: physicochemical properties\n\n\n\n\nWine: physicochemical properties\n\n\nvariable\nunit\n\n\n\n\nfixed acidity\ng(tartaric acid)/dm3\n\n\nvolatile acidity\ng(acetic acid)/dm3\n\n\ncitric acid\ng/dm3\n\n\nresidual sugar\ng/dm3\n\n\nchlorides\ng(sodium chloride)/dm3\n\n\nfree sulfur dioxide\nmg/dm3\n\n\ntotal sulfur dioxide\nmg/dm3\n\n\ndensity\ng/cm3\n\n\npH\n\n\n\nsulphates\ng(potassium sulphate)/dm3\n\n\nalcohol\n% volume\n\n\nquality\n0:10\n\n\ncolor\n{red, white}\n\n\n\n\n\n\n\n\nCorrelations among the 11 numeric features are shown in Figure 7.5 below for red and white wines, respectively.\n\n\n\n\n\n\n\n\n\n\n\n(a) Red wines\n\n\n\n\n\n\n\n\n\n\n\n(b) White wines\n\n\n\n\n\n\n\nFigure 7.5: Wine: correlation among numeric features\n\n\n\nThese figures reveal some substantial correlations, with distinct patterns per wine color.\nTable 7.4 below shows feature-quality correlations for red and white wines, respectively.\n\n\n\n\nTable 7.4: Feature-quality correleations among (red, white) wines\n\n\n\n\nFeature-quality correleations\n\n\nfeature\nq_red\nq_white\n\n\n\n\nfixed acidity\n0.12\n-0.11\n\n\nvolatile acidity\n-0.39\n-0.19\n\n\ncitric acid\n0.23\n-0.01\n\n\nresidual sugar\n0.01\n-0.10\n\n\nchlorides\n-0.13\n-0.21\n\n\nfree sulfur dioxide\n-0.05\n0.01\n\n\ntotal sulfur dioxide\n-0.19\n-0.17\n\n\ndensity\n-0.17\n-0.31\n\n\npH\n-0.06\n0.10\n\n\nsulphates\n0.25\n0.05\n\n\nalcohol\n0.48\n0.44\n\n\n\n\n\n\n\n\nWe see that alcohol volume is a prominent indicator of quality for both red and white wines. On the other hand, citric acid and sulphates are strongly correlated with the quality of red wine, but not white.\nAs demonstrated by P. Cortez et al. (2009), these data patterns offer the possibility of developing a smaller set of features as predictors of wine quality.\n\n\n7.1.4 Cancer Genomics (NCI60)\nThe NCI60 data 5 6 consists of gene expression measurements from 64 cancer cell lines. For each cell line, expression levels were measured for 6830 genes using microarray technology. The cell lines represent 14 different cancer types, including leukemia, melanoma, and cancers of the colon, breast, ovary, lung, and central nervous system. This data set is a canonical example of a high-dimensional data matrix where the number of features \\((d)\\) greatly exceeds the number \\(n\\) of data cases: \\(d \\gg n\\).\nThe data set is part of the NCI-60 panel and associated datasets, which are maintained by the Frederick National Laboratory for Cancer Research (FNLCR) and the National Cancer Institute’s (NCI) Developmental Therapeutics Program (DTP). The data serve as a publicly available platform for the global cancer research community to study tumor biology, evaluate new bioinformatics approaches, and select appropriate cell models for specific research questions.\nThese data present challenges that require dimension reduction, that is, the transformation of the set of 6830 genomic features into a smaller set that capture the dominant patterns of variation. We proceed to describe such challenges.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-curse",
    "href": "reduce-dim.html#sec-dim-r-curse",
    "title": "7  Dimension Reduction",
    "section": "7.2 Dimensionality: Curse and Blessing",
    "text": "7.2 Dimensionality: Curse and Blessing\nAs just noted, the NCI60 data set, with \\(d =\\) 6830 and \\(n =\\) 64, presents challenges that make dimension reduction essential. Several issues arise with any “wide” data set in which \\(d \\gg n\\), and include the following.\n\nVisualization: We cannot plot points in \\(d\\) dimensions to explore patterns or detect outliers.\nOver-fitting: With \\(d \\gg n\\), infinitely many coefficient vectors \\(\\beta_\\bullet\\) produce identical predictions that replicate the response variable within the training data. This makes model selection impossible without regularization.\nComputation: Storing and manipulating a \\(d \\times d\\) covariance matrix becomes prohibitively expensive.\n\nIn 1957 Richard Bellman characterized such challenges as “the curse of dimensionality”. 7\nDonoho (2000) points out that: (1) many established statistical methods assume \\(d &lt; n\\); and (2) we can expect \\(d \\gg n\\) to occur more and more often as data-collection is increasingly automated to retrieve all potentially useful details for subsequent screening.\nIn addition, “Curse of Dimensionality | Wikipedia” (2025) notes that our geometric intuition is grounded in \\(d \\le 3\\) and is overturned as \\(d\\) increases. To illustrate, consider a unit hypercube \\([0,1]^d\\) containing the largest possible inscribed ball. The ratio \\(r(d)\\) of their volumes shrinks dramatically: \\(r(3) \\approx 0.52\\), \\(r(10) \\approx 0.0025\\), and \\(r(100) \\approx 2 \\times 10^{-70}\\). In high dimensions, the preponderance of randomly generated points within the cube land far from the center!\nYet Donoho (2000) also sees opportunities. While randomly generated data in high dimensions behaves pathologically, actual data tend to be more coherent. Consider the example data sets:\n\nUS Arrests: The three crime variables (assault, rape, murder) are highly correlated; they don’t independently span 3D space (Figure 7.2).\nDry_Beans: The 16 shape measurements aren’t independent; they reflect underlying bean geometry.\n\nWine quality: The 11 chemical properties are constrained by fermentation chemistry.\nNCI60: The 6,830 genes participate in shared biological pathways.\n\nIn each case, the data likely occupies a much lower-dimensional structure within the high-dimensional feature space. If we can identify this structure, dimension reduction may actually improve modeling by revealing the true degrees of freedom in the data.\nThe remainder of this chapter develops methods that exploit such opportunities. We begin with Principal Component Analysis (Section 7.3), which finds low-dimensional approximations to high-dimensional data through eigen-decomposition. We then explore how supervision (Section 7.4) can guide dimension reduction when prediction is the goal. Finally, we examine computational strategies (Section 7.5) for extreme cases like NCI60 where \\(d \\gg n\\).",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-pca",
    "href": "reduce-dim.html#sec-dim-r-pca",
    "title": "7  Dimension Reduction",
    "section": "7.3 Principal Component Analysis",
    "text": "7.3 Principal Component Analysis\nRecall the US Arrests dataset with four crime-related variables. Can we represent these 50 states in fewer than four dimensions while preserving most of the information? Principal Component Analysis (PCA) answers this by finding new variables, linear combinations of the original features, that capture maximum variance.\n\n7.3.1 2D Example\nTo develop some geometric intuition we begin with a 2D example. Figure 7.6 below illustrates PCA for (father, son) centered heights from the Galton data presented in Chapter 6.\n\n\n\n\n\n\n\n\nFigure 7.6: Principal Components: (father, son) centered heights\n\n\n\n\n\nThe figure represents two principal components, \\((z_1, z_2)\\), as respective red and blue lines, which are perpendicular to one another.\nThe red line \\((z_1)\\) points in the direction where the data varies most. Projecting points onto this line captures the maximum possible variance in a single dimension. The blue line \\((z_2)\\) is perpendicular to \\(z_1\\) and captures the maximum remaining variance. Together, they form a rotated coordinate system that aligns with the data’s natural variation.\n\n\n7.3.2 Linear Algebra Formulation\nTo define principal components we will describe a step-wise reconstruction of the \\(n \\times d\\) feature matrix \\(X_{\\bullet, \\bullet}\\).\n\n7.3.2.1 Preliminaries\nFirst, to simplify notation, we assume \\(X_{\\bullet, \\bullet}\\) to have already been centered: for each column of the original feature matrix the average value of the column elements has been calculated and subtracted. Therefore the average (or equivalently the sum) of each column of \\(X_{\\bullet, \\bullet}\\) equals zero.\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet} &= (x_{\\bullet, 1}, \\ldots, x_{\\bullet, d}) \\\\ \\\\\n  & \\text{with} \\\\ \\\\\n  1_\\bullet^\\top \\; x_{\\bullet, k} &= \\sum_{i = 1}^n x_{i, k} = 0 \\\\ \\\\\n  &\\text{for } k \\in \\{ 1, \\ldots, d \\} \\\\ \\\\\n  & \\text{so that} \\\\ \\\\\n  1_\\bullet^\\top \\; X_{\\bullet, \\bullet} &= 0_\\bullet \\in \\mathbb{R}^d\n\\end{align}\n\\qquad(7.1)\\]\nAs a consequence, the \\(d \\times d\\) matrix \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\) is a multiple of the sample covariance matrix among features.\n\\[\n\\begin{align}\n  cov \\left ( X_{\\bullet, \\bullet} \\right )\n  &= \\frac{1}{n - 1}\n  \\left (\n  X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n  \\right )\n\\end{align}\n\\qquad(7.2)\\]\nIn particular, the variance of each feature vector is proportional to its squared norm.\n\\[\n\\begin{align}\n  var( x_{\\bullet, k} )\n  &= \\frac{1}{n - 1}\n  \\lVert\n    x_{\\bullet, k}\n  \\rVert^2\n\\end{align}\n\\qquad(7.3)\\]\nAlthough matrix \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\) is a square matrix having dimensions \\(d \\times d\\), its rank may be less than \\(d\\). Let \\(r = rank \\left ( X_{\\bullet, \\bullet} \\right )\\). Then \\(r\\) is also the rank of \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\). It is the dimension of the subspace of \\(\\mathbb{R}^d\\) spanned by the rows of \\(X_{\\bullet, \\bullet}\\), as well as the dimension of the subspace of \\(\\mathbb{R}^n\\) spanned by the columns of \\(X_{\\bullet, \\bullet}\\). Consequently \\(r \\le \\min(n, d)\\).\nOur construction of principal components also relies on the notion of an orthogonal projection \\(P\\) from \\(\\mathbb{R}^d\\) to some subspace of \\(\\mathbb{R}^d\\). Recall that \\(P\\) is an orthogonal projection if and only if: it is idempotent \\((P^2 = P)\\); and symmetric \\((P^\\top = P)\\). Also, if \\(P\\) is an orthogonal projection then so is its complement \\(\\mathcal{I} - P\\).\nThe simplest example is the \\(d \\times d\\) matrix \\(v_\\bullet \\; v_\\bullet^\\top\\), where \\(v_\\bullet \\in \\mathbb{R}^d\\) is a unit vector (a vector of unit norm). This sends \\(x_\\bullet \\in \\mathbb{R}^d\\) to a scalar multiple of \\(v_\\bullet\\), namely \\(\\left &lt; x_\\bullet, v_\\bullet \\right &gt; v_\\bullet\\). 8\nTo any projection \\(P\\) defined on \\(\\mathbb{R}^d\\) we apply the following loss function \\(\\mathcal{L} (\\cdot)\\) in order to measure how well \\(P\\) reproduces the rows of the feature matrix \\(X_{\\bullet, \\bullet}\\).\n\\[\n\\begin{align}\n  \\mathcal{L} \\left ( P \\right )\n  &=\n    \\sum_{i = 1}^n\n    \\; \\left \\lVert\n      x_{i, \\bullet} \\; - \\; P \\; x_{i, \\bullet}\n    \\right \\rVert^2 \\\\\n  &=\n    \\sum_{i = 1}^n\n    \\; \\left \\lVert\n    \\left (\n      \\mathcal{I} \\; - \\; P\n    \\right )\n      \\; x_{i, \\bullet}\n    \\right \\rVert^2 \\\\\n  &=\n    \\sum_{i = 1}^n\n      \\left &lt;\n        x_{i, \\bullet}, \\;\n        \\left (\n          \\mathcal{I} \\; - \\; P\n        \\right ) \\;\n        x_{i, \\bullet}\n      \\right &gt; \\\\\n  &=\n    \\sum_{i = 1}^n\n      \\left &lt;\n        x_{i, \\bullet}, \\;\n        x_{i, \\bullet}\n      \\right &gt; \\; - \\;\n    \\sum_{i = 1}^n\n      \\left &lt;\n        x_{i, \\bullet}, \\;\n        P \\;\n        x_{i, \\bullet}\n      \\right &gt;  \\\\\n  &=\n    \\sum_{i = 1}^n\n      \\lVert\n        x_{i, \\bullet}\n      \\rVert^2 \\; - \\;\n    \\sum_{i = 1}^n\n      \\left &lt;\n        x_{i, \\bullet}, \\;\n        P \\;\n        x_{i, \\bullet}\n      \\right &gt;  \\\\\n  &=\n    \\mathrm{Tr} \\left \\{\n      X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n      \\right \\} \\; - \\;\n    \\sum_{i = 1}^n\n      \\left &lt;\n        x_{i, \\bullet}, \\;\n        P \\;\n        x_{i, \\bullet}\n      \\right &gt; \\\\ \\\\\n    & \\text{subject to the constraint:} \\\\ \\\\\n    & P \\text{ is an orthogonal projection defined on } \\mathbb{R}^d\n\\end{align}\n\\qquad(7.4)\\]\nThus \\(\\mathcal{L} (P)\\) is a sum over row index \\(i\\) of squared residuals in the approximation of \\(x_{i, \\bullet}\\) by \\(P \\; x_{i, \\bullet}\\).\nNow let \\(\\mathcal{P}\\) be a set of projections. From Equation 7.4 we see that projection \\(\\hat{P} \\in \\mathcal{P}\\) minimizes \\(\\mathcal{L} (P)\\) if and only if it maximizes a corresponding quadratic form:\n\\[\n\\begin{align}\n  &  \n  \\arg \\min_{ P \\in \\mathcal{P} } \\;\n  \\mathcal{L} ( P ) \\\\\n  &=\n  \\arg \\max_{ P \\in \\mathcal{P} } \\;\n  \\sum_{i = 1}^n\n    \\left &lt;\n      x_{i, \\bullet}, \\;\n      P \\;\n      x_{i, \\bullet}\n    \\right &gt;\n\\end{align}\n\\qquad(7.5)\\]\n\n\n7.3.2.2 Outline: reconstructing the feature matrix\nHere’s an outline of the reconstruction processs. At step \\(j\\) we define vector \\(v_{\\bullet, j} \\in \\mathbb{R}^d\\) that has unit norm and is orthogonal to any vector \\(v_{\\bullet, \\eta}\\) defined in any previous step. At the end of step \\(j\\) we have accumulated an orthonormal set of vectors \\((v_{\\bullet, 1}, \\ldots, v_{\\bullet, j})\\) that span a \\(j-\\)dimensional subspace, say \\(\\mathcal{R}^{(j)}\\), of \\(\\mathbb{R}^d\\). This orthonormal set defines the following orthogonal projection \\(P^{(j)}\\) from \\(\\mathbb{R}^d\\) to \\(\\mathcal{R}^{(j)}\\).\n\\[\n\\begin{align}\n  P^{(j)} \\; x_\\bullet\n  &=\n    \\sum_{\\eta = 1}^j\n      \\left &lt; x_\\bullet, v_{\\bullet, \\eta} \\right &gt;\n      \\times v_{\\bullet, \\eta} \\\\\n  &=\n    \\sum_{\\eta = 1}^j\n      v_{\\bullet, \\eta} \\times\n      \\left &lt; v_{\\bullet, \\eta}, x_\\bullet \\right &gt;  \\\\\n  &=\n    \\sum_{\\eta = 1}^j\n      v_{\\bullet, \\eta} \\; v_{\\bullet, \\eta}^\\top \\; x_\\bullet\n\\end{align}\n\\qquad(7.6)\\]\nIn other words, \\(P^{(j)}\\) is the sum of orthogonal projections, \\(v_{\\bullet, \\eta} \\; v_{\\bullet, \\eta}^\\top\\), to 1-dimensional subspaces that are mutually orthogonal.\nSince we will apply our loss function \\(\\mathcal{L} (P)\\) to \\(P^{(j)}\\), it will be useful to re-express the quadratic form appearing in Equation 7.5. 9\n\\[\n\\begin{align}\n  &\n  \\sum_{i = 1}^n\n    \\left &lt;\n      x_{i, \\bullet}, \\;\n      P^{(j)} \\;\n      x_{i, \\bullet}\n    \\right &gt; \\\\\n  &=\n  \\sum_{i = 1}^n\n      x_{i, \\bullet} \\;\n      P^{(j)} \\;\n      x_{i, \\bullet}^\\top \\\\\n  &=\n  \\sum_{i = 1}^n\n      x_{i, \\bullet} \\;\n      \\left (\n        \\sum_{\\eta = 1}^j\n            v_{\\bullet, \\eta} \\; v_{\\bullet, \\eta}^\\top \\;\n      \\right )\n      x_{i, \\bullet}^\\top \\\\\n  &=\n  \\sum_{i = 1}^n\n    \\sum_{\\eta = 1}^j\n      x_{i, \\bullet} \\;\n      v_{\\bullet, \\eta} \\;\n      v_{\\bullet, \\eta}^\\top \\;\n      x_{i, \\bullet}^\\top \\\\\n  &=\n  \\sum_{\\eta = 1}^j\n    \\sum_{i = 1}^n  \\;\n      v_{\\bullet, \\eta}^\\top \\;\n      x_{i, \\bullet}^\\top\n      x_{i, \\bullet} \\;\n      v_{\\bullet, \\eta} \\\\\n  &=\n  \\sum_{\\eta = 1}^j\n    v_{\\bullet, \\eta}^\\top \\;\n    \\left (\n    \\sum_{i = 1}^n  \\;\n      x_{i, \\bullet}^\\top\n      x_{i, \\bullet} \\;\n    \\right )\n    v_{\\bullet, \\eta} \\\\\n  &=\n  \\sum_{\\eta = 1}^j\n    v_{\\bullet, \\eta}^\\top \\;\n    \\left (\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet}\n    \\right )\n    v_{\\bullet, \\eta}\n\\end{align}\n\\qquad(7.7)\\]\n\n\n7.3.2.3 Details: reconstructing the feature matrix\nHere are the distinctive details of the reconstruction process. At step 1 we define vector \\(v_{\\bullet, 1}\\) as the unit vector that minimizes \\(\\mathcal{L} (v_\\bullet \\; v_\\bullet^\\top)\\) among all unit vectors \\(v_\\bullet \\in \\mathbb{R}^d\\). From Equation 7.5 and Equation 7.7 we have:\n\\[\n\\begin{align}\n  v_{\\bullet, 1}\n  &=  \n  \\arg \\min_{ \\Vert v_\\bullet \\rVert = 1 } \\;\n  \\mathcal{L} ( v_\\bullet \\; v_\\bullet^\\top ) \\\\\n  &=\n  \\arg \\max_{ \\Vert v_\\bullet \\rVert = 1 } \\;\n  \\sum_{i = 1}^n\n    \\left &lt;\n      x_{i, \\bullet}, \\;\n      v_\\bullet \\; v_\\bullet^\\top \\;\n      x_{i, \\bullet}\n    \\right &gt; \\\\\n  &=\n  \\arg \\max_{ \\Vert v_\\bullet \\rVert = 1 } \\;\n  v_\\bullet^\\top \\;\n  \\left (\n    X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n  \\right ) \\;\n  v_\\bullet\n\\end{align}\n\\qquad(7.8)\\]\nThe solution to Equation 7.8 is well known. The maximal value of \\(v_\\bullet^\\top \\; X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet} \\; v_\\bullet\\) is the largest eigenvalue, say \\(\\sigma_1^2\\), of \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\). And the vector \\(v_{\\bullet, 1}\\) that achieves that maximal value is the eigenvector corresponding to \\(\\sigma_1^2\\).\nHaving defined \\(v_{\\bullet, 1}\\), we proceed inductively, again with \\(r\\) defined as \\(r = rank \\left ( X_{\\bullet, \\bullet} \\right )\\). Suppose that we have defined an orthonormal set of vectors \\(v_{\\bullet, 1}, \\ldots, v_{\\bullet, j}\\), where \\(1 \\le j \\le r\\). As previously noted, this orthonormal set of vectors spans a \\(j-\\)dimensional \\(\\mathbb{R}^d\\) subspace, denoted \\(\\mathcal{R}^{(j)}\\), and also defines an orthogonal projection \\(P^{(j)}\\) from \\(\\mathbb{R}^d\\) to \\(\\mathcal{R}^{(j)}\\).\nIf \\(j = r\\) we terminate the reconstruction. Otherwise we define the next vector, \\(v_{\\bullet, j + 1}\\), as the unit vector that minimizes \\(\\mathcal{L} (Q^{(j + 1)})\\), where \\(Q^{(j + 1)} = P^{(j)} \\; + \\; v_{\\bullet} \\; v_{\\bullet}^\\top\\).\n\\[\n\\begin{align}\n  \\mathcal{L} (Q^{(j + 1)})\n  &=\n    \\sum_{i = 1}^n\n    \\left \\lVert\n    \\left (\n      \\mathcal{I} \\; - \\;\n      Q^{(j + 1)} (v_{\\bullet} \\; v_\\bullet^\\top) \\;\n    \\right )\n    x_{i, \\bullet}\n    \\right \\rVert^2 \\\\ \\\\\n  &\\text{where} \\\\ \\\\\n  Q^{(j + 1)} (v_{\\bullet} \\; v_{\\bullet}^\\top) &= P^{(j)} \\; + \\; v_{\\bullet} \\; v_{\\bullet}^\\top \\\\ \\\\\n  & \\text{subject to the constraints: } \\\\ \\\\\n  & \\lVert v_\\bullet \\rVert = 1 \\; \\text{ and } \\; v_\\bullet \\perp v_{\\bullet, \\eta} \\; \\text{ for } \\; \\eta \\le j\n\\end{align}\n\\qquad(7.9)\\]\nNote that the constraints on \\(v_\\bullet\\) ensure that \\(Q^{(j + 1)} (v_{\\bullet} \\; v_{\\bullet}^\\top)\\) is an orthogonal projection. Then, abbreviating and expanding Equation 7.9, we have:\n\\[\n\\begin{align}\n  \\mathcal{L}_{j + 1} \\left (\n    v_{\\bullet} \\; v_{\\bullet}^\\top\n  \\right )\n  &=\n    \\sum_{i = 1}^n\n    \\left \\lVert\n    \\left (\n      \\mathcal{I} \\; - \\;\n      Q^{(j + 1)} \\;\n    \\right )\n    x_{i, \\bullet}\n    \\right \\rVert^2 \\\\\n  &=\n    \\mathrm{Tr} \\left \\{\n      X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n      \\right \\} \\; - \\;\n    \\sum_{\\eta = 1}^j\n    v_{\\bullet, \\eta}^\\top \\;\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet} \\;\n    v_{\\bullet, \\eta} \\; - \\;\n    v_\\bullet^\\top \\;\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet} \\;\n    v_\\bullet\n\\end{align}\n\\qquad(7.10)\\]\nIt now follows that\n\\[\n\\begin{align}\n  v_{\\bullet, j + 1}\n  &=\n  \\arg \\max \\;\n  \\left \\{\n    v_\\bullet^\\top \\;\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet} \\;\n    v_\\bullet\n  \\right \\} \\\\ \\\\\n  &\\text{subject to: } \\\\ \\\\\n    &\\lVert v_\\bullet \\rVert = 1 \\;\n    \\text{ and } \\;\n    v_\\bullet \\perp v_{\\bullet, \\eta} \\; \\text{ for } \\eta \\le j\n\\end{align}\n\\qquad(7.11)\\]\nAgain, this problem of constrained maximization of a quadratic form is well known. The maximum value is \\(\\sigma_{j + 1}^2\\), the eigenvalue of \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\) of index \\(j + 1\\) in descending order. The vector \\(v_{\\bullet, j + 1}\\) achieving this maximum is the eigenvector corresponding to \\(\\sigma_{j + 1}^2\\).\nThis reconstruction process terminates with an orthonormal set of \\(d-\\)dimensional vectors, \\((v_{\\bullet, 1}, \\ldots, v_{\\bullet, r})\\), that span subspace \\(\\mathcal{R}^{(r)}\\) that is spanned by the rows of \\(X_{\\bullet, \\bullet}\\).\nNow let \\(D\\) denote the \\(r \\times r\\) diagonal matrix \\(diag(\\sigma_1, \\ldots, \\sigma_r)\\). From Equation 7.3 and Equation 7.10 we have\n\\[\n\\begin{align}\n  &\n    (n - 1) \\;\n    \\sum_{k = 1}^d var(x_{\\bullet, k})  \\\\\n  &=\n    \\sum_{k = 1}^d \\; \\lVert x_{\\bullet, k} \\rVert^2  \\\\\n  &=\n    \\mathrm{Tr}\n    \\left (\n      X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n    \\right )  \\\\\n  &=\n    \\sum_{\\eta = 1}^r\n    v_{\\bullet, \\eta}^\\top \\;\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet} \\;\n    v_{\\bullet, \\eta} \\\\\n  &=\n    \\sum_{\\eta = 1}^r\n    v_{\\bullet, \\eta}^\\top \\;\n    ( \\sigma_\\eta^2 \\; v_{\\bullet, \\eta} ) \\\\\n  &=\n    \\sum_{\\eta = 1}^r \\sigma_\\eta^2 \\\\\n  &= \\mathrm{Tr} \\left ( D^2 \\right )  \\\\ \\\\\n  &\\text{where} \\\\ \\\\\n  & D = diag(\\sigma_1, \\ldots, \\sigma_r)\n\\end{align}\n\\qquad(7.12)\\]\nThat is, the sum of the eigenvalues \\(\\sigma_\\eta^2\\) is equal to the sum of squared feature-norms, \\(\\lVert x_{\\bullet, k} \\rVert^2\\).\n\n\n7.3.2.4 Matrix decompositions\nIn the previous section we reconstructed rows of the feature matrix \\(X_{\\bullet, \\bullet}\\) using a sequence of orthogonal projections \\(v_{\\bullet, \\eta} \\; v_{\\bullet, \\eta}^\\top\\). This development gave us a partial eigen-decomposition of the \\(d \\times d\\) matrix \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\), the scaled feature covariance matrix. With \\(r = rank(X_{\\bullet, \\bullet})\\), and for \\(\\eta \\in \\{1, \\ldots, r\\}\\) the \\(d-\\)dimensional vector \\(v_{\\bullet, \\eta}\\) is an eigenvector of \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\), having corresponding eigenvector \\(\\sigma_\\eta^2\\), indexed in descending magnitude. In matrix notation we have:\n\\[\n\\begin{align}\n  & \\left (\n    X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n  \\right ) \\;\n    (v_{\\bullet, 1}, \\ldots, v_{\\bullet, r}) \\\\\n  &=\n  (v_{\\bullet, 1}, \\ldots, v_{\\bullet, r}) \\; D^2\n\\end{align}\n\\qquad(7.13)\\]\nMatrix \\(X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\\) is non-negative definite, which means that it has \\(d\\) non-negative eigenvalues and corresponding eigenvectors. The first \\(r\\) eigenvalues are positive. Arranged in descending order, they are the diagonal elements of \\(D^2 = diag(\\sigma_1^2, \\ldots, \\sigma_r^2)\\). If \\(r = d\\) the matrix is strictly positive-definite, and its complete eigen-decomposition is given by Equation 7.13.\nSuppose now that \\(r &lt; d\\). Then any unit vector orthogonal to \\(\\mathcal{R}^{(r)}\\), the \\(d-\\)dimensional subspace spanned by \\((v_{\\bullet, 1}, \\ldots, v_{\\bullet, r})\\), qualifies as an eigenvector (a “null” eigenvector, we’ll say) having an eigenvalue of zero. Therefore we can complement eigenvectors \\((v_{\\bullet, 1}, \\ldots, v_{\\bullet, r})\\) with an orthonormal set of null eigenvectors \\((v_{\\bullet, r + 1}, \\ldots, v_{\\bullet, d})\\). The combined set of eigenvectors is a complete orthonormal basis for \\(\\mathbb{R}^d\\).\nWe thus have two cases to consider: \\(r &lt; d\\); or \\(r = d\\). In either case we now have a complete orthonormal basis represented by the \\(d \\times d\\) matrix \\(V = (v_{\\bullet, 1}, \\ldots, v_{\\bullet, d})\\). We delineate the two cases as follows.\n\\[\n\\begin{align}\n  V_1 &= (v_{\\bullet, 1}, \\ldots, v_{\\bullet, r}) \\\\\n  V_2 &=\n    \\begin{cases}\n      (v_{\\bullet, r + 1}, \\ldots, v_{\\bullet, d}) & \\text{if } r &lt; d \\\\\n      \\emptyset & \\text{if } r = d\n    \\end{cases} \\\\\n  V &= (V_1, V_2)\n\\end{align}\n\\qquad(7.14)\\]\nWe also record the full set of eigenvalues in the \\(d \\times d\\) diagonal matrix \\(\\Sigma^2\\) as follows.\n\\[\n\\begin{align}\n  \\Sigma^2 &=\n    \\begin{cases}\n      \\begin{pmatrix}\n        D^2 & 0 \\\\\n        0   & 0\n    \\end{pmatrix}\n    & \\text{if } r &lt; d \\\\\n      D^2 & \\text{if } r = d\n    \\end{cases}\n\\end{align}\n\\qquad(7.15)\\]\nThis enables us to re-express Equation 7.13 as follows.\n\\[\n\\begin{align}\n  \\left (\n    X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n  \\right ) \\;\n    V\n  &=\n    V \\; \\Sigma^2\n\\end{align}\n\\qquad(7.16)\\]\nMatrix \\(V\\) is an orthonormal rotation of \\(\\mathbb{R}^d\\), so that \\(V^\\top = V^{-1}\\). Multiplying Equation 7.16 on the right by \\(V^\\top\\) we obtain:\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n  &=\n    V \\; \\Sigma^2 \\; V^\\top\n\\end{align}\n\\qquad(7.17)\\]\nThis last equation can be re-expressed as an eigen-decomposition of the feature covariance matrix:\n\\[\n\\begin{align}\n  & cov(X_{\\bullet, \\bullet}) \\\\\n  &=\n    \\frac{1}{n-1} X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet} \\\\\n  &= V \\;\n    \\left (\n      \\frac{1}{n-1} \\Sigma^2 \\;\n    \\right )\n    V^\\top\n\\end{align}\n\\qquad(7.18)\\]\nWe now define \\((c_{\\bullet, 1}, \\ldots, c_{\\bullet, r})\\), the principal components of \\(X_{\\bullet, \\bullet}\\), as the following respective linear combinations of the columns of \\(X_{\\bullet, \\bullet}\\).\n\\[\n\\begin{align}\n  c_{\\bullet, \\eta}\n  &=\n    X_{\\bullet, \\bullet} \\; v_{\\bullet, \\eta}\n    & \\text{ for } \\eta \\in \\{ 1, \\ldots, r \\}\n\\end{align}\n\\qquad(7.19)\\]\nNote that distinct principal components are orthogonal, and the squared magnitude of \\(c_{\\bullet, \\eta}\\) is \\(\\sigma_\\eta^2\\):\n\\[\n\\begin{align}\n  &\n  c_{\\bullet, \\kappa}^\\top \\; c_{\\bullet, \\eta} \\\\\n  &=\n    v_{\\bullet, \\kappa}^\\top \\;\n    X_{\\bullet, \\bullet}^\\top \\;\n    X_{\\bullet, \\bullet} \\;\n    v_{\\bullet, \\eta} \\\\\n  &=\n    v_{\\bullet, \\kappa}^\\top \\;\n    \\left (\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet} \\;\n      v_{\\bullet, \\eta}\n    \\right ) \\\\\n  &=\n    v_{\\bullet, \\kappa}^\\top \\;\n    \\left (\n      \\sigma_\\eta^2 \\;\n      v_{\\bullet, \\eta}\n    \\right ) \\\\\n  &=\n    \\sigma_\\eta^2 \\;\n    v_{\\bullet, \\kappa}^\\top \\;\n    v_{\\bullet, \\eta}  \\\\\n  &=\n    \\begin{cases}\n      \\sigma_\\eta^2 & \\text{ if } \\kappa = \\eta \\\\\n      0 & \\text{ if } \\kappa \\ne \\eta\n    \\end{cases}\n\\end{align}\n\\qquad(7.20)\\]\nDividing \\(c_{\\bullet, \\eta}\\) by its magnitude \\(\\sigma_\\eta\\), we obtain a unit vector \\(u_{\\bullet, \\eta}\\) that represents the direction of \\(c_{\\bullet, \\eta}\\).\n\\[\n\\begin{align}\n  u_{\\bullet, \\eta}\n  &=\n    \\frac{ c _{\\bullet, \\eta} }{ \\lVert c _{\\bullet, \\eta} \\rVert } \\\\\n  &=\n    \\frac{ c _{\\bullet, \\eta} }{ \\sigma_\\eta }\n    & \\text{ for } \\eta \\in \\{ 1, \\ldots, r \\}\n\\end{align}\n\\qquad(7.21)\\]\nSince the principal components are pair-wise orthogonal, the unit vectors just defined make up an orthonormal set of \\(n-\\)dimensional vectors. Note that\n\\[\n\\begin{align}\n  u_{\\bullet, \\eta}^\\top \\; X_{\\bullet, \\bullet}\n  &=\n    \\frac{ 1 }{ \\sigma_\\eta } \\;\n    c_{\\bullet, \\eta}^\\top \\; X_{\\bullet, \\bullet} \\\\\n  &=\n    \\frac{ 1 }{ \\sigma_\\eta } \\;\n    \\left (\n      X_{\\bullet, \\bullet} \\; v_{\\bullet, \\eta}\n    \\right )^\\top   \\;\n    X_{\\bullet, \\bullet} \\\\\n  &=\n    \\frac{ 1 }{ \\sigma_\\eta } \\;\n    \\left (\n      v_{\\bullet, \\eta}^\\top \\; X_{\\bullet, \\bullet}^\\top  \n    \\right )   \\;\n    X_{\\bullet, \\bullet} \\\\\n  &=\n    \\frac{ 1 }{ \\sigma_\\eta } \\;\n      v_{\\bullet, \\eta}^\\top \\;\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet} \\\\\n  &=\n    \\frac{ 1 }{ \\sigma_\\eta } \\;\n    \\left (\n      X_{\\bullet, \\bullet}^\\top \\;\n      X_{\\bullet, \\bullet} \\;\n      v_{\\bullet, \\eta}\n    \\right )^\\top \\\\\n  &=\n    \\frac{ 1 }{ \\sigma_\\eta } \\;\n    \\left (\n      \\sigma_\\eta^2 \\;\n      v_{\\bullet, \\eta}\n    \\right )^\\top  \\\\\n  &=\n    \\sigma_\\eta \\;\n    v_{\\bullet, \\eta}^\\top\n\\end{align}\n\\qquad(7.22)\\]\nWe now define \\(U_1 = ( u_{\\bullet, 1}, \\ldots, u_{\\bullet, r} )\\) to be the \\(n \\times r\\) matrix having these \\(r\\) unit vectors as its columns. And we extend, if needed, this set of unit vectors to create an orthonormal basis for \\(\\mathbb{R}^n\\). That is, if \\(r &lt; n\\) then the vector space spanned by the columns of \\(U_1\\) is a proper subspace of \\(\\mathbb{R}^n\\). The orthogonal complement to this subspace has dimension \\(n - r\\), and thus admits an orthonormal basis, which we label as \\(( u_{\\bullet, r + 1}, \\ldots, u_{\\bullet, n} )\\). And we define \\(U_2\\) to be the matrix having these vectors as its columns. Finally, we define the \\(n \\times n\\) orthogonal matrix \\(U = ( u_{\\bullet, 1}, \\ldots, u_{\\bullet, n} )\\).\n\\[\n\\begin{align}\n  U_1 &= (u_{\\bullet, 1}, \\ldots, u_{\\bullet, r}) \\\\\n  U_2 &=\n    \\begin{cases}\n      (u_{\\bullet, r + 1}, \\ldots, u_{\\bullet, n}) & \\text{if } r &lt; n \\\\\n      \\emptyset & \\text{if } r = n\n    \\end{cases} \\\\\n  U &= (U_1, U_2)\n\\end{align}\n\\qquad(7.23)\\]\nWe can now re-express Equation 7.22 as follows.\n\\[\n\\begin{align}\n  U^\\top \\; X_{\\bullet, \\bullet}\n  &=\n    \\Sigma \\; V^\\top\n\\end{align}\n\\qquad(7.24)\\]\nMultiplying this last equation on the left by matrix \\(U\\) we obtain:\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet}\n  &=\n    U \\; \\Sigma \\; V^\\top\n\\end{align}\n\\qquad(7.25)\\]\nThis factorization of the feature matrix \\(X_{\\bullet, \\bullet}\\) is called a singular value decomposition (SVD).\n\n\n\n7.3.3 Definitions [OLD]\nWe now define PCA mathematically in vector-matrix notation. For convenience of notation assume that the feature matrix \\(X_{\\bullet, \\bullet}\\) has been centered, that is, for each column of the original feature matrix, the average value has been calculated and subtracted. Therefore the average (or equivalently the sum) of each column of \\(X_{\\bullet, \\bullet}\\) equals zero.\n\\[\n\\begin{align}\n  X_{\\bullet, \\bullet} &= (x_{\\bullet, 1}, \\ldots, x_{\\bullet, d}) \\\\ \\\\\n  & \\text{with} \\\\ \\\\\n  1_\\bullet^\\top \\; x_{\\bullet, k} &= \\sum_{i = 1}^n x_{i, k} = 0 \\\\ \\\\\n  &\\text{for } k \\in \\{ 1, \\ldots, d \\} \\\\ \\\\\n  & \\text{so that} \\\\ \\\\\n  1_\\bullet^\\top \\; X_{\\bullet, \\bullet} &= 0_\\bullet \\in \\mathbb{R}^d\n\\end{align}\n\\qquad(7.26)\\]\nThe first step in this reconstruction is to seek a vector in feature space, say \\(z_\\bullet = X_{\\bullet, \\bullet} \\; \\gamma_\\bullet\\) that minimizes the following squared error loss function, say \\(\\mathcal{L} (\\gamma_\\bullet)\\).\n\\[\n\\begin{align}\n  \\mathcal{L} (\\gamma_\\bullet) &=\n    \\sum_{i = 1}^n\n    \\lVert\n      x_{i, \\bullet} \\; - \\;\n    \\rVert\n\\end{align}\n\\qquad(7.27)\\]\nThe first principal component of \\(X_{\\bullet, \\bullet}\\) is determined by the “direction” in feature space of maximum variance.\nTo elaborate, each vector of coefficients \\(\\gamma_\\bullet \\in \\mathbb{R}^d\\) can be mapped to a linear combination of feature vectors, \\(X_{\\bullet, \\bullet} \\gamma_\\bullet\\). Such linear combinations constitute a subspace of \\(\\mathbb{R}^n\\) called “feature space”. Each non-zero coefficient vector can be expressed as the product of a scalar magnitude and a unit vector, namely:\n\\[\n\\begin{align}\n  \\gamma_\\bullet &= \\lVert \\gamma_\\bullet \\rVert \\times\n  \\left (\n    \\frac{\\gamma_\\bullet}{\\lVert \\gamma_\\bullet \\rVert}\n  \\right )\n    & \\text{ for } \\gamma_\\bullet \\ne 0_\\bullet\n\\end{align}\n\\qquad(7.28)\\]\nThe above unit vector is called the direction of vector \\(\\gamma_\\bullet\\). Now the first principal component of \\(X_{\\bullet, \\bullet}\\), is defined by the coefficient unit vector, say \\(u_{\\bullet, 1}\\), that maximizes the norm of \\(X_{\\bullet, \\bullet} u_\\bullet\\) among all unit vectors \\(u_\\bullet \\in \\mathbb{R}^d\\).\n\\[\n\\begin{align}\n  u_{\\bullet, 1} &=\n  \\arg \\max_{\\lVert u_\\bullet \\rVert = 1}\n  \\Vert X_{\\bullet, \\bullet} \\; u_\\bullet \\rVert\n\\end{align}\n\\qquad(7.29)\\]\nThen the first principal component, denoted \\(z_{\\bullet, 1}\\), is the feature space vector defined by \\(u_{\\bullet, 1}\\).\n\\[\n\\begin{align}\n  z_{\\bullet, 1} &= X_{\\bullet, \\bullet} \\; u_{\\bullet, 1}\n\\end{align}\n\\qquad(7.30)\\]\nNote that the elements of the first principal component \\(z_{\\bullet, 1}\\) sum to zero, since this is true of each feature vector, and \\(z_{\\bullet, 1}\\) is a linear combination of the feature vectors.\n\\[\n\\begin{align}\n  1_\\bullet^\\top z_{\\bullet, 1} &=\n  1_\\bullet^\\top X_{\\bullet, \\bullet} \\; u_{\\bullet, 1} = 0 \\\\ \\\\\n  & \\text{so that} \\\\ \\\\\n  \\bar{z}_1 &= \\frac{1}{n} \\sum_{i = 1}^n z_{i, 1} \\\\\n  &= \\frac{1}{n} \\; 1_\\bullet^\\top \\;  z_{\\bullet, 1} \\\\\n  &= 0\n\\end{align}\n\\qquad(7.31)\\]\nTherefore, the sample variance of \\(z_{\\bullet, 1}\\) is:\n\\[\n\\begin{align}\n  var ( z_{\\bullet, 1} ) &=\n  \\frac{1}{n - 1} \\sum_{i = 1}^n (z_{i, 1} - \\bar{z}_1)^2 \\\\\n  &=\n  \\frac{1}{n - 1} \\sum_{i = 1}^n z_{i, 1}^2 \\\\\n  &= \\frac{1}{n - 1} \\; \\Vert z_{\\bullet, 1} \\rVert^2\n\\end{align}\n\\qquad(7.32)\\]\nThat is, by maximizing the norm \\(\\Vert z_{\\bullet, 1} \\rVert\\) the coefficient vector \\(u_{\\bullet, 1}\\) maximizes the sample variance of \\(z_{\\bullet, 1}\\), which was the intended outcome.\nWe define the remaining principal components inductively as follows. Suppose the we have defined principal components \\(z_{\\bullet, 1}, \\ldots, z_{\\bullet, k}\\), where \\(1 \\le k \\le d\\). These vectors span a \\(k-\\)dimensional subspace of feature space. If the subspace is in fact all of feature space, we seek no additional principal components. Otherwise we define the next principal component as the linear combination \\(X_{\\bullet, \\bullet} \\; u_{\\bullet, k + 1}\\) of maximal norm among all such linear combinations that are orthogonal to each of the preceding \\(k\\) principal components.\n\\[\n\\begin{align}\n  u_{\\bullet, k + 1} &=\n  \\arg \\max_{\\lVert u_\\bullet \\rVert = 1}\n  \\left \\{\n  \\Vert X_{\\bullet, \\bullet} \\; u_\\bullet \\rVert : \\;\n    X_{\\bullet, \\bullet} \\; u_\\bullet \\; \\perp \\; z_{\\bullet, \\kappa}, \\forall \\kappa\n  \\right \\}\n\\end{align}\n\\qquad(7.33)\\]\nLet’s take a moment to describe this constraint that the new principal component be orthogonal to its predecessors. The constraint amounts to setting to zero the following quadratic form defined for pairs of coefficient vectors:\n\\[\n\\begin{align}\n  0 &= \\left &lt;\n  X_{\\bullet, \\bullet} \\; u_\\bullet, \\; z_{\\bullet, \\kappa}\n  \\right &gt; \\\\\n  &= u_\\bullet^\\top \\; X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet} \\; u_{\\bullet, \\kappa} \\\\\n  &= u_\\bullet^\\top \\; \\left ( X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet} \\right ) \\; u_{\\bullet, \\kappa}\n\\end{align}\n\\qquad(7.34)\\]\nBecause \\(X_{\\bullet, \\bullet}\\) is centered, the \\(d \\times d\\) matrix defining the quadratic form above is a multiple of the sample covariance matrix among features.\n\\[\n\\begin{align}\n  cov \\left ( X_{\\bullet, \\bullet} \\right )\n  &= \\frac{1}{n - 1}\n  \\left (\n  X_{\\bullet, \\bullet}^\\top \\; X_{\\bullet, \\bullet}\n  \\right )\n\\end{align}\n\\qquad(7.35)\\]\nWe use this identity in Section 7.5 to discuss algorithms for calculating principal components.\nThis inductive process terminates with a complete set of \\(r\\) principal components, \\(\\{ z_{\\bullet, 1}, \\ldots, z_{\\bullet, r} \\}\\), where \\(r\\) is the dimension of feature space, or equivalently \\(r = rank(X_{\\bullet, \\bullet})\\), the rank of feature matrix \\(X_{\\bullet, \\bullet}\\), with \\(r \\le \\min(n, d)\\).\nThe principal components thus span feature space. In fact they constitute an ordered orthogonal basis of decreasing norms. Therefore each feature vector can be expressed as a linear combination of the principal components:\n\\[\n\\begin{align}\n  x_{\\bullet, j}\n  &=\n    \\sum_{k = 1}^r\n      \\left &lt; x_{\\bullet, j}, z_{\\bullet, k} \\right &gt; \\times\n      \\frac{ z_{\\bullet, k} } { \\Vert z_{\\bullet, k} \\rVert^2 } \\\\\n  &=\n    \\sum_{k = 1}^r\n      \\frac{ z_{\\bullet, k} } { \\Vert z_{\\bullet, k} \\rVert^2 } \\times\n      \\left &lt; z_{\\bullet, k}, x_{\\bullet, j} \\right &gt;  \\\\\n  &=\n  (z_{\\bullet, 1}, \\ldots, z_{\\bullet, r}) \\;\n  D^{-2} \\;\n  (z_{\\bullet, 1}, \\ldots, z_{\\bullet, r})^\\top \\;\n  x_{\\bullet, j} \\\\ \\\\\n  & \\text{where} \\\\ \\\\\n  D &= diag\n  \\left (\n    \\lVert z_{\\bullet, 1} \\rVert, \\ldots, \\lVert z_{\\bullet, r} \\rVert\n  \\right )\n\\end{align}\n\\qquad(7.36)\\]",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-supervised",
    "href": "reduce-dim.html#sec-dim-r-supervised",
    "title": "7  Dimension Reduction",
    "section": "7.4 Supervised Dimension Reduction",
    "text": "7.4 Supervised Dimension Reduction",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-computation",
    "href": "reduce-dim.html#sec-dim-r-computation",
    "title": "7  Dimension Reduction",
    "section": "7.5 Computational Considerations",
    "text": "7.5 Computational Considerations",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-choose-method",
    "href": "reduce-dim.html#sec-dim-r-choose-method",
    "title": "7  Dimension Reduction",
    "section": "7.6 Choosing and Evaluating Methods",
    "text": "7.6 Choosing and Evaluating Methods",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-other-methods",
    "href": "reduce-dim.html#sec-dim-r-other-methods",
    "title": "7  Dimension Reduction",
    "section": "7.7 Other Methods",
    "text": "7.7 Other Methods",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-summary",
    "href": "reduce-dim.html#sec-dim-r-summary",
    "title": "7  Dimension Reduction",
    "section": "7.8 Summary",
    "text": "7.8 Summary",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "reduce-dim.html#sec-dim-r-exercises",
    "href": "reduce-dim.html#sec-dim-r-exercises",
    "title": "7  Dimension Reduction",
    "section": "7.9 Exercises",
    "text": "7.9 Exercises\n\n\n\n\nBellman, Richard Ernest. 1957. Dynamic Programming. Princeton University Press.\n\n\nCortez, Paulo, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009. “Wine Quality.” https://doi.org/https://doi.org/10.24432/C56S3T.\n\n\nCortez, P., Antonio Luíz Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. 2009. “Modeling Wine Preferences by Data Mining from Physicochemical Properties.” Decis. Support Syst. 47: 547–53. https://api.semanticscholar.org/CorpusID:2996254.\n\n\n“Curse of Dimensionality | Wikipedia.” 2025. Wikipedia, November. https://en.wikipedia.org/wiki/Curse_of_dimensionality.\n\n\nDonoho, David L. 2000. “High-Dimensional Data Analysis: The Curses and Blessings of Dimensionality,” 1–32. https://www.researchgate.net/publication/220049061_High-Dimensional_Data_Analysis_The_Curses_and_Blessings_of_Dimensionality.\n\n\nKoklu, Mehmet, and Ibrahim A Ozkan. 2020. “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture 174: 105507.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. O’Reilly Media. https://www.tmwr.org/.\n\n\nMcNeil, Donald R. 1977. Interactive Data Analysis: A Practical Primer. New York, USA: John Wiley & Sons.\n\n\nRoss, Douglas T, Uwe Scherf, Michael B Eisen, Charles M Perou, Christian Rees, Paul Spellman, Vishwanath Iyer, et al. 2000. “Systematic Variation in Gene Expression Patterns in Human Cancer Cell Lines.” Nature Genetics 24: 227–35. https://doi.org/10.1038/73432.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  }
]