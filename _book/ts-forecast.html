<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Send comments to: Tony T (adthral)">
<meta name="author" content="Send comments to: Tony T (tthrall)">

<title>12&nbsp; Time Series Forecasting – EDA for Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ts-fourier.html" rel="next">
<link href="./ts-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-364982630eef5352dd1537128a8ed5cb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ts-intro.html">Time Series Data</a></li><li class="breadcrumb-item"><a href="./ts-forecast.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Time Series Forecasting</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">EDA for Machine Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tthrall/eda4ml/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conditioning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Clustering: EDA in Higher Dimensions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Statistical Simulation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./study-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sampling and Study Design</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Linear Algebra</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./la-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Some Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reduce-dim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Dimension Reduction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Text Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Text Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dirichlet-dstn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Dirichlet Distribution</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./latent-dirichlet-alloc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Latent Dirichlet Allocation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Time Series Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ts-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Time Series Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ts-forecast.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Time Series Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ts-fourier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Time Series Spectrum Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Graph Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./graph-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Graph Theory for Machine Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./em-algorithm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">EM: the Expectation-Maximization Algorithm</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">12.1</span> Introduction</a></li>
  <li><a href="#mathematical-framework" id="toc-mathematical-framework" class="nav-link" data-scroll-target="#mathematical-framework"><span class="header-section-number">12.2</span> Mathematical Framework</a>
  <ul class="collapse">
  <li><a href="#data-derived-from-a-random-process" id="toc-data-derived-from-a-random-process" class="nav-link" data-scroll-target="#data-derived-from-a-random-process"><span class="header-section-number">12.2.1</span> Data derived from a random process</a></li>
  <li><a href="#stationarity" id="toc-stationarity" class="nav-link" data-scroll-target="#stationarity"><span class="header-section-number">12.2.2</span> Stationarity</a></li>
  <li><a href="#acf-autocorrelation-function" id="toc-acf-autocorrelation-function" class="nav-link" data-scroll-target="#acf-autocorrelation-function"><span class="header-section-number">12.2.3</span> ACF: autocorrelation function</a></li>
  <li><a href="#pacf-partial-autocorrelation-function" id="toc-pacf-partial-autocorrelation-function" class="nav-link" data-scroll-target="#pacf-partial-autocorrelation-function"><span class="header-section-number">12.2.4</span> PACF: partial autocorrelation function</a></li>
  </ul></li>
  <li><a href="#operations-on-time-series" id="toc-operations-on-time-series" class="nav-link" data-scroll-target="#operations-on-time-series"><span class="header-section-number">12.3</span> Operations on Time Series</a>
  <ul class="collapse">
  <li><a href="#transformation-of-each-observation" id="toc-transformation-of-each-observation" class="nav-link" data-scroll-target="#transformation-of-each-observation"><span class="header-section-number">12.3.1</span> Transformation of Each Observation</a></li>
  <li><a href="#smoothing" id="toc-smoothing" class="nav-link" data-scroll-target="#smoothing"><span class="header-section-number">12.3.2</span> Smoothing</a></li>
  <li><a href="#differences-of-successive-observations" id="toc-differences-of-successive-observations" class="nav-link" data-scroll-target="#differences-of-successive-observations"><span class="header-section-number">12.3.3</span> Differences of Successive Observations</a></li>
  <li><a href="#seasonal-differencing" id="toc-seasonal-differencing" class="nav-link" data-scroll-target="#seasonal-differencing"><span class="header-section-number">12.3.4</span> Seasonal Differencing</a></li>
  <li><a href="#integration-restoring-differenced-data" id="toc-integration-restoring-differenced-data" class="nav-link" data-scroll-target="#integration-restoring-differenced-data"><span class="header-section-number">12.3.5</span> Integration: Restoring Differenced Data</a></li>
  <li><a href="#filtering" id="toc-filtering" class="nav-link" data-scroll-target="#filtering"><span class="header-section-number">12.3.6</span> Filtering</a></li>
  </ul></li>
  <li><a href="#model-components" id="toc-model-components" class="nav-link" data-scroll-target="#model-components"><span class="header-section-number">12.4</span> Model Components</a>
  <ul class="collapse">
  <li><a href="#covariates-and-trend" id="toc-covariates-and-trend" class="nav-link" data-scroll-target="#covariates-and-trend"><span class="header-section-number">12.4.1</span> Covariates and Trend</a></li>
  <li><a href="#auto-regression-ar" id="toc-auto-regression-ar" class="nav-link" data-scroll-target="#auto-regression-ar"><span class="header-section-number">12.4.2</span> Auto-regression (AR)</a></li>
  <li><a href="#moving-average-ma" id="toc-moving-average-ma" class="nav-link" data-scroll-target="#moving-average-ma"><span class="header-section-number">12.4.3</span> Moving Average (MA)</a></li>
  </ul></li>
  <li><a href="#armap-q" id="toc-armap-q" class="nav-link" data-scroll-target="#armap-q"><span class="header-section-number">12.5</span> ARMA(p, q)</a>
  <ul class="collapse">
  <li><a href="#co-prime-polynomials" id="toc-co-prime-polynomials" class="nav-link" data-scroll-target="#co-prime-polynomials"><span class="header-section-number">12.5.1</span> Co-prime Polynomials</a></li>
  <li><a href="#causal-form" id="toc-causal-form" class="nav-link" data-scroll-target="#causal-form"><span class="header-section-number">12.5.2</span> Causal Form</a></li>
  <li><a href="#inverted-form" id="toc-inverted-form" class="nav-link" data-scroll-target="#inverted-form"><span class="header-section-number">12.5.3</span> Inverted Form</a></li>
  </ul></li>
  <li><a href="#linear-predictors" id="toc-linear-predictors" class="nav-link" data-scroll-target="#linear-predictors"><span class="header-section-number">12.6</span> Linear Predictors</a>
  <ul class="collapse">
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement"><span class="header-section-number">12.6.1</span> Problem Statement</a></li>
  <li><a href="#linear-equations" id="toc-linear-equations" class="nav-link" data-scroll-target="#linear-equations"><span class="header-section-number">12.6.2</span> Linear Equations</a></li>
  </ul></li>
  <li><a href="#arma-forecastingtxt-arma-forecasting" id="toc-arma-forecastingtxt-arma-forecasting" class="nav-link" data-scroll-target="#arma-forecastingtxt-arma-forecasting"><span class="header-section-number">12.7</span> ARMA Forecasting</a>
  <ul class="collapse">
  <li><a href="#notation-and-assumptions" id="toc-notation-and-assumptions" class="nav-link" data-scroll-target="#notation-and-assumptions"><span class="header-section-number">12.7.1</span> Notation and Assumptions</a></li>
  <li><a href="#predicted-variable" id="toc-predicted-variable" class="nav-link" data-scroll-target="#predicted-variable"><span class="header-section-number">12.7.2</span> Predicted Variable</a></li>
  <li><a href="#predictor-causal-form" id="toc-predictor-causal-form" class="nav-link" data-scroll-target="#predictor-causal-form"><span class="header-section-number">12.7.3</span> Predictor: Causal Form</a></li>
  <li><a href="#predictor-inverted-form" id="toc-predictor-inverted-form" class="nav-link" data-scroll-target="#predictor-inverted-form"><span class="header-section-number">12.7.4</span> Predictor: Inverted Form</a></li>
  <li><a href="#predictor-residuals" id="toc-predictor-residuals" class="nav-link" data-scroll-target="#predictor-residuals"><span class="header-section-number">12.7.5</span> Predictor Residuals</a></li>
  <li><a href="#truncated-predictorstxt-trunc-predict" id="toc-truncated-predictorstxt-trunc-predict" class="nav-link" data-scroll-target="#truncated-predictorstxt-trunc-predict"><span class="header-section-number">12.7.6</span> Truncated Predictors</a></li>
  <li><a href="#arma1-1-simulationtxt-arma_1_1_sim" id="toc-arma1-1-simulationtxt-arma_1_1_sim" class="nav-link" data-scroll-target="#arma1-1-simulationtxt-arma_1_1_sim"><span class="header-section-number">12.7.7</span> ARMA(1, 1) Simulation</a></li>
  <li><a href="#recruitment-forecast" id="toc-recruitment-forecast" class="nav-link" data-scroll-target="#recruitment-forecast"><span class="header-section-number">12.7.8</span> Recruitment Forecast</a></li>
  </ul></li>
  <li><a href="#non-stationary-models" id="toc-non-stationary-models" class="nav-link" data-scroll-target="#non-stationary-models"><span class="header-section-number">12.8</span> Non-Stationary Models</a>
  <ul class="collapse">
  <li><a href="#arimap-d-q" id="toc-arimap-d-q" class="nav-link" data-scroll-target="#arimap-d-q"><span class="header-section-number">12.8.1</span> ARIMA(p, d, q)</a></li>
  <li><a href="#random-walk-with-drift" id="toc-random-walk-with-drift" class="nav-link" data-scroll-target="#random-walk-with-drift"><span class="header-section-number">12.8.2</span> Random Walk with Drift</a></li>
  <li><a href="#sec-EWMA" id="toc-sec-EWMA" class="nav-link" data-scroll-target="#sec-EWMA"><span class="header-section-number">12.8.3</span> EWMA Example</a></li>
  <li><a href="#sec-ETS" id="toc-sec-ETS" class="nav-link" data-scroll-target="#sec-ETS"><span class="header-section-number">12.8.4</span> ETS</a></li>
  <li><a href="#seasonal-arima-models-txt-sarima-models" id="toc-seasonal-arima-models-txt-sarima-models" class="nav-link" data-scroll-target="#seasonal-arima-models-txt-sarima-models"><span class="header-section-number">12.8.5</span> Seasonal ARIMA Models </a></li>
  <li><a href="#co2-example-txt-cardox-xmpl" id="toc-co2-example-txt-cardox-xmpl" class="nav-link" data-scroll-target="#co2-example-txt-cardox-xmpl"><span class="header-section-number">12.8.6</span> CO2 Example </a></li>
  </ul></li>
  <li><a href="#closing-remarks" id="toc-closing-remarks" class="nav-link" data-scroll-target="#closing-remarks"><span class="header-section-number">12.9</span> Closing Remarks</a></li>
  <li><a href="#a-glossary-of-math-symbols" id="toc-a-glossary-of-math-symbols" class="nav-link" data-scroll-target="#a-glossary-of-math-symbols"><span class="header-section-number">12.10</span> A: Glossary of Math Symbols</a></li>
  <li><a href="#b-data-examples" id="toc-b-data-examples" class="nav-link" data-scroll-target="#b-data-examples"><span class="header-section-number">12.11</span> B: Data Examples</a>
  <ul class="collapse">
  <li><a href="#hare-and-lynx-populations" id="toc-hare-and-lynx-populations" class="nav-link" data-scroll-target="#hare-and-lynx-populations"><span class="header-section-number">12.11.1</span> Hare and Lynx Populations</a></li>
  <li><a href="#jj-quarterly-earnings" id="toc-jj-quarterly-earnings" class="nav-link" data-scroll-target="#jj-quarterly-earnings"><span class="header-section-number">12.11.2</span> JJ Quarterly Earnings</a></li>
  <li><a href="#global-temperatures" id="toc-global-temperatures" class="nav-link" data-scroll-target="#global-temperatures"><span class="header-section-number">12.11.3</span> Global Temperatures</a></li>
  <li><a href="#dow-jones-industrial-average" id="toc-dow-jones-industrial-average" class="nav-link" data-scroll-target="#dow-jones-industrial-average"><span class="header-section-number">12.11.4</span> Dow Jones Industrial Average</a></li>
  <li><a href="#cardiovascular-mortality-in-los-angeles" id="toc-cardiovascular-mortality-in-los-angeles" class="nav-link" data-scroll-target="#cardiovascular-mortality-in-los-angeles"><span class="header-section-number">12.11.5</span> Cardiovascular Mortality in Los Angeles</a></li>
  <li><a href="#el-niño-and-fish-population" id="toc-el-niño-and-fish-population" class="nav-link" data-scroll-target="#el-niño-and-fish-population"><span class="header-section-number">12.11.6</span> El Niño and Fish Population</a></li>
  <li><a href="#co2-mauna-loa-observatory" id="toc-co2-mauna-loa-observatory" class="nav-link" data-scroll-target="#co2-mauna-loa-observatory"><span class="header-section-number">12.11.7</span> CO2, Mauna Loa Observatory</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">12.12</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/tthrall/eda4ml/edit/main/ts-forecast.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/tthrall/eda4ml/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ts-intro.html">Time Series Data</a></li><li class="breadcrumb-item"><a href="./ts-forecast.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Time Series Forecasting</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ts-forecasting" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Time Series Forecasting</span></span></h1>
<p class="subtitle lead">An Introduction</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Send comments to: Tony T (adthral) </p>
             <p>Send comments to: Tony T (tthrall) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">21:03 Tue 14-Oct-2025</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    Introduce models used to forecast time series. <br><br>
  </div>
</div>


</header>


<section id="introduction" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">12.1</span> Introduction</h2>
<p>This technical note introduces the basic components of widely used statistical models of time series data. Such models support the understanding of underlying processes. They are also heavily used to forecast future values, which is the emphasis of this note.</p>
<p>The examples and methods presented here are from <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span> and the <code>R</code> package <code>astsa</code> <span class="citation" data-cites="astsa">(<a href="references.html#ref-astsa" role="doc-biblioref"><strong>astsa?</strong></a>)</span>.</p>
</section>
<section id="mathematical-framework" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="mathematical-framework"><span class="header-section-number">12.2</span> Mathematical Framework</h2>
<section id="data-derived-from-a-random-process" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="data-derived-from-a-random-process"><span class="header-section-number">12.2.1</span> Data derived from a random process</h3>
<p>Statistical applications are often based on one or more data frames in which each row represents an observation and each column represents a variable of interest. Consider for example the following predator-prey data (<a href="#tbl-hl-pelts" class="quarto-xref">Table&nbsp;<span>12.1</span></a> and <a href="#fig-pred-prey" class="quarto-xref">Figure&nbsp;<span>12.9</span></a>), based on the record of snowshoe hare (<code>astsa::Hare</code>) and lynx (<code>astsa::Lynx</code>) pelts purchased by the Hudson’s Bay Company of Canada from 1845 to 1935.</p>
<div id="tbl-hl-pelts" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-hl-pelts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;12.1: Hare and lynx pelts (thousands)
</figcaption>
<div aria-describedby="tbl-hl-pelts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 91 × 3
     yr  hare  lynx
  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1  1845  19.6  30.1
2  1846  19.6  45.2
3  1847  19.6  49.2
4  1848  12.0  39.5
5  1849  28.0  21.2
# ℹ 86 more rows</code></pre>
</div>
</div>
</figure>
</div>
<p>The distinction of time series analysis is that observations are indexed by time, and are not assumed to be statistically independent. Instead, time series data are typically modeled as a realization of a random process of the following form.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span id="eq-X-bullet"><span class="math display">\[
\begin{align}
  X_\bullet (t) = (X_1 (t), \ldots, X_d (t)) \in \mathbb{R}^d
\end{align}
\qquad(12.1)\]</span></span></p>
<p>If the number of data columns is two or greater <span class="math inline">\((d \ge 2)\)</span>, the process is said to be multivariate. Otherwise, if <span class="math inline">\(d = 1\)</span> the process is said to be univariate, and the notation is simplified to <span class="math inline">\(X (t)\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>In our example there are two data columns (<code>hare</code>, <code>lynx</code>), so that <span class="math inline">\(d = 2\)</span>, the unit of time is one year, and the sampling frequency is once per year.</p>
<p>The random process is idealized to span all time <span class="math inline">\((t \in \mathbb{Z})\)</span>, but the data of course span some finite period, <span class="math inline">\(\tau_T\)</span>, of length <span class="math inline">\(T\)</span>.</p>
<p><span id="eq-tau-T"><span class="math display">\[
\begin{align}
  \tau_T &amp;=  \{ t_0, \; t_0 + 1, \ldots, t_f \} \\
  t_0 &amp;=  \text{ initial data index} = \min \tau_T \\
  t_f &amp;=  \text{ final data index} = \max \tau_T \\
  T &amp;=  \text{ number of observatons } = t_f - t_0 + 1 \\
  \nu_T &amp;= \tau_T - t_0 =  \{ 0, 1, \ldots, T-1 \}
\end{align}
\qquad(12.2)\]</span></span></p>
<p>The expected value of the random process may be modeled by various functions of time: a constant, a linear trend, a seasonal component (periodic function), etc.</p>
<p><span id="eq-m-bullet"><span class="math display">\[
\begin{align}
  m_\bullet (t) &amp;= E \{ X_\bullet (t) \}
\end{align}
\qquad(12.3)\]</span></span></p>
</section>
<section id="stationarity" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="stationarity"><span class="header-section-number">12.2.2</span> Stationarity</h3>
<p>In an additive model (which is most common), the residual random process, <span class="math inline">\(X_\bullet (t) - m_\bullet (t)\)</span> , is assumed to be <strong>second-order stationary</strong>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> That is, if we shift <span class="math inline">\(X_\bullet (\cdot)\)</span> by any number of time units <span class="math inline">\(s\)</span> to obtain a new process <span class="math inline">\(Y_\bullet (\cdot)\)</span>, we assume that the respective covariance structures of <span class="math inline">\(X_\bullet (\cdot)\)</span> and <span class="math inline">\(Y_\bullet (\cdot)\)</span> are the same.</p>
<p><span id="eq-back-shift-s"><span class="math display">\[
\begin{align}
  Y_\bullet (t) &amp;= \mathcal{B}^s \{ X_\bullet (\cdot) \} (t)  \\
  &amp;= X_\bullet (t - s)
\end{align}
\qquad(12.4)\]</span></span></p>
<p>Here <span class="math inline">\(\mathcal{B}\)</span> denotes the <em>back-shift</em> operator that shifts time by one unit, so that <span class="math inline">\(\mathcal{B}^s\)</span> (that is, <span class="math inline">\(s\)</span> repeated applications of <span class="math inline">\(\mathcal{B}\)</span>) shifts time by <span class="math inline">\(s\)</span> units. Then second-order stationarity can be expressed as follows.</p>
<p><span class="math display">\[
\begin{align}
  Cov \{ X_a (t + u), X_b (t) \}
  &amp;= Cov \{ Y_a (t + u), Y_b (t) \} \\
  &amp;= Cov \{ X_a (t + u - s), X_b (t - s) \} \\
  \\
  &amp; \text{for all } s \in \mathbb{Z} \text{ and } a, b \in \{ 1, \ldots, d \}
\end{align}
\]</span></p>
</section>
<section id="acf-autocorrelation-function" class="level3" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="acf-autocorrelation-function"><span class="header-section-number">12.2.3</span> ACF: autocorrelation function</h3>
<p>Setting <span class="math inline">\(s = t\)</span> in the last expression we have</p>
<p><span id="eq-stationarity-2"><span class="math display">\[
\begin{align}
  Cov \{ X_a (t + u), X_b (t) \}
  &amp;= Cov \{ X_a (u), X_b (0) \}
\end{align}
\qquad(12.5)\]</span></span></p>
<p>Consequently, second-order stationarity enables one to define and estimate the following <em>auto-covariance</em> function.</p>
<p><span id="eq-ACovF"><span class="math display">\[
\begin{align}
  \gamma_{\bullet, \bullet} (u) &amp;= \left \{ \gamma_{a, b} (u) \right \}_{a, b = 1}^d \\
  \\
  \text{where} \\
  \gamma_{a, b} (u) &amp;= Cov \{ X_a (t + u), X_b (t) \} \\
  &amp;= Cov \{ X_a (u), X_b (0) \}
\end{align}
\qquad(12.6)\]</span></span></p>
<p>Although <span class="math inline">\(\gamma_{\bullet, \bullet} (u)\)</span> is a symmetric matrix at <span class="math inline">\(u = 0\)</span>, it is not generally symmetric at <span class="math inline">\(u \ne 0\)</span>. The symmetry relation we do have is that matrix <span class="math inline">\(\gamma_{\bullet, \bullet} (-u)\)</span> is the transpose of matrix <span class="math inline">\(\gamma_{\bullet, \bullet} (u)\)</span>.</p>
<p><span id="eq-ACovF-symm"><span class="math display">\[
\begin{align}
  \gamma_{a, b} (-u) &amp;= Cov \{ X_a (t - u), X_b (t) \} \\
  &amp;= Cov \{ X_a (t), X_b (t + u) \}  \\
  &amp;= Cov \{ X_b (t + u), X_a (t) \}   \\
  &amp;= \gamma_{b, a} (u)
\end{align}
\qquad(12.7)\]</span></span></p>
<p>In particular, <span class="math inline">\(\gamma_{a, a} (\cdot)\)</span> is an even function: <span class="math inline">\(\gamma_{a, a} (-u) = \gamma_{a, a} (u)\)</span>.</p>
<p>Consequently, the time-reversed process <span class="math inline">\(Y_\bullet (t) = X_\bullet (-t)\)</span> has the same auto-covariance structure as the original process, allowing for this matrix transposition.</p>
<p>The <em>autocorrelation (ACF)</em> function <span class="math inline">\(\rho_{\bullet, \bullet}(\cdot)\)</span> is the following scaled version of the auto-covariance function.</p>
<p><span id="eq-ACF"><span class="math display">\[
\begin{align}
\rho_{\bullet, \bullet} (u) &amp;= \left \{ \rho_{a, b} (u) \right \}_{a, b = 1}^d \\
  \\
  \text{where} \\
  \rho_{a, b} (u) &amp;= corr \{ X_a (u), X_b (0) \} \\
  &amp;= \frac{\gamma_{a, b} (u)}{\sqrt{\gamma_{a, a} (0) \; \gamma_{b, b} (0)}}
\end{align}
\qquad(12.8)\]</span></span></p>
</section>
<section id="pacf-partial-autocorrelation-function" class="level3" data-number="12.2.4">
<h3 data-number="12.2.4" class="anchored" data-anchor-id="pacf-partial-autocorrelation-function"><span class="header-section-number">12.2.4</span> PACF: partial autocorrelation function</h3>
<p>The <em>partial autocorrelation function (PACF)</em><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> is a refinement of the ACF and a useful diagnostic tool.</p>
<p>We first recall the related concept of conditional expectation. If <span class="math inline">\(X, Y, Z_\bullet = \{Z_1, \ldots, Z_K\}\)</span> are random variables having a joint probability distribution, then we denote the conditional expectation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z_\bullet\)</span> as follows.</p>
<p><span id="eq-eXY-Z"><span class="math display">\[
\begin{align}
  \tilde{X} &amp;= E(X \; | \; Z_\bullet) \\
  \tilde{Y} &amp;= E(Y \; | \; Z_\bullet)
\end{align}
\qquad(12.9)\]</span></span></p>
<p>Of all possible functions of <span class="math inline">\(Z_\bullet\)</span>, the conditional expectation above minimizes the mean squared approximation error.</p>
<p><span id="eq-mse-XY-Z"><span class="math display">\[
\begin{align}
  MSE \left \{ X, \; f(Z_\bullet) \right \} &amp;= E \left \{ (X - f(Z_\bullet))^2 \right \} \\
  MSE \left \{ Y, \; g(Z_\bullet) \right \} &amp;= E \left \{ (Y - g(Z_\bullet))^2 \right \}
\end{align}
\qquad(12.10)\]</span></span></p>
<p>If <span class="math inline">\(X, Y, Z_\bullet\)</span> have a joint normal distribution, then <span class="math inline">\(\tilde{X}\)</span> is an affine function of <span class="math inline">\(Z_\bullet\)</span>, that is, a constant plus a linear combination of the components of <span class="math inline">\(Z_\bullet\)</span>, and so is <span class="math inline">\(\tilde{Y}\)</span>.</p>
<p>In general, however <span class="math inline">\(X, Y, Z_\bullet\)</span> may be jointly distributed, we denote by <span class="math inline">\(\hat{X}\)</span> and <span class="math inline">\(\hat{Y}\)</span> the affine functions of <span class="math inline">\(Z_\bullet\)</span> that minimize the mean squared approximation error.</p>
<p><span id="eq-hat-X-Z"><span class="math display">\[
\begin{align}
  \hat{\beta}_\bullet &amp;= \arg \min_{\beta_\bullet} \; MSE \left \{ X, \; \beta_0 + \sum_{k = 1}^K \beta_k Z_k \right \} \\
  \hat{X} &amp;= \hat{\beta}_0 + \sum_{k = 1}^K \hat{\beta}_k Z_k
\end{align}
\qquad(12.11)\]</span></span></p>
<p><span class="math inline">\(\hat{Y}\)</span> is similarly defined. In the normal case we have <span class="math inline">\(\tilde{X} = \hat{X}\)</span> and <span class="math inline">\(\tilde{Y} = \hat{Y}\)</span>.</p>
<p>Then the partial correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z_\bullet\)</span> is the correlation between the respective <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> residuals, as follows.</p>
<p><span id="eq-corXY-Z"><span class="math display">\[
\begin{align}
  \rho_{X, Y \bullet Z_\bullet} &amp;= corr \left (X - \hat{X} \; , \; Y - \hat{Y} \right)
\end{align}
\qquad(12.12)\]</span></span></p>
<p>In the time series context we introduce the PACF with reference to a univariate, stationary Gaussian process, <span class="math inline">\(\mathcal{N}(t)\)</span>, having zero mean.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>At lags <span class="math inline">\(|u| \le 1\)</span> the partial autocorrelation is defined as the autocorrelation <span class="math inline">\(\rho (u)\)</span>. For <span class="math inline">\(u &gt; 1\)</span> the PACF is defined to be the correlation between <span class="math inline">\(\mathcal{N}(t + u)\)</span> and <span class="math inline">\(\mathcal{N}(t)\)</span> conditioned on the intervening variables <span class="math inline">\(\mathcal{N}(t + 1), \ldots, \mathcal{N}(t + u - 1)\)</span>. Let <span class="math inline">\(\Delta_u\)</span> denote these intervening integer increments <span class="math inline">\(\{ 1, 2, \ldots, u-1 \}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
   \Delta_u &amp;= \{ 1, 2, \ldots, u-1 \} \\
   -\Delta_u &amp;= \{ -1, -2, \ldots, 1-u \} \\
   \\
   &amp; \text{ for } u \ge 2
\end{align}
\]</span></p>
<p>The calculations are as follows.</p>
<p><span id="eq-N-given-past"><span class="math display">\[
\begin{align}
   \hat{\mathcal{N}}(t + u \; | -\Delta_u)
   &amp;= E \left \{ \mathcal{N}(t + u) \;
     | \; \mathcal{N}(t + u + \nu), \; \nu \in -\Delta_u \right \} \\
   &amp;= \beta_1 \mathcal{N}(t + u - 1) + \cdots
     + \beta_{u-1} \mathcal{N}(t + 1)
\end{align}
\qquad(12.13)\]</span></span></p>
<p><span id="eq-N-given-future"><span class="math display">\[
\begin{align}
   \hat{\mathcal{N}}(t \; | \; \Delta_u)
   &amp;= E \left \{ \mathcal{N}(t + u) \;
     | \; \mathcal{N}(t + \nu), \; \nu \in \Delta_u \right \} \\
   &amp;= \beta_1 \mathcal{N}(t + 1) + \cdots
     + \beta_{u-1} \mathcal{N}(t + u - 1)
\end{align}
\qquad(12.14)\]</span></span></p>
<p>The conditional expectation is linear in the conditioning variables thanks to the assumption that <span class="math inline">\(\mathcal{N}(t)\)</span> is a Gaussian process having zero mean. In the non-Gaussian case the linear combination is defined as the one that minimizes the mean-squared approximation error.</p>
<p>The coefficients in the linear combination are shared by <span class="math inline">\(\hat{\mathcal{N}}(t)\)</span> and <span class="math inline">\(\hat{\mathcal{N}}(t + u)\)</span> but are applied in reverse order. This is because the time-reversed process <span class="math inline">\(\mathcal{N}(-t)\)</span> has the same auto-covariance structure as the original process <span class="math inline">\(\mathcal{N}(t)\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>The partial autocorrelation function is now defined as the correlation of the residual variables:<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p><span id="eq-PACF"><span class="math display">\[
\begin{align}
  \rho (u | u) &amp;= corr \left (\mathcal{N}(t + u) - \hat{\mathcal{N}}(t + u \; | -\Delta_u) \; , \; \mathcal{N}(t) - \hat{\mathcal{N}}(t \; | \; \Delta_u) \right)
\end{align}
\qquad(12.15)\]</span></span></p>
<p>The calculation of the PACF can be based on the recursive Durbin-Levinson algorithm<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>:</p>
<p><span id="eq-DL-algo"><span class="math display">\[
\begin{align}
  \rho (u | u) &amp;= \frac{\rho(u) - \sum_{\nu = 1}^{u - 1} \rho(u-1 | \nu) \;  \rho(u-\nu) }{1 - \sum_{\nu = 1}^{u - 1} \rho(u-1 | \nu) \;  \rho(\nu) } \\
  \\
  \text{where} \\
  \\
  \rho (u | \nu) &amp;= \rho (u-1 | \nu) \; - \; \rho (u | u) \; \rho (u-1 | u-\nu)
\end{align}
\qquad(12.16)\]</span></span></p>
</section>
</section>
<section id="operations-on-time-series" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="operations-on-time-series"><span class="header-section-number">12.3</span> Operations on Time Series</h2>
<section id="transformation-of-each-observation" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="transformation-of-each-observation"><span class="header-section-number">12.3.1</span> Transformation of Each Observation</h3>
<p>The data transformations used on independent observations are applicable to time series data. For example, in the case of Johnson and Johnson quarterly earnings (<a href="#fig-jj" class="quarto-xref">Figure&nbsp;<span>12.10</span></a>), we see that the transformed time series, <span class="math inline">\(Y(t) = \log_e (X(t))\)</span>, can be more closely approximated by a linear trend.</p>
</section>
<section id="smoothing" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="smoothing"><span class="header-section-number">12.3.2</span> Smoothing</h3>
<p>In addition to the data transformations used on independent observations, we often want to smooth out local fluctuations in time series data to show the overall trend more clearly.</p>
<section id="running-average" class="level4" data-number="12.3.2.1">
<h4 data-number="12.3.2.1" class="anchored" data-anchor-id="running-average"><span class="header-section-number">12.3.2.1</span> Running Average</h4>
<p>A running average (also called a moving average) of a random process <span class="math inline">\(X(\cdot)\)</span> is a new process <span class="math inline">\(Y(\cdot)\)</span> formed as a local average of <span class="math inline">\(X\)</span> values. A <em>weighted</em> running average, with weights <span class="math inline">\(\{ w_u \}_u\)</span> takes the following form.</p>
<p><span id="eq-ma-K"><span class="math display">\[
\begin{align}
  Y (t) &amp;= \sum_{u = -K}^K w_u \; X (t - u) \\
  \\
  &amp; \text{where } \\
  \\
  w_u &amp;\ge 0 \quad \text{for all } u \\
  1 &amp;= \sum_{u = -K}^K w_u
\end{align}
\qquad(12.17)\]</span></span></p>
<p>As an example, <a href="#fig-gt-lo-ma5" class="quarto-xref">Figure&nbsp;<span>12.1</span></a> shows a 5-year <em>simple</em> moving average</p>
<p><span id="eq-ma-5"><span class="math display">\[
\begin{align}
  Y_\bullet (t)
  &amp;= \frac{1}{5} \sum_{u = -2}^2 X_\bullet (t - u)
\end{align}
\qquad(12.18)\]</span></span></p>
<p><a href="#fig-gtemp-lo" class="quarto-xref">Figure&nbsp;<span>12.11</span></a> shows the original global temperature data. As mentioned, the operation smooths out local fluctuations to show the overall trend more clearly.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-gt-lo-ma5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gt-lo-ma5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-gt-lo-ma5-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gt-lo-ma5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.1: Global temperatures: 5-year moving average
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="exponential-smoothing" class="level4" data-number="12.3.2.2">
<h4 data-number="12.3.2.2" class="anchored" data-anchor-id="exponential-smoothing"><span class="header-section-number">12.3.2.2</span> Exponential Smoothing</h4>
<p>Exponential smoothing (also called an exponentially weighted moving average, or EWMA) assigns a prescribed weight <span class="math inline">\(\alpha \in (0, 1)\)</span> to the current observation and exponentially decaying weights to prior observations.</p>
<p>For a mathematically defined process <span class="math inline">\(X(\cdot)\)</span> spanning all time, the operation can be represented by the following formula.</p>
<p><span id="eq-exp-smooth-infty"><span class="math display">\[
\begin{align}
  Y(t)
  &amp;= \alpha \; \sum_{u \ge 0} (1 - \alpha)^u X(t - u)
\end{align}
\qquad(12.19)\]</span></span></p>
<p>For finite time series data the operation can be implemented by the following recursive algorithm.</p>
<p><span id="eq-exp-smooth-recur"><span class="math display">\[
\begin{align}
  Y(t) &amp;=
  \begin{cases}
    X(t) &amp; \text{for } t = t_0 \\
    \alpha \; X(t) + (1 - \alpha) \; Y(t - 1) &amp; \text{for } t &gt; t_0
  \end{cases}
\end{align}
\qquad(12.20)\]</span></span></p>
</section>
</section>
<section id="differences-of-successive-observations" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="differences-of-successive-observations"><span class="header-section-number">12.3.3</span> Differences of Successive Observations</h3>
<p>Time series data often exhibit trends over time. The data fluctuate around some level that varies consistently and can be modeled as some function of time, <span class="math inline">\(m(t)\)</span>. Suppose we want to extract the fluctuations about <span class="math inline">\(m(t)\)</span> from the original data. One approach is to estimate <span class="math inline">\(m(t)\)</span> and then form the residual series. This is called “de-trending”. Another approach is to apply an operator that removes the trend directly.</p>
<p>For example, suppose that the process <span class="math inline">\(X(t)\)</span> has a linear trend <span class="math inline">\(m(t) = \beta_0 + \beta_1 t\)</span>.</p>
<p><span id="eq-linear-trend"><span class="math display">\[
\begin{align}
  m(t) &amp;= E\{ X(t) \} = \beta_0 + \beta_1 t
\end{align}
\qquad(12.21)\]</span></span></p>
<p>Then forming successive differences removes the linear trend.</p>
<p><span id="eq-e-diff"><span class="math display">\[
\begin{align}
  E \left \{ X(t) - X(t-1) \right \} &amp;= \beta_1
\end{align}
\qquad(12.22)\]</span></span></p>
<p>Here we have applied the differencing operator <span class="math inline">\(\nabla\)</span>.</p>
<p><span id="eq-diff-op-demo"><span class="math display">\[
\begin{align}
  \{\nabla \; X(\cdot) \} \; (t) &amp;= X(t) - X(t-1) \\
  \\
  \text{where } \\
  \\
  \nabla &amp;= \mathcal{I} - \mathcal{B}
\end{align}
\qquad(12.23)\]</span></span></p>
<p>The DJIA stock index (<a href="#fig-djia" class="quarto-xref">Figure&nbsp;<span>12.12</span></a>) illustrates the differencing operation. Returns are defined as the log-ratio of successive closing values, calculated as the difference in successive values of the logarithm of the closing values.</p>
<p>To remove a polynomial trend of degree <span class="math inline">\(d\)</span>, one can apply <span class="math inline">\(\nabla^d\)</span>, meaning <span class="math inline">\(d\)</span> repeated applications of <span class="math inline">\(\nabla\)</span>.</p>
</section>
<section id="seasonal-differencing" class="level3" data-number="12.3.4">
<h3 data-number="12.3.4" class="anchored" data-anchor-id="seasonal-differencing"><span class="header-section-number">12.3.4</span> Seasonal Differencing</h3>
<p>To remove a seasonal component one can apply a variant of <span class="math inline">\(\nabla\)</span>, namely seasonal differencing, <span class="math inline">\(\nabla_\sigma\)</span>: <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p><span id="eq-seasonal-diff"><span class="math display">\[
\begin{align}
  \nabla_\sigma &amp;= \mathcal{I} - \mathcal{B}^\sigma
\end{align}
\qquad(12.24)\]</span></span></p>
<p>Here <span class="math inline">\(\sigma\)</span> denotes the number of time indices that span one season. For analyses of economic or business performance, seasonal differences are often used to show the difference between current values versus those from one year prior. In that context one would set <span class="math inline">\(\sigma = 4\)</span> for quarterly data, <span class="math inline">\(\sigma = 12\)</span> for monthly data, etc.</p>
</section>
<section id="integration-restoring-differenced-data" class="level3" data-number="12.3.5">
<h3 data-number="12.3.5" class="anchored" data-anchor-id="integration-restoring-differenced-data"><span class="header-section-number">12.3.5</span> Integration: Restoring Differenced Data</h3>
<p>Time series analysis may entail differencing the original process <span class="math inline">\(Y(\cdot) = \nabla \; X(\cdot)\)</span> and then building a model of the differenced process <span class="math inline">\(\hat{Y}(\cdot) \sim Y(\cdot)\)</span>. We can apply the model to the original process using the following simple recursive algorithm.</p>
<p><span id="eq-integrate"><span class="math display">\[
\begin{align}
  \hat{X} (t) &amp;=
  \begin{cases}
    X(t) &amp; \text{ for } t = t_0 \\
    \hat{Y}(t) + \hat{X}(t-1) &amp; \text{ for } t &gt; t_0
  \end{cases}
\end{align}
\qquad(12.25)\]</span></span></p>
<p><a href="#fig-rw-drift" class="quarto-xref">Figure&nbsp;<span>12.4</span></a> illustrates the use of such integration to forecast values of a random walk with drift.</p>
</section>
<section id="filtering" class="level3" data-number="12.3.6">
<h3 data-number="12.3.6" class="anchored" data-anchor-id="filtering"><span class="header-section-number">12.3.6</span> Filtering</h3>
<p>Running averages and successive differences are each an example of a linear, time-invariant <em>filter</em>, having the following general form.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p><span id="eq-filter"><span class="math display">\[
\begin{align}
  Y_\bullet (t)
  &amp;= a_{\bullet, \bullet} (\cdot) \; * \; X_\bullet (\cdot)  \\
  &amp;= \sum_u a_{\bullet, \bullet} (u) \; \times \; X_\bullet (t - u)
\end{align}
\qquad(12.26)\]</span></span></p>
<p>where <span class="math inline">\(a_{\bullet, \bullet} (\cdot)\)</span> is a sequence of <span class="math inline">\(d \times d\)</span> matrices and <span class="math inline">\(*\)</span> denotes discrete convolution.</p>
<p>For example, here’s the 5-year moving average above expressed in this format.</p>
<p><span id="eq-ma-5-filter"><span class="math display">\[
\begin{align}
  a_{\bullet, \bullet} (u) &amp;=
    \begin{cases}
      \frac{1}{5} I &amp; \text{ for } |u| \le 2 \\
      0 &amp; \text{ for } |u| &gt; 2
    \end{cases}
\end{align}
\qquad(12.27)\]</span></span></p>
<p>And here’s the difference operator <span class="math inline">\(\nabla\)</span> in the filtering format.</p>
<p><span id="eq-diff-filter"><span class="math display">\[
\begin{align}
  a_{\bullet, \bullet} (u) &amp;=
    \begin{cases}
      I  &amp; \text{ for } u = 0 \\
      -I &amp; \text{ for } u = 1 \\
      0  &amp; \text{ otherwise }
    \end{cases}
\end{align}
\qquad(12.28)\]</span></span></p>
<p>A moving average operation smooths the time series on which it operates. That is, it allows low-frequency components to pass, while diminishing high-frequency components. Therefore the moving average operation is categorized as a <em>low-pass</em> filter. The differencing operator can be categorized as a <em>high-pass</em> filter.</p>
<p>In signal-processing applications a linear, time-invariant filter <span class="math inline">\(a_{\bullet, \bullet} (\cdot)\)</span> may be designed to extract certain types of signals that are contaminated by noise.<span class="citation" data-cites="hamming1989">(<a href="references.html#ref-hamming1989" role="doc-biblioref"><strong>hamming1989?</strong></a>)</span></p>
</section>
</section>
<section id="model-components" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="model-components"><span class="header-section-number">12.4</span> Model Components</h2>
<p>The process of building a statistical model (time series or not) is typically cyclic:</p>
<ul>
<li>examine data (or model residuals)</li>
<li>propose model (or refinement of current model)</li>
<li>fit proposed model</li>
<li>examine model residuals</li>
</ul>
<p>One might continue this cycle until the model residuals seem to be reasonably free of trend and autocorrelation.</p>
<section id="covariates-and-trend" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="covariates-and-trend"><span class="header-section-number">12.4.1</span> Covariates and Trend</h3>
<p>Time series and other statistical data sets are often collected in order to study the possible effects of one set of variables on one or more of the remaining variables.</p>
<p>Consider, for example, the cardiovascular mortality data shown in <a href="#fig-cmort" class="quarto-xref">Figure&nbsp;<span>12.13</span></a>, where mortality is modeled as a response to temperature and the level of airborne particulate matter. In this example we can formulate the expected value of mortality in the following terms.</p>
<p><span class="math display">\[
\begin{align}
  m(t, X_{2:3}(t))
    &amp;= E \left \{ X_1(t) \; | \; X_2(t), \; X_3(t) \; \right \}
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{align}
  X_1(t) = M(t) &amp;= \text{mortality count in week } t \\
    X_2(t) = T(t) &amp;= \text{temperature } (^\circ F) \text{ in week } t \\
    X_3(t) = P(t) &amp;= \text{level of airborne particulate matter in week } t
\end{align}
\]</span></p>
<p>Each of the three components of <span class="math inline">\(X_\bullet (t)\)</span> exhibits an annual periodicity, as one might expect. The textbook authors initially propose a model for <span class="math inline">\(X_1(t)\)</span> of the following form<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</p>
<p><span class="math display">\[
\begin{align}
  X_1(t) &amp;= m(t, X_{2:3}(t)) + W(t)\\
   \\
    \text{ where } \\
    \\
    m(t, X_{2:3}(t)) &amp;= \beta_0 + f(t) + \beta_2 X_2(t) + \beta_3 X_3(t) \\
    \\
    W(\cdot) &amp;\sim wn(0, \sigma_W^2)
\end{align}
\]</span></p>
<p>Here <span class="math inline">\(wn(0, \sigma_W^2)\)</span> denotes an uncorrelated sequence having zero mean and variance <span class="math inline">\(\sigma_W^2\)</span>, referred to as white noise.</p>
<p>According to this model<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> the pattern over time of <span class="math inline">\(X_1(t)\)</span> is captured by the mean over time, <span class="math inline">\(m(t, X_{2:3}(t))\)</span>. Once the mean level is accounted for, the residual series <span class="math inline">\(W(t) = X_1 (t) - m(t, X_{2:3}(t))\)</span> is modeled as being free of autocorrelation. The model is consistent with the assumption that, apart from a trend component, weekly numbers of cardiovascular deaths are independent, conditioned on temperature and particulate levels.</p>
<p>Later in the text (page 155) the authors refine this initial model. After examining the autocorrelation function (ACF) of the residual series (no longer labeled <span class="math inline">\(W(t)\)</span>), they develop an autocorrelated (<span class="math inline">\(AR(2)\)</span>) model.</p>
<p>This is an example of the cyclic model-building process previously outlined. A model is developed, examined, and revised.</p>
</section>
<section id="auto-regression-ar" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="auto-regression-ar"><span class="header-section-number">12.4.2</span> Auto-regression (AR)</h3>
<p>Auto-regressive models are based on the idea that the current value of the series, <span class="math inline">\(X(t)\)</span>, can be approximated, or forecast, as a function of <span class="math inline">\(p\)</span> past values, <span class="math inline">\(X(t−1), X(t−2), \ldots, X(t−p)\)</span>, which leads to the following formulation of an auto-regressive model of order <span class="math inline">\(p\)</span>, abbreviated as <span class="math inline">\(AR(p)\)</span>.</p>
<p><span id="eq-ARp-alpha"><span class="math display">\[
\begin{align}
  X(t) &amp;= \alpha \; + \; \sum_{\nu = 1}^p \phi_\nu X(t-\nu) \; + \; W(t) \\
  \text{where } \\
  \alpha &amp;= \mu_X \; - \; \mu_X \; \sum_{\nu = 1}^p \phi_\nu \\
    \\
    W(t) &amp;\sim wn(0, \sigma_W^2)
\end{align}
\qquad(12.29)\]</span></span></p>
<p>That is, <span class="math inline">\(W(t)\)</span> again denotes a residual “white noise” stationary process: free of autocorrelation and having zero mean.</p>
<section id="polynomial-representation" class="level4" data-number="12.4.2.1">
<h4 data-number="12.4.2.1" class="anchored" data-anchor-id="polynomial-representation"><span class="header-section-number">12.4.2.1</span> Polynomial representation</h4>
<p>The <span class="math inline">\(AR(p)\)</span> model can also be expressed using the back-shift operator <span class="math inline">\(\mathcal{B}\)</span> as follows.</p>
<p><span id="eq-ARp-bs-poly"><span class="math display">\[
\begin{align}
  \left \{\phi (\mathcal{B}) \; (X(\cdot) - \mu_X) \right \} \; (t) &amp;= W(t) \\
  \\
  \text{where } \\
  \phi (z) &amp;= 1 - \sum_{\nu = 1}^p \phi_\nu \; z^\nu
  \\
  \text{so that } \\
  \phi (\mathcal{B}) &amp;= \mathcal{I} \; - \; \sum_{\nu = 1}^p \phi_\nu \mathcal{B}^\nu
\end{align}
\qquad(12.30)\]</span></span></p>
<p>The polynomial <span class="math inline">\(\phi(\cdot)\)</span> applied to the back-shift operator <span class="math inline">\(\mathcal{B}\)</span> is an <em>auto-regressive operator of order</em> <span class="math inline">\(p\)</span>.</p>
<p>Now, the <span class="math inline">\(AR(p)\)</span> model is defined to be that of a <em>stationary</em> process <span class="math inline">\(X(t)\)</span>, which constrains the coefficients of polynomial <span class="math inline">\(\phi(\cdot)\)</span>. Additional requirements that the model be <em>causal</em><a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> and <em>invertible</em><a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> restrict attention to polynomials <span class="math inline">\(\phi(\cdot)\)</span> whose roots are all greater than unity in magnitude. This ensures that the multiplicative inverse function <span class="math inline">\(\phi (z)^{-1}\)</span> is well defined and has a power series expansion for <span class="math inline">\(|z| \le 1\)</span>.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
</section>
<section id="ar1" class="level4" data-number="12.4.2.2">
<h4 data-number="12.4.2.2" class="anchored" data-anchor-id="ar1"><span class="header-section-number">12.4.2.2</span> AR(1)</h4>
<p>Consider the <span class="math inline">\(AR(1)\)</span> model. We have</p>
<p><span class="math display">\[
\begin{align}
  X(t) &amp;= \mu_X \; + \; \phi_1 (X(t-1) - \mu_X) \; + \; W(t)
\end{align}
\]</span></p>
<p>so that</p>
<p><span id="eq-AR1-resid"><span class="math display">\[
\begin{align}
  W(t) &amp;= (X(t) - \mu_X)  \; - \; \phi_1 (X(t-1) - \mu_X) \\
  &amp;= \left \{ (\mathcal{I} - \phi_1 \mathcal{B}) \; (X(\cdot) - \mu_X) \right \} \; (t) \\
  &amp;= \left \{ \phi (\mathcal{B}) \; (X(\cdot) - \mu_X) \right \} \; (t)
\end{align}
\qquad(12.31)\]</span></span></p>
<p>The auto-regressive polynomial <span class="math inline">\(\phi (\cdot)\)</span> has degree 1.</p>
<p><span class="math display">\[
\begin{align}
  \phi (z) &amp;= 1 \; - \; \phi_1 z &amp; \text{ with } | \phi_1 | &amp;&lt; 1
\end{align}
\]</span></p>
<p>The restriction on the real-valued coefficient <span class="math inline">\(\phi_1\)</span> ensures that <span class="math inline">\(\phi (z) \ne 0\)</span> for <span class="math inline">\(|z| \le 1\)</span>, as previously mentioned.</p>
<p>Now one can show by induction that for <span class="math inline">\(u &gt; 0\)</span> we have</p>
<p><span class="math display">\[
\begin{align}
  X(t) - \mu_X  \\
  &amp;= \phi_1^u \; (X(t-u) - \mu_X) + \sum_{\nu = 0}^{u-1} \phi_1^\nu \; W(t-\nu)
\end{align}
\]</span></p>
<p>In the limit, as <span class="math inline">\(u \rightarrow \infty\)</span>, we see that <span class="math inline">\(\phi_1^u \rightarrow 0\)</span> so that</p>
<p><span class="math display">\[
\begin{align}
  X(t) - \mu_X \\
  &amp;= \sum_{\nu = 0}^\infty \phi_1^\nu \; W(t-\nu) \\
  &amp;= \sum_{\nu = 0}^\infty \phi_1^\nu \; \mathcal{B}^\nu \; W(t)
\end{align}
\]</span></p>
<p>This result can be obtained more directly by inverting the auto-regressive operator <span class="math inline">\(\phi (\mathcal{B})\)</span>.</p>
<p><span id="eq-AR1-bs-inv"><span class="math display">\[
\begin{align}
  W(t) &amp;= \left \{ \phi (\mathcal{B}) \; (X(\cdot) - \mu_X) \right \} \; (t) \\
  X(t) - \mu_X &amp;= \left \{ \phi (\mathcal{B})^{-1} \; W(\cdot) \right \} (t)
\end{align}
\qquad(12.32)\]</span></span></p>
<p>since</p>
<p><span class="math display">\[
\begin{align}
  \phi (z)^{-1} &amp;= \frac{1}{1 - \phi_1 \; z} \\
  &amp;= \sum_{\nu = 0}^\infty \phi_1^\nu \; z^\nu &amp; \text{ for } |z| \le 1
\end{align}
\]</span></p>
<p>Based on these expressions one can show<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> that the auto-covariance and autocorrelation functions of <span class="math inline">\(X(\cdot)\)</span> take the following forms.</p>
<p><span id="eq-AR1-rho"><span class="math display">\[
\begin{align}
  \gamma_X (u) &amp;= \frac{\phi_1^{|u|}}{1 - \phi_1^2} \; \sigma_W^2\\
  \\
  \rho_X (u) &amp;= \phi_1^{|u|}
\end{align}
\qquad(12.33)\]</span></span></p>
<p>Using the Durbin-Levinson algorithm, one can go on to show<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> that the partial autocorrelation function (PACF) of <span class="math inline">\(X(\cdot)\)</span> has the following simple form.</p>
<p><span id="eq-AR1-PACF"><span class="math display">\[
\begin{align}
  \rho_X (u | u) &amp;=
  \begin{cases}
    1 &amp; \text{ for } u = 0 \\
    \phi_1 &amp; \text{ for } u = \pm 1 \\
    0 &amp; \text{ for } |u| &gt; 1
  \end{cases}
\end{align}
\qquad(12.34)\]</span></span></p>
</section>
<section id="arp" class="level4" data-number="12.4.2.3">
<h4 data-number="12.4.2.3" class="anchored" data-anchor-id="arp"><span class="header-section-number">12.4.2.3</span> AR(p)</h4>
<p>As previously noted, an <span class="math inline">\(AR(p)\)</span> model can be defined via a polynomial <span class="math inline">\(\phi(\cdot)\)</span> that is applied to the back-shift operator <span class="math inline">\(\mathcal{B}\)</span>. The coefficients of <span class="math inline">\(\phi(\cdot)\)</span> are real, but it is useful to apply the polynomial to the complex variable <span class="math inline">\(z\)</span>. From <a href="#eq-ARp-bs-poly" class="quarto-xref">Equation&nbsp;<span>12.30</span></a> we have</p>
<p><span id="eq-ARp-poly"><span class="math display">\[
\begin{align}
  \phi(z) &amp;= 1 \; - \; \sum_{\nu = 1}^p \; \phi_\nu \; z^\nu
\end{align}
\qquad(12.35)\]</span></span></p>
<p>Also as previously mentioned, we require the roots of <span class="math inline">\(\phi(\cdot)\)</span> to lie outside the closed unit disc, that is, to be greater than unity in magnitude. This requirement ensures that the model is stationary, causal, and invertible.</p>
<p>Consequently we have the following causal form of the model.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></p>
<p><span id="eq-ARp-causal-form"><span class="math display">\[
\begin{align}
  X(t) - \mu_X &amp;= \left \{ \phi(\mathcal{B})^{-1} \; W(\cdot) \right \} (t) \\
  &amp;= \sum_{\nu = 0}^\infty \; \chi_\nu \; \mathcal{B}^\nu \; W(t) \\
  &amp;= \sum_{\nu = 0}^\infty \; \chi_\nu \; W(t - \nu) \\
  \text{where } \\
  &amp; \chi_0 = 1 \\
  \\
  \text{and } \\
  &amp; \sum_{\nu = 0}^\infty \; |\chi_\nu| \; &lt; \; \infty
\end{align}
\qquad(12.36)\]</span></span></p>
<p><a href="#eq-ARp-bs-poly" class="quarto-xref">Equation&nbsp;<span>12.30</span></a> also shows that <span class="math inline">\(W(t)\)</span> depends on current and past values of <span class="math inline">\(X(t)\)</span>, with past values terminating at <span class="math inline">\(X(t-p)\)</span>.</p>
<p><span class="math display">\[
\begin{align}
  W(t) &amp;= \left \{\phi (\mathcal{B}) \; (X(\cdot) - \mu_X) \right \} \; (t) \\
  &amp;= X(t) - \mu_X \; - \; \sum_{\nu = 1}^p \phi_\nu \; (X(t-\nu) - \mu_X)
\end{align}
\]</span></p>
<p>From these equations one can show that for lag <span class="math inline">\(u &gt; p\)</span> we have</p>
<p><span class="math display">\[
\begin{align}
  \hat{X}(t+u \; | -\Delta_u) &amp;= \mu_X + \sum_{\nu = 1}^p \; \phi_\nu \; (X(t + u - \nu) - \mu_X) \\
  X(t+u) \; - \; \hat{X}(t+u \; | -\Delta_u) &amp;= W(t+u)
\end{align}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align}
  \hat{X}(t \; | \Delta_u) &amp;= \mu_X + \sum_{\nu = 1}^p \; \phi_\nu \; (X(t + \nu) - \mu_X) \\
  X(t) \; - \; \hat{X}(t \; | \Delta_u) &amp;=  \sum_{\nu = 1}^p \; \phi_\nu \; (X(t - \nu) - X(t + \nu)) \; + W(t)
\end{align}
\]</span></p>
<p>Consequently, for <span class="math inline">\(u &gt; p\)</span> we have</p>
<p><span class="math display">\[
\begin{align}
  &amp; Cov(X(t+u) \; - \; \hat{X}(t+u \; | -\Delta_u), \; X(t) \; - \; \hat{X}(t \; | \Delta_u)) \\
  &amp;= Cov(W(t+u), \; X(t) \; - \; \hat{X}(t \; | \Delta_u)) \\
  &amp;= Cov(W(t+u), \; X(t)) \; - \; Cov(W(t+u), \; \hat{X}(t \; | \Delta_u)) \\
  &amp;= 0
\end{align}
\]</span></p>
<p>Therefore the PACF is zero at lags <span class="math inline">\(u &gt; p\)</span>.</p>
<p><span class="math display">\[
\begin{align}
  \rho_X (u | u) &amp;= 0 &amp; \text{ for } u &gt; p
\end{align}
\]</span></p>
</section>
</section>
<section id="moving-average-ma" class="level3" data-number="12.4.3">
<h3 data-number="12.4.3" class="anchored" data-anchor-id="moving-average-ma"><span class="header-section-number">12.4.3</span> Moving Average (MA)</h3>
<p>A moving average model of order <span class="math inline">\(q\)</span> has the following form.</p>
<p><span class="math display">\[
\begin{align}
  X(t) &amp;= \mu_X \; + \; W(t) \; + \; \sum_{\nu = 1}^q \; \theta_\nu \; W(t - \nu) \\
  \text{where } \\
  W(\cdot) &amp;\sim wn(0, \sigma_W^2)
\end{align}
\]</span></p>
<p>Like AR models, an MA model can be expressed by applying a polynomial <span class="math inline">\(\theta(\cdot)\)</span> to the back-shift operator <span class="math inline">\(\mathcal{B}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
  X(t) - \mu_x  &amp;= W(t) \; + \; \sum_{\nu = 1}^q \; \theta_\nu \; W(t - \nu) \\
  &amp;= \left \{\mathcal{I} + \sum_{\nu = 1}^q \; \theta_\nu \; \mathcal{B}^\nu \right \} \; W(t) \\
  &amp;= \theta(\mathcal{B}) \; W(t) \\
  \\
  \text{where} \\
  \theta (z) &amp;= 1 + \sum_{\nu = 1}^q \; \theta_\nu \; z^\nu
\end{align}
\]</span></p>
<p>That is, <span class="math inline">\(X(\cdot)\)</span> is filtered white noise, and is therefore stationary for any finite set of filter coefficients. But some constraint on polynomial <span class="math inline">\(\theta(\cdot)\)</span> is required to eliminate ambiguity.</p>
<section id="invertibility" class="level4" data-number="12.4.3.1">
<h4 data-number="12.4.3.1" class="anchored" data-anchor-id="invertibility"><span class="header-section-number">12.4.3.1</span> Invertibility</h4>
<p>Consider the following two MA(1) models.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<p><span class="math display">\[
\begin{align}
  X(t) &amp;= W(t) + \frac{1}{5} W(t-1) &amp; \text{with } W(\cdot) \sim N(0, 25) \\
  \\
  Y(t) &amp;= V(t) + 5 \; V(t-1) &amp; \text{with } V(\cdot) \sim N(0, 1)
\end{align}
\]</span></p>
<p>The probability distributions of the two processes <span class="math inline">\(X(\cdot)\)</span> and <span class="math inline">\(Y(\cdot)\)</span> are identical<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>, so anyone attempting to identify the respective models could not distinguish them based on their realizations as time series data.</p>
<p>A solution to this conundrum is to follow the example set by AR models, namely by requiring the MA model to be invertible. That is, we require the roots of polynomial <span class="math inline">\(\theta(\cdot)\)</span> to have magnitude greater than unity. In the example above we therefore choose the model for <span class="math inline">\(X(\cdot)\)</span> rather than the model for <span class="math inline">\(Y(\cdot)\)</span>.</p>
</section>
<section id="ma1" class="level4" data-number="12.4.3.2">
<h4 data-number="12.4.3.2" class="anchored" data-anchor-id="ma1"><span class="header-section-number">12.4.3.2</span> MA(1)</h4>
<p>Consider the <span class="math inline">\(MA(1)\)</span> model</p>
<p><span id="eq-MA1"><span class="math display">\[
\begin{align}
  X(t) &amp;= W(t) + \theta_1 W(t-1) \\
  \\
  &amp;\text{ with } |\theta_1| &lt; 1 \\
  &amp;\text{ and } W(\cdot) \sim wn(0, \sigma_W^2)
\end{align}
\qquad(12.37)\]</span></span></p>
<p>Here is the auto-covariance function for this process.</p>
<p><span id="eq-MA1-ACovF"><span class="math display">\[
\begin{align}
  \gamma_X (0) &amp;= \sigma_X^2 \\
  &amp;= \sigma_W^2 \; (1 + \theta_1^2) \\
  \\
  \gamma_X (1) &amp;= Cov(W(t+1) + \theta_1 W(t), \; W(t) + \theta_1 W(t-1)) \\
  &amp;= \sigma_W^2 \; \theta_1 \\
  \\
  \gamma_X (u) &amp;= 0 \quad \text{for } |u| \ge 2
\end{align}
\qquad(12.38)\]</span></span></p>
<p>And here is the corresponding autocorrelation function (ACF)<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>.</p>
<p><span id="eq-MA1-ACF"><span class="math display">\[
\begin{align}
  \rho_X (u) &amp;=
  \begin{cases}
    1 &amp; \text{ for } u = 0 \\
    \frac{\theta_1}{1 + \theta_1^2} &amp; \text{ for } u = \pm 1 \\
    0 &amp; \text{ for } |u| \ge 2 \\  
  \end{cases}
\end{align}
\qquad(12.39)\]</span></span></p>
<p>The PACF turns out to have the following form<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>.</p>
<p><span id="eq-MA1-PACF"><span class="math display">\[
\begin{align}
  \rho_X (u | u) &amp;=  \frac{(- \theta_1)^u(1 - \theta_1^2)}{1 - \theta_1^{2 (u+1)}} &amp; \text{ for } u \ge 2
\end{align}
\qquad(12.40)\]</span></span></p>
</section>
<section id="maq" class="level4" data-number="12.4.3.3">
<h4 data-number="12.4.3.3" class="anchored" data-anchor-id="maq"><span class="header-section-number">12.4.3.3</span> MA(q)</h4>
<p>An <span class="math inline">\(MA(q)\)</span> model is generated by a polynomial <span class="math inline">\(\theta(\cdot)\)</span> of degree <span class="math inline">\(q\)</span> of the following form.</p>
<p><span id="eq-MAq-poly"><span class="math display">\[
\begin{align}
  \theta(z) &amp;= 1 + \sum_{\nu = 1}^q \theta_\nu \; z^\nu \\
  &amp;= \sum_{\nu = 0}^q \theta_\nu \; z^\nu &amp; \text{with } \theta_0 = 1
\end{align}
\qquad(12.41)\]</span></span></p>
<p>Applying <span class="math inline">\(\theta(\cdot)\)</span> to the back-shift operator <span class="math inline">\(\mathcal{B}\)</span>, we have the following <span class="math inline">\(MA(q)\)</span> model.</p>
<p><span id="eq-MAq"><span class="math display">\[
\begin{align}
  X(t) &amp;= W(t) + \sum_{\nu = 1}^q \theta_\nu \; W(t - \nu) \\
  &amp;= \left \{ \theta(\mathcal{B}) \; W(\cdot) \right \} (t)
  \\
  \\
  &amp;\text{ with } W(\cdot) \sim wn(0, \sigma_W^2)
\end{align}
\qquad(12.42)\]</span></span></p>
<p>With <span class="math inline">\(MA(1)\)</span> serving as an example, it is easy to see that for an <span class="math inline">\(MA(q)\)</span> model of <span class="math inline">\(X(\cdot)\)</span>, the auto-covariance function <span class="math inline">\(\gamma_X (u)\)</span> and the autocorrelation function <span class="math inline">\(\rho_X (u)\)</span> equal zero at lags <span class="math inline">\(|u| &gt; q\)</span>.</p>
<p><span id="eq-MAq-ACF"><span class="math display">\[
\begin{align}
  \text{for } X &amp;\sim MA(q) \\
  \text{and } |u| &amp;&gt; q \\
  \\
  \rho_X (u) &amp;= 0
\end{align}
\qquad(12.43)\]</span></span></p>
<p>We require the roots of polynomial <span class="math inline">\(\theta(\cdot)\)</span> to have magnitudes greater than unity to ensure that its multiplicative inverse function, <span class="math inline">\(\theta(z)^{-1}\)</span>, has a power series expansion valid for <span class="math inline">\(|z| \le 1\)</span>.</p>
<p><span id="eq-MAq-poly-inv"><span class="math display">\[
\begin{align}
  \theta (z)^{-1} &amp;= \frac{1}{\theta (z)} \\
  &amp;= \sum_{\nu = 0}^\infty \psi_\nu \; z^\nu \\
  \\
  &amp; \text{for } |z| \le 1 \\
  \\
  &amp; \text{where } \psi_0 = 1
\end{align}
\qquad(12.44)\]</span></span></p>
<p>Then <span class="math inline">\(\theta(\mathcal{B})^{-1}\)</span> is well-defined, and we have</p>
<p><span id="eq-MAq-inv"><span class="math display">\[
\begin{align}
  W(t) &amp;= \left \{ \theta(\mathcal{B})^{-1} \; X(\cdot) \right \} (t)  \\
  &amp;= X(t) + \sum_{\nu = 1}^\infty \psi_\nu \; X(t - \nu)
\end{align}
\qquad(12.45)\]</span></span></p>
<p>Equivalently, we have</p>
<p><span id="eq-AR-infty"><span class="math display">\[
\begin{align}
  X(t) &amp;= W(t) - \sum_{\nu = 1}^\infty \psi_\nu \; X(t - \nu)
\end{align}
\qquad(12.46)\]</span></span></p>
<p>which can be viewed as an infinite-order auto-regressive model. Now for an <span class="math inline">\(AR(p)\)</span>, with <span class="math inline">\(p\)</span> finite, the PACF can be shown to be non-zero at lag <span class="math inline">\(u = p\)</span> (and zero thereafter). In the case of the <span class="math inline">\(MA(q)\)</span> model, equivalent to an infinite-order <span class="math inline">\(AR\)</span> model, the PACF tends to zero but does not vanish as <span class="math inline">\(|u| \rightarrow \infty\)</span>.</p>
</section>
</section>
</section>
<section id="armap-q" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="armap-q"><span class="header-section-number">12.5</span> ARMA(p, q)</h2>
<p>The <span class="math inline">\(AR(p)\)</span> and <span class="math inline">\(MA(q)\)</span> models can be combined into the following <span class="math inline">\(ARMA(p,q)\)</span> model.</p>
<p><span id="eq-ARMA-p-q-proc"><span class="math display">\[
\begin{align}
  \phi(\mathcal{B}) \; (X(\cdot) - \mu_X) &amp;= \theta(\mathcal{B}) \; W(\cdot) &amp; \text{with } W(\cdot) \sim wn(0, \sigma_W^2)
\end{align}
\qquad(12.47)\]</span></span></p>
<p>so that</p>
<p><span id="eq-ARMA-p-q"><span class="math display">\[
\begin{align}
  X(t) &amp;= \mu_X + \sum_{j = 1}^p \phi_j \; (X(t - j) - \mu_X) \; + \; W(t) \; + \; \sum_{k = 1}^q \theta_k \; W(t - k)
\end{align}
\qquad(12.48)\]</span></span></p>
<p>or more succinctly</p>
<p><span id="eq-ARMA-p-q-alpha"><span class="math display">\[
\begin{align}
  X(t) &amp;= \alpha + \sum_{j = 1}^p \phi_j \; X(t - j) \; + \; W(t) \; + \; \sum_{k = 1}^q \theta_k \; W(t - k) \\
  \text{where } \\
  \alpha &amp;= \mu_x \; \left ( 1  - \sum_{j = 1}^p \phi_j \right )
\end{align}
\qquad(12.49)\]</span></span></p>
<section id="co-prime-polynomials" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="co-prime-polynomials"><span class="header-section-number">12.5.1</span> Co-prime Polynomials</h3>
<p>We must impose a restriction on the polynomials <span class="math inline">\(\phi(\cdot)\)</span> and <span class="math inline">\(\theta(\cdot)\)</span> in addition to requiring their respective roots to all have magnitude greater than unity. The additional requirement is that they have no roots in common.</p>
<p>Consider for example the following putative <span class="math inline">\(ARMA(1,1)\)</span> model.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></p>
<p><span id="eq-ARMA-1-1-not"><span class="math display">\[
\begin{align}
  X(t) &amp;= \beta_1 \; X(t - 1) \; + \; W(t) \; - \; \beta_1 \; W(t - 1) \\   \\
  \text{so that } \\
  \\
  p(\mathcal{B}) \; X(\cdot) &amp;= p(\mathcal{B}) \; W(\cdot) \\   \\
  \text{where } \\
  \\
  p(z) &amp;= 1 - \beta_1 \; z \quad \text{with } |\beta_1| &lt; 1
\end{align}
\qquad(12.50)\]</span></span></p>
<p>The operator <span class="math inline">\(p(\mathcal{B})\)</span> can be cancelled from both sides of the equation, leaving only a white noise process: <span class="math inline">\(X(\cdot) = W(\cdot)\)</span>.</p>
<p>More generally we require the elimination of any polynomial factors common to both <span class="math inline">\(\phi(\cdot)\)</span> and <span class="math inline">\(\theta(\cdot)\)</span>. Equivalently, we require the roots of <span class="math inline">\(\phi(\cdot)\)</span> to be distinct from those of <span class="math inline">\(\theta(\cdot)\)</span>.</p>
</section>
<section id="causal-form" class="level3" data-number="12.5.2">
<h3 data-number="12.5.2" class="anchored" data-anchor-id="causal-form"><span class="header-section-number">12.5.2</span> Causal Form</h3>
<p>The <span class="math inline">\(ARMA(p,q)\)</span> model can be expressed in causal form as follows.<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> <a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></p>
<p><span id="eq-ARMA-p-q-causal"><span class="math display">\[
\begin{align}
  X(t) - \mu_X &amp;= \phi(\mathcal{B})^{-1} \;  \theta(\mathcal{B}) \; W(t) \\
  &amp;= \sum_{\nu = 0}^\infty \; \psi_\nu \; \mathcal{B}^\nu \; W(t) \\
  &amp;= \sum_{\nu = 0}^\infty \; \psi_\nu \; W(t - \nu) \\
  \text{where } \\
  &amp; \psi_0 = 1 \\
  \\
  \text{and } \\
  &amp; \sum_{\nu = 0}^\infty \; |\psi_\nu| \; &lt; \; \infty
\end{align}
\qquad(12.51)\]</span></span></p>
</section>
<section id="inverted-form" class="level3" data-number="12.5.3">
<h3 data-number="12.5.3" class="anchored" data-anchor-id="inverted-form"><span class="header-section-number">12.5.3</span> Inverted Form</h3>
<p>It is also useful to invert the MA operator in an ARMA model to obtain the white noise process <span class="math inline">\(W(t)\)</span> as a linear function of current and past values of <span class="math inline">\(X(t)\)</span>.<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> <a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a></p>
<p><span id="eq-ARMA-p-q-inv"><span class="math display">\[
\begin{align}
  W(t) &amp;= \theta(\mathcal{B})^{-1} \phi(\mathcal{B}) \; (X(t) - \mu_X) \\
  &amp;= \sum_{\nu = 0}^\infty \; \pi_\nu \; \mathcal{B}^\nu \; (X(t) - \mu_X) \\
  &amp;= \sum_{\nu = 0}^\infty \; \pi_\nu \; (X(t - \nu) - \mu_X) \\
  \text{where } \\
  &amp; \pi_0 = 1 \\
  \\
  \text{and } \\
  &amp; \sum_{\nu = 0}^\infty \; |\pi_\nu| \; &lt; \; \infty
\end{align}
\qquad(12.52)\]</span></span></p>
</section>
</section>
<section id="linear-predictors" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="linear-predictors"><span class="header-section-number">12.6</span> Linear Predictors</h2>
<section id="problem-statement" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="problem-statement"><span class="header-section-number">12.6.1</span> Problem Statement</h3>
<p>Let <span class="math inline">\(X(\cdot)\)</span> be a stationary process, whose <span class="math inline">\(T\)</span> time indices, <span class="math inline">\(\tau_T\)</span>, begin with <span class="math inline">\(t_0\)</span> and end with <span class="math inline">\(t_f\)</span> per <a href="ts-fourier.html#eq-tau-T" class="quarto-xref">Equation&nbsp;<span>12.2</span></a>.</p>
<p>For <span class="math inline">\(u \ge 1\)</span> let <span class="math inline">\(\hat{X} (u | \tau_T)\)</span> denote the <em>linear least squares predictor</em> of <span class="math inline">\(X(u + t_f)\)</span> as an affine function of <span class="math inline">\(\{ X(t) \}_{t \in \tau_T}\)</span>.</p>
<p><span id="eq-linear-predictor-defn"><span class="math display">\[
\begin{align}
  (\hat{\alpha}, \hat{\beta}_\bullet) &amp;= \arg \min_{\alpha, \beta_\bullet} \; MSE \left \{ X(u + t_f), \; \alpha + \sum_{\nu = 0}^{T-1} \beta_\nu X(t_0 + \nu) \right \} \\
  \hat{X} (u | \tau_T) &amp;= \hat{\alpha} + \sum_{\nu = 0}^{T-1} \hat{\beta}_\nu X(t_0 + \nu)
\end{align}
\qquad(12.53)\]</span></span></p>
</section>
<section id="linear-equations" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="linear-equations"><span class="header-section-number">12.6.2</span> Linear Equations</h3>
<p>We assume for the moment that the first and second-order statistics (mean, variance, auto-covariance, …) of <span class="math inline">\(X(\cdot)\)</span> are known in order to express the coefficients <span class="math inline">\(\hat{\alpha}, \hat{\beta}_\bullet\)</span> as functions of those statistics.</p>
<p>Consider the expression for mean-squared error (MSE).</p>
<p><span id="eq-predictor-MSE"><span class="math display">\[
\begin{align}
  MSE \left \{ X(u + t_f), \; \alpha + \sum_{\nu = 0}^{T-1} \beta_\nu X(t_0 + \nu) \right \} \\
  = E \left \{ \left ( X(u + t_f) \; - \alpha - \sum_{\nu = 0}^{T-1} \beta_\nu X(t_0 + \nu) \right )^2 \right \}
\end{align}
\qquad(12.54)\]</span></span></p>
<p>We take the derivative of this expression with respect to each coefficient <span class="math inline">\((\alpha, \beta_\bullet)\)</span> and set that derivative to zero to define <span class="math inline">\((\hat{\alpha}, \hat{\beta}_\bullet)\)</span>. For <span class="math inline">\(\hat{\alpha}\)</span> we have</p>
<p><span id="eq-predictor-const-coeff-eqn"><span class="math display">\[
\begin{align}
  0 &amp;= E \left \{ \left ( X(u + t_f) \; - \hat{\alpha} - \sum_{\nu = 0}^{T-1} \hat{\beta}_\nu X(t_0 + \nu) \right ) \right \} \\
  &amp;= \mu_X - \hat{\alpha}  \; - \; \sum_{\nu = 0}^{T-1} \hat{\beta}_\nu \; \mu_X
\end{align}
\qquad(12.55)\]</span></span></p>
<p>so that</p>
<p><span id="eq-predictor-const-coeff-hat"><span class="math display">\[
\begin{align}
  \hat{\alpha} &amp;= \mu_X \left (1 - \sum_{\nu = 0}^{T-1} \hat{\beta}_\nu \right )
\end{align}
\qquad(12.56)\]</span></span></p>
<p>For each of the remaining coefficients <span class="math inline">\(\hat{\beta}_j\)</span> we obtain</p>
<p><span id="eq-predictor-slope-j-coeff-eqn"><span class="math display">\[
\begin{align}
  0 &amp;= E \left \{ \left ( X(u + t_f) \; - \hat{\alpha} - \sum_{\nu = 0}^{T-1} \hat{\beta}_\nu X(t_0 + \nu) \right ) \; X(t_0 + j) \right \} \\
  &amp;= E \left \{ \left ( X(u + t_0 + T - 1) - \mu_x - \sum_{\nu = 0}^{T-1} \hat{\beta}_\nu \left ( X(t_0 + \nu) - \mu_x \right ) \right ) \; X(t_0 + j) \right \} \\
  &amp;= \gamma_X(u - j + T - 1) \; - \; \sum_{\nu = 0}^{T-1} \hat{\beta}_\nu \; \gamma_X(\nu - j)
\end{align}
\qquad(12.57)\]</span></span></p>
<p>In vector-matrix notation we have the following linear system.<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a></p>
<p><span id="eq-predictor-matrix-eqn"><span class="math display">\[
\begin{align}
  \Gamma[\cdot, \cdot] \; \hat{\beta}_\bullet &amp;= \tilde{\gamma} [\cdot]
\end{align}
\qquad(12.58)\]</span></span></p>
<p>where we set the vector and matrix indices to <span class="math inline">\(j, \nu \in \nu_T = \{ 0, \dots, T-1 \}\)</span>, so that</p>
<p><span id="eq-predictor-matrix-vars"><span class="math display">\[
\begin{align}
  \Gamma [j, \nu] &amp; = \gamma_X(\nu - j) \\
  \tilde{\gamma} [j] &amp;= \gamma_X (u - j + T - 1)
\end{align}
\qquad(12.59)\]</span></span></p>
<p><span class="math inline">\(\Gamma\)</span> is a covariance matrix, and is therefore non-negative definite. In fact <span class="math inline">\(\Gamma\)</span> is positive definite (non-singular) unless <span class="math inline">\(X(\cdot)\)</span> is a degenerate process<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>. In any case <span class="math inline">\(\hat{\beta}_\bullet\)</span> can be defined by applying the inverse (or a generalized inverse) of <span class="math inline">\(\Gamma\)</span> to <span class="math inline">\(\tilde{\gamma} [\cdot]\)</span>.</p>
<p>As noted above, we have assumed for the sake of discussion that the entire auto-covariance function <span class="math inline">\(\gamma_X (\cdot)\)</span> is known. In practice such statistics must be estimated from the data. But direct estimation is not possible, due to the restricted range of differences in data indices.<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> Instead, we must rely on a model of the process for such estimates.</p>
</section>
</section>
<section id="arma-forecastingtxt-arma-forecasting" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="arma-forecastingtxt-arma-forecasting"><span class="header-section-number">12.7</span> ARMA Forecasting<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a></h2>
<section id="notation-and-assumptions" class="level3" data-number="12.7.1">
<h3 data-number="12.7.1" class="anchored" data-anchor-id="notation-and-assumptions"><span class="header-section-number">12.7.1</span> Notation and Assumptions</h3>
<p>For presentation purposes we continue to assume that all random process parameters are known. And to simplify notation we first consider the case of a stationary Gaussian process <span class="math inline">\(\mathcal{N} (\cdot)\)</span> having mean 0.</p>
<p><span id="eq-Gaussian-0-mean"><span class="math display">\[
\begin{align}
  \mathcal{N}(t) &amp;\sim N(0, \sigma_\mathcal{N}^2) \quad \forall \; t
\end{align}
\qquad(12.60)\]</span></span></p>
<p>In this case we can express the intended least squares linear approximation of <span class="math inline">\(\mathcal{N}(t)\)</span> based on <span class="math inline">\(\{ \mathcal{N}(s) \}_{s \in \mathcal{S}}\)</span> as a conditional expectation. In fact it will be convenient to be able to condition on the entire past of the random process covering indices <span class="math inline">\(\wp (t_f)\)</span> up to and including the index <span class="math inline">\(t_f\)</span> of the final observation.</p>
<p><span id="eq-le-tf"><span class="math display">\[
\begin{align}
  \wp (t_f) &amp;= \left \{ s \in \mathbb{Z} \; | \; s \le t_f \right \}
\end{align}
\qquad(12.61)\]</span></span></p>
<p>We adopt the following notation for the expectation of <span class="math inline">\(\mathcal{N}(u + t_f)\)</span>, the process at a future time, conditioned on <span class="math inline">\(\{ \mathcal{N}(s) \}_{s \le t_f}\)</span>, the history of the process prior to and including the final observation.<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a></p>
<p><span id="eq-e-future-given-past"><span class="math display">\[
\begin{align}
  \tilde{\mathcal{N}} (u + t_f \; | \; \wp (t_f)) &amp;= E \left \{ \mathcal{N} (u + t_f) \; | \; \mathcal{N} (s), \; s \le t_f \right \}
\end{align}
\qquad(12.62)\]</span></span></p>
<p>Due to the normality of <span class="math inline">\(\mathcal{N}(\cdot)\)</span> this conditional expectation <span class="math inline">\(\tilde{\mathcal{N}}\)</span> coincides with the linear least squares predictor <span class="math inline">\(\hat{\mathcal{N}}\)</span>, which is the defining linear predictor for non-normal processes.</p>
<p><span id="eq-lin-pred-and-condl-exp"><span class="math display">\[
\begin{align}
  \hat{\mathcal{N}} (u + t_f \; | \; \wp (t_f)) &amp;= \tilde{\mathcal{N}} (u + t_f \; | \; \wp (t_f))
\end{align}
\qquad(12.63)\]</span></span></p>
<p>We also assume that this Gaussian process follows an <span class="math inline">\(ARMA(p, q)\)</span> model,</p>
<p><span id="eq-normal-ARMA-proc"><span class="math display">\[
\begin{align}
  \phi(\mathcal{B}) \; \mathcal{N} (\cdot) &amp;= \theta(\mathcal{B}) \; W(\cdot) &amp; \text{with iid } W(\cdot) \sim N(0, \sigma_W^2)
\end{align}
\qquad(12.64)\]</span></span></p>
<p>where the roots of both the AR polynomial, <span class="math inline">\(\phi(\cdot)\)</span>, and the MA polynomial, <span class="math inline">\(\theta(\cdot)\)</span>, have magnitudes greater than unity. This ensures that <span class="math inline">\(\phi(\mathcal{B})^{-1}\)</span> and <span class="math inline">\(\theta(\mathcal{B})^{-1}\)</span> are well defined.</p>
</section>
<section id="predicted-variable" class="level3" data-number="12.7.2">
<h3 data-number="12.7.2" class="anchored" data-anchor-id="predicted-variable"><span class="header-section-number">12.7.2</span> Predicted Variable</h3>
<p>For fixed <span class="math inline">\(u \ge 1\)</span> the predicted variable is <span class="math inline">\(\mathcal{N} (u + t_f)\)</span>, expressed below in both causal and inverted forms.</p>
<p><span id="eq-predicted-causal-form"><span class="math display">\[
\begin{align}
  \mathcal{N} (u + t_f) &amp;= \sum_{\nu = 0}^\infty \; \psi_\nu \; W(u + t_f - \nu), \quad \psi_0 = 1
\end{align}
\qquad(12.65)\]</span></span></p>
<p><span id="eq-predicted-inv-form"><span class="math display">\[
\begin{align}
  W(u + t_f) &amp;= \sum_{\nu = 0}^\infty \; \pi_\nu \; \mathcal{N} (u + t_f - \nu), \quad \pi_0 = 1
\end{align}
\qquad(12.66)\]</span></span></p>
<p>Since <span class="math inline">\(\pi_0 = 1\)</span> the inverted form can also be expressed as an infinite AR model.</p>
<p><span id="eq-predicted-AR-infty"><span class="math display">\[
\begin{align}
  \mathcal{N} (u + t_f) &amp;= W(u + t_f) -  \sum_{\nu = 1}^\infty \; \pi_\nu \; \mathcal{N} (u + t_f - \nu)
\end{align}
\qquad(12.67)\]</span></span></p>
</section>
<section id="predictor-causal-form" class="level3" data-number="12.7.3">
<h3 data-number="12.7.3" class="anchored" data-anchor-id="predictor-causal-form"><span class="header-section-number">12.7.3</span> Predictor: Causal Form</h3>
<p>We now take the expectations in <a href="#eq-predicted-causal-form" class="quarto-xref">Equation&nbsp;<span>12.65</span></a> conditioned on <span class="math inline">\(\{ \mathcal{N} (s) \}_{s \le t_f}\)</span>.</p>
<p><span id="eq-predictor-causal-form-0"><span class="math display">\[
\begin{align}
  \tilde{\mathcal{N}} (u \; | \; \wp(t_f)) &amp;= \sum_{\nu = 0}^\infty \; \psi_\nu \; E \left \{ W(u + t_f - \nu) \; | \; \mathcal{N} (s), \; s \le t_f \right \}
\end{align}
\qquad(12.68)\]</span></span></p>
<p>Expanding <span class="math inline">\(\mathcal{N} (s)\)</span> in causal form, we see that</p>
<p><span class="math display">\[
\begin{align}
  E \left \{ W(u + t_f - \nu) \; | \; \mathcal{N} (s), \; s \le t_f \right \} &amp;=
  \begin{cases}
    0 &amp; \text{ for } \nu &lt; u \\
    W(u + t_f - \nu) &amp; \text{ for } \nu \ge u
  \end{cases}
\end{align}
\]</span></p>
<p>This eliminates the first <span class="math inline">\(u\)</span> terms in <a href="#eq-predictor-causal-form-0" class="quarto-xref">Equation&nbsp;<span>12.68</span></a> and simplifies the remaning terms.</p>
<p><span id="eq-predictor-causal-form-u"><span class="math display">\[
\begin{align}
  \tilde{\mathcal{N}} (u + t_f \; | \; \wp(t_f)) &amp;= \sum_{\nu = u}^\infty \; \psi_\nu \; W(u + t_f - \nu)
\end{align}
\qquad(12.69)\]</span></span></p>
</section>
<section id="predictor-inverted-form" class="level3" data-number="12.7.4">
<h3 data-number="12.7.4" class="anchored" data-anchor-id="predictor-inverted-form"><span class="header-section-number">12.7.4</span> Predictor: Inverted Form</h3>
<p>Next, applying the same conditional expectations to <a href="#eq-predicted-inv-form" class="quarto-xref">Equation&nbsp;<span>12.66</span></a>, we have</p>
<p><span class="math display">\[
\begin{align}
  0 &amp;= \sum_{\nu = 0}^\infty \; \pi_\nu \; \tilde{\mathcal{N}} (u + t_f - \nu \; | \; \wp (t_f))
\end{align}
\]</span></p>
<p>Since <span class="math inline">\(\pi_0 = 1\)</span>, this is equivalent to:</p>
<p><span id="eq-predictor-inv-form-1"><span class="math display">\[
\begin{align}
  \tilde{\mathcal{N}} (u + t_f \; | \; \wp (t_f) &amp;= - \sum_{\nu = 1}^\infty \; \pi_\nu \; \tilde{\mathcal{N}} (u + t_f - \nu \; | \; \wp (t_f))
\end{align}
\qquad(12.70)\]</span></span></p>
<p>Note that the terms on the right can be simplified for <span class="math inline">\(\nu \ge u\)</span>.</p>
<p><span class="math display">\[
\begin{align}
  \tilde{\mathcal{N}} (u - \nu +t_f \; | \; \wp (t_f)
  &amp;= \mathcal{N} (u - \nu + t_f) \quad \text{for } \nu \ge u
\end{align}
\]</span></p>
<p>Applying this result we have</p>
<p><span id="eq-predictor-inv-form-u"><span class="math display">\[
\begin{align}
  \tilde{\mathcal{N}} (u + t_f \; | \; \wp (t_f) &amp;= - \sum_{\nu = 1}^{u - 1} \; \pi_\nu \; \tilde{\mathcal{N}} (u + t_f - \nu \; | \; \wp (t_f)) \; - \; \sum_{\nu = u}^\infty \; \pi_\nu \; \mathcal{N} (u + t_f - \nu)
\end{align}
\qquad(12.71)\]</span></span></p>
</section>
<section id="predictor-residuals" class="level3" data-number="12.7.5">
<h3 data-number="12.7.5" class="anchored" data-anchor-id="predictor-residuals"><span class="header-section-number">12.7.5</span> Predictor Residuals</h3>
<p>We now examine the residuals from the linear least squares predictors defined above. Let <span class="math inline">\(R(\cdot \; | \; \wp (t_f))\)</span> denote the residuals of the predictors.</p>
<p><span id="eq-predictor-resid-defn"><span class="math display">\[
\begin{align}
  R(t \; | \; \wp (t_f)) &amp;= \mathcal{N} (t) \; - \; \tilde{\mathcal{N}} (t \; | \; \wp (t_f)), \quad t &gt; t_f
\end{align}
\qquad(12.72)\]</span></span></p>
<p>Combining <a href="#eq-predicted-AR-infty" class="quarto-xref">Equation&nbsp;<span>12.67</span></a> and <a href="#eq-predictor-inv-form-u" class="quarto-xref">Equation&nbsp;<span>12.71</span></a> we can express process <span class="math inline">\(R(u + t_f \; | \; \wp (t_f))\)</span> for <span class="math inline">\(u \ge 1\)</span> in a finite auto-regressive form (that expands with <span class="math inline">\(u\)</span>).</p>
<p><span id="eq-predictor-resid-AR"><span class="math display">\[
\begin{align}
  R(u + t_f \; | \; \wp (t_f)) &amp;= \mathcal{N} (u + t_f) \; - \; \tilde{\mathcal{N}} (u + t_f \; | \; \wp (t_f)) \\
  &amp;= W(u + t_f) - \sum_{\nu = 1}^{u - 1} \; \pi_\nu \; R(u - \nu\; | \; \wp (t_f))
\end{align}
\qquad(12.73)\]</span></span></p>
<p>Similarly, combing <a href="#eq-predicted-causal-form" class="quarto-xref">Equation&nbsp;<span>12.65</span></a> and <a href="#eq-predictor-causal-form-u" class="quarto-xref">Equation&nbsp;<span>12.69</span></a> we can express <span class="math inline">\(R(u + t_f \; | \; \wp (t_f))\)</span> as the following finite moving average of the white noise terms (where the number of terms again increases with <span class="math inline">\(u\)</span>).</p>
<p><span id="eq-predictor-resid-MA"><span class="math display">\[
\begin{align}
  R(u + t_f \; | \; \wp (t_f)) &amp;= \mathcal{N} (u + t_f) \; - \; \tilde{\mathcal{N}} (u \; | \; \wp (t_f) \\
  &amp;= \sum_{\nu = 0}^{u - 1} \; \psi_\nu \; W(u + t_f - \nu)
\end{align}
\qquad(12.74)\]</span></span></p>
<p>From this last result we see that the residual random variable <span class="math inline">\(R(u + t_f \; | \; \wp (t_f))\)</span> has zero mean.</p>
<p><span id="eq-e-predictor-resid-0"><span class="math display">\[
\begin{align}
  E\{ R(u + t_f \; | \; \wp (t_f)) \} &amp;= 0
\end{align}
\qquad(12.75)\]</span></span></p>
<p>Also note that <span class="math inline">\(R(u + t_f \; | \; \wp (t_f))\)</span> and <span class="math inline">\(R(u + k + t_f \; | \; \wp (t_f))\)</span> are correlated in general, for <span class="math inline">\(u \ge 1\)</span> and <span class="math inline">\(k \ge 0\)</span>.</p>
<p><span id="eq-cov-predictor-resid"><span class="math display">\[
\begin{align}
  E \left \{ R(u + t_f \; | \; \wp (t_f)) \times R(u + k + t_f \; | \; \wp (t_f)) \right \} &amp;= \sigma_W^2 \; \sum_{\nu = 0}^{u - 1} \; \psi_\nu \; \psi_{\nu + k}
\end{align}
\qquad(12.76)\]</span></span></p>
<p>Setting <span class="math inline">\(k = 0\)</span> we obtain the following expected square of <span class="math inline">\(R(u + t_f \; | \; \wp (t_f))\)</span>, called mean squared prediction error.<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a></p>
<p><span id="eq-var-predictor-resid"><span class="math display">\[
\begin{align}
  E\{ R(u + t_f \; | \; \wp (t_f))^2 \} &amp;= \sigma_W^2 \; \sum_{\nu = 0}^{u - 1} \; \psi_\nu^2
\end{align}
\qquad(12.77)\]</span></span></p>
<p>As <span class="math inline">\(u \rightarrow \infty\)</span> this residual variance approaches the variance of the original process <span class="math inline">\(\mathcal{N} (\cdot)\)</span>.</p>
<p><span id="eq-lim-predictor-resid-MSE"><span class="math display">\[
\begin{align}
  \lim_{u \rightarrow \infty} E\{ R(u\; | \; \wp (t_f))^2 \} &amp;= \sigma_W^2 \; \sum_{\nu = 0}^\infty \; \psi_\nu^2 = \sigma_\mathcal{N}^2
\end{align}
\qquad(12.78)\]</span></span></p>
</section>
<section id="truncated-predictorstxt-trunc-predict" class="level3" data-number="12.7.6">
<h3 data-number="12.7.6" class="anchored" data-anchor-id="truncated-predictorstxt-trunc-predict"><span class="header-section-number">12.7.6</span> Truncated Predictors<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a></h3>
<p>Using the entire history of the process <span class="math inline">\(\{ \mathcal{N} (s), \; s \le t_f \}\)</span> preceding and including the observed values, we’ve expressed the linear predictor <span class="math inline">\(\tilde{\mathcal{N}} (u + t_f \; | \; \wp(t_f))\)</span> as an infinite moving average (<a href="#eq-predictor-causal-form-u" class="quarto-xref">Equation&nbsp;<span>12.69</span></a>) and, alternatively, as an infinite auto-regression (<a href="#eq-predictor-inv-form-u" class="quarto-xref">Equation&nbsp;<span>12.71</span></a>).</p>
<p>Now, restricting the predictor to the available data by using the <span class="math inline">\(ARMA(p, q)\)</span> model, we recursively define the following “truncated” predictor, <span class="math inline">\(\breve{\mathcal{N}} (t \; | \; \tau_T)\)</span>, of the process <span class="math inline">\(\mathcal{N} (t)\)</span> at time <span class="math inline">\(t\)</span>.</p>
<p><span id="eq-trunc-predictor"><span class="math display">\[
\begin{align}
  \breve{\mathcal{N}} (u + t_f \; | \; \tau_T) &amp;= \sum_{j = 1}^p \phi_j \; \breve{\mathcal{N}} (u - j + t_f \; | \; \tau_T) \\
  &amp;+ \sum_{k = 0}^q \theta_k \; \breve{W} (u - k + t_f \; | \; \tau_T) \\
  \\
  \text{where } \\
  \\
  \breve{W} (t \; | \; \tau_T) &amp;= E \left \{ W(t) \; | \; \mathcal{N} (s), \; s \in \tau_T \right \}
\end{align}
\qquad(12.79)\]</span></span></p>
<p>In this recursive definition, when the time index <span class="math inline">\(t\)</span> is within the range <span class="math inline">\(\tau_T\)</span> of observed values, the truncated predictor of the data value is just the data value. And the truncated predictor of the process prior to the data is set to the mean value of the process, which is zero here. That is</p>
<p><span id="eq-trunc-predictor-cases"><span class="math display">\[
\begin{align}
  \breve{\mathcal{N}} (t \; | \; \tau_T) &amp;=
  \begin{cases}
    \mathcal{N} (t) &amp; \text{if } t \in \tau_T \\
    0 &amp; \text{if } t &lt; t_0
  \end{cases}
\end{align}
\qquad(12.80)\]</span></span></p>
<p>In addition, the conditional expectation of the white noise terms are based on the <span class="math inline">\(ARMA(p, q)\)</span> model as follows.</p>
<p><span class="math display">\[
\begin{align}
  \breve{W} (t \; | \; \tau_T) \\
  &amp;=
  \begin{cases}
    \phi (\mathcal{B}) \mathcal{N} (t) + (\mathcal{I} - \theta(\mathcal{B})) \; \breve{W} (t \; | \; \tau_T)  &amp; \text{if } t \in \tau_T \\
    0 &amp; \text{if } t \not \in \tau_T
  \end{cases}
\end{align}
\]</span></p>
<p>As previously noted, we assume all parameters to be known, so that the polynomials <span class="math inline">\(\phi(\cdot)\)</span> and <span class="math inline">\(\theta(\cdot)\)</span> are completely specified. Then the system of equations above can be solved recursively, as illustrated in the next example.</p>
</section>
<section id="arma1-1-simulationtxt-arma_1_1_sim" class="level3" data-number="12.7.7">
<h3 data-number="12.7.7" class="anchored" data-anchor-id="arma1-1-simulationtxt-arma_1_1_sim"><span class="header-section-number">12.7.7</span> ARMA(1, 1) Simulation<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a></h3>
<p>We illustrate the truncated predictor for an <span class="math inline">\(ARMA(1, 1)\)</span> model:</p>
<p><span id="eq-ARMA-1-1-xmpl"><span class="math display">\[
\begin{align}
  \mathcal{N}(t) &amp;= \phi_1 \; \mathcal{N}(t - 1) \; + \; W(t) \; + \; \theta_1 \; W(t - 1)
\end{align}
\qquad(12.81)\]</span></span></p>
<p>The truncated predictor is</p>
<p><span id="eq-tpu-1-1"><span class="math display">\[
\begin{align}
  \breve{\mathcal{N}} (u + t_f \; | \; \tau_T) &amp;= \phi_1 \; \breve{\mathcal{N}} (u - 1 + t_f \; | \; \tau_T) \; + \; \breve{W} (u + t_f \; | \; \tau_T) \; + \; \theta_1 \; \breve{W} (u - 1 + t_f \; | \; \tau_T)
\end{align}
\qquad(12.82)\]</span></span></p>
<p>Setting <span class="math inline">\(u = 1\)</span>, we have</p>
<p><span id="eq-tp1-1-1"><span class="math display">\[
\begin{align}
  \breve{\mathcal{N}} (1 + t_f \; | \; \tau_T) &amp;= \phi_1 \; \breve{\mathcal{N}} (t_f \; | \; \tau_T) \; + \; \breve{W} (1 + t_f \; | \; \tau_T) \; + \; \theta_1 \; \breve{W} (t_f \; | \; \tau_T) \\
  &amp;= \phi_1 \; \mathcal{N} (t_f \; | \; \tau_T) \; + \; 0 \; + \; \theta_1 \; \breve{W} (t_f \; | \; \tau_T)
\end{align}
\qquad(12.83)\]</span></span></p>
<p>For <span class="math inline">\(u \ge 2\)</span> we have</p>
<p><span id="eq-tp2-1-1"><span class="math display">\[
\begin{align}
  \breve{\mathcal{N}} (u + t_f \; | \; \tau_T) &amp;= \phi_1 \; \breve{\mathcal{N}} (u - 1 + t_f \; | \; \tau_T)
\end{align}
\qquad(12.84)\]</span></span></p>
<p>which can be calculated recursively.</p>
<p>To initialize these recursive calculations we still need to determine <span class="math inline">\(\breve{W} (t_f \; | \; \tau_T)\)</span>. To this end we re-express the ARMA model and set the following initial conditions.</p>
<p><span id="eq-ARMA-1-1-init"><span class="math display">\[
\begin{align}
  \breve{W} (t \; | \; \tau_T) &amp;= \mathcal{N}(t) \; - \; \phi_1 \; \mathcal{N}(t - 1) \; - \; \theta_1 \; \breve{W} (t - 1 \; | \; \tau_T) \\
  \\
  &amp;\text{for } t \in \tau_T, \text{ and setting } \\
  \\
  0 &amp;= \mathcal{N} (t_0 - 1) = \breve{W} (t_0 - 1 \; | \; \tau_T)
\end{align}
\qquad(12.85)\]</span></span></p>
<p>We can solve the last equation in a forward recursion with <span class="math inline">\(t \in (t_0, \ldots, t_f)\)</span> to obtain <span class="math inline">\(\breve{W} (t_f \; | \; \tau_T)\)</span>, as required.</p>
<p>The forecast variance can be approximated from <a href="#eq-var-predictor-resid" class="quarto-xref">Equation&nbsp;<span>12.77</span></a> using <span class="math inline">\(\psi\)</span> coefficients that turn out to satisfy <a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> <a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a></p>
<p><span id="eq-ARMA-1-1-psi"><span class="math display">\[
\begin{align}
  \psi_j &amp;= (\phi_1 + \theta_1) \; \phi_1^{j-1} \quad \text{for } j \ge 1
\end{align}
\qquad(12.86)\]</span></span></p>
<p>We then have</p>
<p><span id="eq-var-tp-1-1-resid"><span class="math display">\[
\begin{align}
  Var \{ \breve{R} (u + t_f \; | \; \tau_T) \} &amp;= E \{ \breve{R} (u + t_f \; | \; \tau_T)^2 \} \\
  &amp;= \sigma_W^2 \; \sum_{\nu = 0}^{u - 1} \; \psi_\nu^2 \\
  &amp;= \sigma_W^2 \; \left ( 1 + \frac{(\phi_1 + \theta_1)^2 (1 - \phi_1^{2 (u - 1)})}{1 - \phi_1^2} \right ) \\
  \\
  &amp; \text{where } \\
  \\
  \breve{R} (u + t_f \; | \; \tau_T) &amp;= \mathcal{N}(u + t_f) \; - \; \breve{\mathcal{N}} (u + t_f \; | \; \tau_T)
\end{align}
\qquad(12.87)\]</span></span></p>
<p>The precision of the forecast at time index <span class="math inline">\(u + t_f\)</span> can be assessed with a prediction interval of the following form</p>
<p><span id="eq-trunc-pred-ivl-alpha"><span class="math display">\[
\begin{align}
  \breve{\mathcal{N}} (u + t_f \; | \; \tau_T) \; &amp;\pm \; c_{\alpha / 2} \; \sqrt{ Var \{ \breve{R} (u + t_f \; | \; \tau_T) \} } \\
  \\
  &amp; \text{where coefficient } c_{\alpha / 2} \text{ is the normal quantile:} \\
  \\
  c_{\alpha / 2} &amp;= \Phi^{-1} (1 - \alpha / 2)
\end{align}
\qquad(12.88)\]</span></span></p>
<p>We now set <span class="math inline">\(\phi_1 = \theta_1 = 0.5\)</span> in order to simulate 150 observations from this model and forecast the last 50 of the 150 observations. This example uses <code>astsa</code> functions <code>sarima.sim()</code> and <code>sarima.for()</code>.<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-tp-1-1-sim" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tp-1-1-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-tp-1-1-sim-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tp-1-1-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.2: ARMA(1, 1) simulation, with coefficients (.5, .5)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="recruitment-forecast" class="level3" data-number="12.7.8">
<h3 data-number="12.7.8" class="anchored" data-anchor-id="recruitment-forecast"><span class="header-section-number">12.7.8</span> Recruitment Forecast</h3>
<p><a href="#fig-soi" class="quarto-xref">Figure&nbsp;<span>12.14</span></a> shows the Southern Oscillation Index (SOI, <code>astsa::soi</code>) and associated Recruitment (<code>astsa::rec</code>), an index of the number of young fish entering the cohort available for commercial fishing. Both series consist of 453 monthly values during the years 1950–1987.</p>
<p><span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span> fit the following preliminary AR(2) model to the recruitment index.</p>
<p><span id="eq-rec-fit-AR2"><span class="math display">\[
\begin{align}
  \mathcal{N} (t) &amp;= \alpha + \phi_1 \mathcal{N} (t-1) + \phi_2 \mathcal{N} (t-2) + W(t) \\
  \\
  &amp; \text{with estimated coefficients } \\
  \\
  \hat{\alpha} &amp;= 6.74 \\
  \hat{\phi}_1 &amp;= 1.35 \\
  \hat{\phi}_2 &amp;= -0.46 \\
  \hat{\sigma}_W^2 &amp;= 89.72
\end{align}
\qquad(12.89)\]</span></span></p>
<p>For purposes of discussion let us accept the coefficient estimates as given constants that specify the <span class="math inline">\(AR(2)\)</span> model. Then the truncated predictor can be determined recursively from the following formula.</p>
<p><span id="eq-tp-rec-AR2"><span class="math display">\[
\begin{align}
  \breve{\mathcal{N}} (t \; | \; \tau_T) &amp;= \alpha +  \phi_1 \breve{\mathcal{N}} (t-1 \; | \; \tau_T) + \phi_2 \breve{\mathcal{N}} (t-2 \; | \; \tau_T) + \breve{W}(t \; | \; \tau_T) \\
  \\
  \text{with } \\
  \\
  \breve{\mathcal{N}} (t \; | \; \tau_T) &amp;=
  \begin{cases}
    \mathcal{N} (t) &amp; \text{if } t \in \tau_T \\
    0 &amp; \text{if } t &lt; t_0
  \end{cases} \\
  \\
  \text{and } \\
  \\
  \breve{W}(t \; | \; \tau_T) &amp;= 0 \quad \text{if } t \not \in \tau_T
\end{align}
\qquad(12.90)\]</span></span></p>
<p>To approximate prediction variance we need the <span class="math inline">\(\psi\)</span> coefficients of the causal form of the <span class="math inline">\(AR(2)\)</span> model. From <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span> we have</p>
<p><span class="math display">\[
\begin{align}
  \psi_j &amp;= \phi_1 \psi_{j-1} + \phi_2 \psi_{j-2} \quad \text{for } j \ge 2 \\
  \\
  &amp;\text{with initial conditions } \\
  \\
  \psi_0 &amp;= 1 \\
  \psi_1 &amp;= \phi_1
\end{align}
\]</span></p>
<p>These calculations are incorporated into the following <code>astsa</code> forecast.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-rec-AR-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rec-AR-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-rec-AR-2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rec-AR-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.3: Recruitment index followed by a 24-month AR(2) forecast
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="non-stationary-models" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="non-stationary-models"><span class="header-section-number">12.8</span> Non-Stationary Models</h2>
<section id="arimap-d-q" class="level3" data-number="12.8.1">
<h3 data-number="12.8.1" class="anchored" data-anchor-id="arimap-d-q"><span class="header-section-number">12.8.1</span> ARIMA(p, d, q)</h3>
<p>Time series models have a long history. <span class="citation" data-cites="whittle1951hypothesis">(<a href="references.html#ref-whittle1951hypothesis" role="doc-biblioref"><strong>whittle1951hypothesis?</strong></a>)</span> introduced ARMA models of stationary processes. To treat non-stationary processes, <span class="citation" data-cites="box1970time">(<a href="references.html#ref-box1970time" role="doc-biblioref"><strong>box1970time?</strong></a>)</span> popularized auto-regressive integrated moving average (ARIMA) models. Here’s the form of an ARIMA model having <span class="math inline">\(p, d, q\)</span> as its respective orders of auto-regression, differencing, and moving average.<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a></p>
<p><span id="eq-ARIMA-p-d-q-proc"><span class="math display">\[
\begin{align}
  \phi(\mathcal{B}) \; \nabla^d \; (X(\cdot) - \mu_X)
  &amp;= \theta(\mathcal{B}) \; W(\cdot) \\
  \\
  \text{where } \\
  &amp; \phi (z) = 1 - \sum_{j = 1}^p \phi_j \; z^j \\
  &amp; \theta (z) = 1 + \sum_{k = 1}^q \theta_k \; z^k
\end{align}
\qquad(12.91)\]</span></span></p>
<p>By convention, <span class="math inline">\(d \in \{ 0, 1, 2, \ldots \}\)</span> so that <span class="math inline">\(ARIMA(p, 0, q)\)</span> is synonymous with <span class="math inline">\(ARMA(p, q)\)</span>.</p>
<p>If <span class="math inline">\(d \ge 1\)</span> then <span class="math inline">\(\nabla^d\)</span> removes <span class="math inline">\(\mu_X\)</span>, which can be omitted from the formula for simplificaton. As previously noted the operator <span class="math inline">\(\nabla^d\)</span> removes any polynomial trend of degree <span class="math inline">\(d\)</span> or less.</p>
<p>In addition to removing non-random trends in the data, the differencing operator <span class="math inline">\(\nabla\)</span> can also remove random drift. Consider the following model.</p>
<p><span id="eq-ARIMA-1-1-0-xmpl"><span class="math display">\[
\begin{align}
  X(t) &amp;= Y(t) + W(t) \\
  Y(t) &amp;= Y(t-1) + V(t) \\
  \\
  W(\cdot) &amp;\sim \text{iid } wn(0, \sigma_W^2) \\
  V(\cdot) &amp;\sim \text{iid } wn(0, \sigma_V^2) \\
  W(\cdot), \; V(\cdot) &amp;\sim \text{independent }
\end{align}
\qquad(12.92)\]</span></span></p>
<p>The process <span class="math inline">\(Y(\cdot)\)</span> is a <em>random walk</em> whose variance increases over time and is thus non-stationary. Therefore <span class="math inline">\(X(\cdot)\)</span> is also non-stationary. But applying the differencing operator <span class="math inline">\(\nabla\)</span> to <span class="math inline">\(X(\cdot)\)</span> induces stationarity.<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a></p>
<p><span class="math display">\[
\begin{align}
  \nabla \; X(t) &amp;= \nabla \; Y(t) + \nabla \; W(t) \\
  &amp;= V(t) \; + \; \nabla \; W(t)
\end{align}
\]</span></p>
</section>
<section id="random-walk-with-drift" class="level3" data-number="12.8.2">
<h3 data-number="12.8.2" class="anchored" data-anchor-id="random-walk-with-drift"><span class="header-section-number">12.8.2</span> Random Walk with Drift</h3>
<p>Now consider the following model which, like the example just given, is a variation on a simple random walk.</p>
<p><span id="eq-RW-drift"><span class="math display">\[
\begin{align}
  X(t) &amp;= \alpha \; + \; X(t-1) + \; W(t)
\end{align}
\qquad(12.93)\]</span></span></p>
<p><span class="math inline">\(X(\cdot)\)</span> is not stationary since its mean and variance change over time. In this case, however, <span class="math inline">\(\nabla X(t)\)</span> is simply a constant plus white noise.</p>
<p><span class="math display">\[
\begin{align}
  \nabla X(t) &amp;= X(t) \; - \; X(t-1) \\
  &amp;= \alpha \; + \; W(t)
\end{align}
\]</span></p>
<p>The coefficient <span class="math inline">\(\alpha\)</span> is called <em>drift</em> in this context.</p>
<p>The truncated predictor at <span class="math inline">\(u = 1\)</span> (called the <em>one step ahead forecast</em>) can be determined as follows. Consider the case in which both <span class="math inline">\(X(\cdot)\)</span> and <span class="math inline">\(W(\cdot)\)</span> are Gaussian processes. This enables us to use conditional expectations to derive the form of the best linear predictor in the non-Gaussian case. We have</p>
<p><span id="eq-RW-drift-tp-1"><span class="math display">\[
\begin{align}
  \breve{X} (1 + t_f \; | \; \tau_T) &amp;= E \left \{ X(1 + t_f) \; | \; X(t_0), \; \ldots, \; X(t_f) \right \} \\
  &amp;= E \left \{ \alpha \; + \; X(t_f) + \; W(1 + t_f) \; | \; X(t_0), \; \ldots, \; X(t_f) \right \} \\
  &amp;= \alpha \; + \; X(t_f)
\end{align}
\qquad(12.94)\]</span></span></p>
<p>At <span class="math inline">\(u = 2\)</span> we have</p>
<p><span id="eq-RW-drift-tp-2"><span class="math display">\[
\begin{align}
  \breve{X} (2 + t_f \; | \; \tau_T) &amp;= E \left \{ X(2 + t_f) \; | \; X(t_0), \; \ldots, \; X(t_f) \right \} \\
  &amp;= E \left \{ \alpha \; + \; X(1 +t_f) + \; W(2 + t_f) \; | \; X(t_0), \; \ldots, \; X(t_f) \right \} \\
  &amp;= \alpha \; + \; \breve{X} (1 + t_f \; | \; \tau_T) \\
  &amp;= 2 \alpha \; + \; X(t_f)
\end{align}
\qquad(12.95)\]</span></span></p>
<p>By induction we have</p>
<p><span id="eq-RW-drift-tp-u"><span class="math display">\[
\begin{align}
  \breve{X} (u + t_f \; | \; \tau_T) &amp;= u \times  \alpha \; + \; X(t_f) \quad \text{for } u \ge 1
\end{align}
\qquad(12.96)\]</span></span></p>
<p>Thus the sequence of predictors forms a ray anchored at <span class="math inline">\(X(t_f)\)</span> having slope <span class="math inline">\(\alpha\)</span>.</p>
<p>The error bounds for this predictor are based on the mean squared prediction error, that is the expected value of the squared residual:</p>
<p><span class="math display">\[
\begin{align}
  E \{ \breve{R} (u + t_f \; | \; \tau_T)^2 \}
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{align}
  \breve{R} (u + t_f \; | \; \tau_T) &amp;= X(u + t_f) \; - \; \breve{X} (u + t_f \; | \; \tau_T)
\end{align}
\]</span></p>
<p>Now by induction one can show that</p>
<p><span class="math display">\[
\begin{align}
  X(u + t_f) &amp;= u \times \alpha \; + \; X(t_f) + \; \sum_{\nu = 1}^u W(\nu + t_f) \\
  &amp;= \breve{X} (u + t_f \; | \; \tau_T) + \; \sum_{\nu = 1}^u W(\nu + t_f)
\end{align}
\]</span></p>
<p>Therefore</p>
<p><span class="math display">\[
\begin{align}
  \breve{R} (u + t_f \; | \; \tau_T) &amp;= X(u + t_f) \; - \; \breve{X} (u + t_f \; | \; \tau_T) \\
  &amp;= \sum_{\nu = 1}^u W(\nu + t_f)
\end{align}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\begin{align}
  E \{ \breve{R} (u + t_f \; | \; \tau_T)^2 \} &amp;= u \times \sigma_W^2
\end{align}
\]</span></p>
<p>We now simulate this model, setting the drift parameter <span class="math inline">\(\alpha = 0.2\)</span> and simulating <span class="math inline">\(W(\cdot)\)</span> as iid standard normal variables, so that <span class="math inline">\(\sigma_W^2 = 1\)</span>. We simulate a total of 150 values, with the first 100 serving as the observations from which the predictors are calculated and compared to the final 50 values. The results are shown in figure <a href="#fig-rw-drift" class="quarto-xref">Figure&nbsp;<span>12.4</span></a>.</p>
<p><span class="math display">\[
\begin{align}
  \breve{R} (u + t_f \; | \; \tau_T) &amp;= X(u + t_f) \; - \; \breve{X} (u + t_f \; | \; \tau_T)
\end{align}
\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-rw-drift" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rw-drift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-rw-drift-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rw-drift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.4: Random walk with drift: cumsum of N(drift = 0.2, sd = 1)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-EWMA" class="level3" data-number="12.8.3">
<h3 data-number="12.8.3" class="anchored" data-anchor-id="sec-EWMA"><span class="header-section-number">12.8.3</span> EWMA Example</h3>
<p>The following <span class="math inline">\(ARIMA(0, 1, 1)\)</span> model has been applied to many economic time series.<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a></p>
<p><span id="eq-IMA-1-1-lambda"><span class="math display">\[
\begin{align}
  \nabla \; X(\cdot) &amp;= (\mathcal{I - \lambda B}) \; W(\cdot) \\
  \\
  &amp; \text{so that } \\
  \\
  X(t) &amp;= X(t-1) + W(t) - \lambda \; W(t-1) \\
  \\
  &amp; \text{with } | \lambda | &lt; 1, \text{ and } W(\cdot) \sim wn(0, \sigma_W^2)
\end{align}
\qquad(12.97)\]</span></span></p>
<p>The model is also referred to as an integrated moving average model, or <span class="math inline">\(IMA(1, 1)\)</span>. The model leads to a frequently used forecasting method called exponentially weighted moving averages (EWMA), and also called simple exponential smoothing (SES).</p>
<p>Note that the multiplicative inverse of the polynomial <span class="math inline">\(1 - \lambda z\)</span> is a geometric series in <span class="math inline">\(\lambda z\)</span>. This gives the following inverted form of the model.</p>
<p><span class="math display">\[
\begin{align}
  W(t) &amp;= \left ( \mathcal{I} - \lambda \mathcal{B} \right )^{-1} \; \nabla X (t) \\
  &amp;= \sum_{\nu = 0}^\infty \lambda^\nu \; \mathcal{B}^\nu \; \nabla X (t) \\
  &amp;= \sum_{\nu = 0}^\infty \lambda^\nu \; (X(t - \nu) \; - \; X(t - \nu - 1)) \\
  &amp;= X(t) \; - \; (X(t - 1) - \; \lambda \; X(t - 1)) \; - \cdots \\
  &amp;= X(t) \; - \; \sum_{\nu = 1}^\infty (1 - \lambda) \; \lambda^{\nu - 1} \; X(t - \nu)
\end{align}
\]</span></p>
<p>Rearranging terms we have</p>
<p><span id="eq-EWMA-plus-wn"><span class="math display">\[
\begin{align}
  X(t) &amp;= (1 - \lambda) \; \sum_{\nu = 1}^\infty \lambda^{\nu - 1} \; X(t - \nu) \; + \; W(t)
\end{align}
\qquad(12.98)\]</span></span></p>
<p>That is, the current value <span class="math inline">\(X(t)\)</span> is an exponential smoothing of all past values plus <span class="math inline">\(W(t)\)</span>, the white noise term.</p>
<p>The linear prediction of the current value based on past values is thus the same exponential smoothing of past values.<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a></p>
<p><span id="eq-SES"><span class="math display">\[
\begin{align}
  \hat{X}(t \; | \; \wp (t-1)) &amp;= (1 - \lambda) \; \sum_{\nu = 1}^\infty \lambda^{\nu - 1} \; X(t - \nu)
\end{align}
\qquad(12.99)\]</span></span></p>
<p>so that</p>
<p><span id="eq-SES-plus-wn"><span class="math display">\[
\begin{align}
  X(t) &amp;= \hat{X}(t \; | \; \wp (t-1)) \; + \; W(t)
\end{align}
\qquad(12.100)\]</span></span></p>
<p>The predictor <span class="math inline">\(\hat{X}(t \; | \; \wp (t-1))\)</span> is called the <em>one step ahead linear predictor</em>. We now have the following linear recursion formula.</p>
<p><span id="eq-EWMA-recur-1"><span class="math display">\[
\begin{align}
  \hat{X}(t \; | \; \wp (t-1)) &amp;= (1 - \lambda) \; X(t - 1) \; + \; (1 - \lambda) \; \sum_{\nu = 2}^\infty \lambda^{\nu - 1} \; X(t - \nu) \\
  &amp;= (1 - \lambda) \; X(t - 1) \; + \; (1 - \lambda) \; \sum_{\nu = 1}^\infty \lambda^\nu \; X(t -1 - \nu) \\
  &amp;= (1 - \lambda) \; X(t - 1) \; + \; \lambda \left \{ (1 - \lambda) \; \sum_{\nu = 1}^\infty \lambda^{\nu - 1} \; X(t -1 - \nu) \right \}  \\
  &amp;= (1 - \lambda) \; X(t - 1) \; + \; \lambda \; \hat{X}(t -1 \; | \; \wp (t-2))
\end{align}
\qquad(12.101)\]</span></span></p>
<p>That is, the one step ahead predictor of the current process value <span class="math inline">\(X(t)\)</span> based on the past is a convex combination of <span class="math inline">\(X(t-1)\)</span>, the most recent process value, and the preceding one step ahead predictor.</p>
<p>In order to predict two steps ahead we modify <a href="#eq-IMA-1-1-lambda" class="quarto-xref">Equation&nbsp;<span>12.97</span></a> by changing <span class="math inline">\(t\)</span> to <span class="math inline">\(t + 1\)</span>, while continuing to take conditional expectations with respect to <span class="math inline">\(\{X(s), \; s \le t - 1 \}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
  X(t+1) - X(t) &amp;= W(t+1) - \lambda \; W(t)
\end{align}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\begin{align}
  \hat{X}(t + 1 \; | \; \wp (t-1)) \; - \; \hat{X}(t \; | \; \wp (t-1) &amp;= 0
\end{align}
\]</span></p>
<p>That is,</p>
<p><span id="eq-EWMA-recur-2"><span class="math display">\[
\begin{align}
  \hat{X}(t + 1 \; | \; \wp (t-1)) &amp;= \hat{X}(t \; | \; \wp (t-1)
\end{align}
\qquad(12.102)\]</span></span></p>
<p>Then by induction on <span class="math inline">\(u \ge 1\)</span> we obtain</p>
<p><span id="eq-EWMA-flatline"><span class="math display">\[
\begin{align}
  \hat{X}(t + u \; | \; \wp (t-1)) &amp;= \hat{X}(t \; | \; \wp (t-1) \quad \text{for } u \ge 1
\end{align}
\qquad(12.103)\]</span></span></p>
<p>In words, forecasting beyond one step reverts back to the one step ahead predictor.</p>
<p>Now, the truncated one step ahead predictor based on the data follows a similar recursion.</p>
<p><span id="eq-tp-one-step-recur"><span class="math display">\[
\begin{align}
  \breve{X} \left ( 1 + t \; | \; [t_0, \; t] \right ) &amp;=
  \begin{cases}
    X(t_0) &amp; \text{for } t = t_0 \\
    (1 - \lambda) \; X(t) + \lambda \breve{X}(t \; | \; [t_0, \; t - 1] &amp; \text{for } t_0 &lt; t &lt; t_f
  \end{cases}
\end{align}
\qquad(12.104)\]</span></span></p>
<p>To calculate predictor error bounds we require <span class="math inline">\(\psi\)</span> coefficients from the causal form of the model. In this example they are the power series coefficients of the rational function <span class="math inline">\(\psi(z) = (1 - \lambda \; z)(1 - z)^{-1}\)</span>. This gives</p>
<p><span class="math display">\[
\begin{align}
  \psi_\nu &amp;=
  \begin{cases}
    1 &amp; \text{for } \nu = 0 \\
    1 - \lambda &amp; \text{for } \nu \ge 1
  \end{cases}
\end{align}
\]</span></p>
<p>Squaring these <span class="math inline">\(\psi\)</span> coefficients we have</p>
<p><span class="math display">\[
\begin{align}
  &amp; MSE \left\{ X (u + t_0), \; \breve{X}(u + t_0 \; | \; \tau_u) \right \} \\
  &amp;= \sigma_W^2 \; \left \{ 1 + (u-1)\; (1 - \lambda)^2 \right \} \quad \text{for } 0 &lt; u &lt; T
\end{align}
\]</span></p>
<p>To forecast beyond the data we assume for the moment that <span class="math inline">\(X(\cdot)\)</span> and <span class="math inline">\(W(\cdot)\)</span> are Gaussian processes in order to derive least squares predictors form conditional expectations.</p>
<p><span class="math display">\[
\begin{align}
  \breve{X}(u + t_f \; | \; \tau_T)
  &amp;= E \left \{ X (u + t_f) \; | \; X(s), s \in \tau_T \right \}
\end{align}
\]</span></p>
<p>Now, based on the <span class="math inline">\(IMA(1, 1)\)</span> model we have <a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a></p>
<p><span class="math display">\[
\begin{align}
  &amp; X (u + t_f) \\
  &amp;= X(u - 1 + t_f) + W(u + t_f) - \lambda W(u - 1 + t_f) \\
  &amp; \vdots \\
  &amp;= X(t_f) + W(u + t_f) - \lambda W(t_f) + (1 - \lambda) \; \sum_{\nu = 1}^{u -1} W(u - \nu + t_f) \\
  \\
  &amp; \text{where } u \ge 1
\end{align}
\]</span></p>
<p>Consequently</p>
<p><span class="math display">\[
\begin{align}
  &amp; \breve{X}(u + t_f \; | \; \tau_T) \\
  &amp;= E \left \{ X (u + t_f) \; | \; X(s), s \in \tau_T \right \} \\
  &amp;= X(t_f)
\end{align}
\]</span></p>
<p>The mean squared prediction error is thus the expected square of <span class="math inline">\(X(u + t_f) \; - \; X(t_f)\)</span>.</p>
<p><span class="math display">\[
\begin{align}
  &amp; MSE \left\{ X (u + t_f), \; \breve{X}(u + t_f \; | \; \tau_T) \right \} \\
  &amp;= \sigma_W^2 \; \left \{ 1 + \lambda^2 + (u-1)\; (1 - \lambda)^2 \right \} \quad \text{for } u \ge 1
\end{align}
\]</span></p>
<p>Having determined the MSE, we summarize results for the one step ahead truncated predictor itself.</p>
<p><span id="eq-tp-one-step-all-cases"><span class="math display">\[
\begin{align}
  \breve{X} \left ( 1 + t \; | \; [t_0, \; t] \cap \tau_T \right ) &amp;=
  \begin{cases}
    X(t_0) &amp; \text{for } t = t_0 \\
    (1 - \lambda) \; X(t) + \lambda \breve{X}(t \; | \; [t_0, \; t - 1] &amp; \text{for } t_0 &lt; t &lt; t_f \\
    X(t_f) &amp; \text{for } t_f \le t
  \end{cases}
\end{align}
\qquad(12.105)\]</span></span></p>
<p><a href="#fig-IMA-EWMA" class="quarto-xref">Figure&nbsp;<span>12.5</span></a> shows a simulation of the IMA process <span class="math inline">\(X(\cdot)\)</span>, along with one step ahead exponential smoothing of past values, and the <span class="math inline">\(IMA(1, 1)\)</span> forecast of future values. We set <span class="math inline">\(\lambda =\)</span> 0.8, equivalent to an EWMA smoothing parameter of <span class="math inline">\(\alpha = 1 - \lambda =\)</span> 0.2.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-IMA-EWMA" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-IMA-EWMA-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-IMA-EWMA-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-IMA-EWMA-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.5: IMA(1, 1) with exponenetial smoothing
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-ETS" class="level3" data-number="12.8.4">
<h3 data-number="12.8.4" class="anchored" data-anchor-id="sec-ETS"><span class="header-section-number">12.8.4</span> ETS</h3>
<section id="ets-intro" class="level4" data-number="12.8.4.1">
<h4 data-number="12.8.4.1" class="anchored" data-anchor-id="ets-intro"><span class="header-section-number">12.8.4.1</span> ETS Intro</h4>
<p>Exponential smoothing with trend and seasonality (ETS) is a modeling framework, distinct from ARIMA, based on Holt-Winters forecasting, which is widely used in economic analyses. <a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a> <a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a></p>
<p>In this framework the forecast <span class="math inline">\(\breve{X}(u + t \; | \; s \le t)\)</span>, with <span class="math inline">\(s, t \in \tau_T\)</span>, consists of three components:</p>
<ul>
<li>level <span class="math inline">\(\ell (t)\)</span>;</li>
<li>local trend <span class="math inline">\(\mathit{b} (t)\)</span>; and</li>
<li>seasonality <span class="math inline">\(\mathscr{s} (u + t \; | \; s \le t)\)</span>.</li>
</ul>
<p>The forecast equation expresses <span class="math inline">\(\breve{X}(u + t \; | \; s \le t)\)</span> as a linear combination of these components, and each component has its own linear equation. Together these equations define a <em>state-space</em> model.</p>
<p>In this section we merely outline the ETS framework. For further details see the cited resources. Also, note that <code>R</code> function <code>fable::ETS()</code> selects and fits the models described in this section, requiring only modest guidance from the user.</p>
</section>
<section id="level-component" class="level4" data-number="12.8.4.2">
<h4 data-number="12.8.4.2" class="anchored" data-anchor-id="level-component"><span class="header-section-number">12.8.4.2</span> Level component</h4>
<p>We begin with simple exponential smoothing (SES), which entails the level component <span class="math inline">\(\ell(\cdot)\)</span> alone, a case covered from the ARIMA perspective in <a href="#sec-EWMA" class="quarto-xref"><span>Section 12.8.3</span></a>. Following ETS conventions we change parameters by setting <span class="math inline">\(\alpha = 1 - \lambda\)</span>, with <span class="math inline">\(\alpha\)</span> restricted to the open unit interval. Then we have the following linear recursion for the one step ahead predictor.</p>
<p><span id="eq-ets-ANN-one-step-recur-alpha"><span class="math display">\[
\begin{align}
  \breve{X} \left ( t + 1 \; | \; [t_0, \; t] \cap \tau_T \right ) &amp;=
  \begin{cases}
    X(t_0) &amp; \text{for } t = t_0 \\
    \alpha \; X(t) + (1 - \alpha) \breve{X}(t \; | \; [t_0, \; t - 1] &amp; \text{for } t_0 &lt; t &lt; t_f \\
    X(t_f) &amp; \text{for } t_f \le t
  \end{cases} \\
  \\
  &amp;\text{where } \alpha \in (0, 1)
\end{align}
\qquad(12.106)\]</span></span></p>
<p>Now setting aside these boundary conditions for the moment, the state-space form of this linear recursion is as follows.</p>
<p><span id="eq-ets-ANN-ss"><span class="math display">\[
\begin{align}
  \breve{X}(t + 1 \; | \; s \le t) &amp;= \ell (t) \\
  \ell (t) &amp;= \alpha \; X(t) + (1 - \alpha) \; \ell (t - 1)
\end{align}
\qquad(12.107)\]</span></span></p>
<p>where <span class="math inline">\(\ell (t)\)</span> predicts <span class="math inline">\(X(t + 1)\)</span> based on <span class="math inline">\(\{ X(s) \}_{s \le t}\)</span>.</p>
<p>From <a href="#eq-EWMA-flatline" class="quarto-xref">Equation&nbsp;<span>12.103</span></a> and <a href="#eq-ets-ANN-one-step-recur-alpha" class="quarto-xref">Equation&nbsp;<span>12.106</span></a> we see that for <span class="math inline">\(u \ge 1\)</span>, <span class="math inline">\(\breve{X}(t + u \; | \; s \le t)\)</span> reverts to <span class="math inline">\(\breve{X}(t + 1 \; | \; s \le t)\)</span>. Thus we have</p>
<p><span id="eq-ets-ANN-u-steps-ahead"><span class="math display">\[
\begin{align}
  \breve{X}(t + u \; | \; s \le t) &amp;= \ell (t) \\
  \ell (t) &amp;= \alpha \; X(t) + (1 - \alpha) \; \ell (t - 1)
\end{align}
\qquad(12.108)\]</span></span></p>
<p>We model the difference <span class="math inline">\(X(t) - \ell (t - 1)\)</span> as a Gaussian white noise process, <span class="math inline">\(W(t)\)</span>. <a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a></p>
<p><span id="eq-ets-ANN-ss-wn"><span class="math display">\[
\begin{align}
  W(t) &amp;= X(t) - \ell (t - 1) \\
  \\
  &amp; \text{so that } \\
  \\
  X(t) &amp;= \ell(t-1) + W(t) \\
  \\
  &amp; \text{with } W(\cdot) \sim \text{iid } N(0, \sigma_W^2)
\end{align}
\qquad(12.109)\]</span></span></p>
<p>The white noise term <span class="math inline">\(W(t)\)</span> is also called an error term, and this state space model is said to have <em>additive error</em>. Equivalently, the level component <span class="math inline">\(\ell (\cdot)\)</span> is said to be additive.</p>
</section>
<section id="trend-component" class="level4" data-number="12.8.4.3">
<h4 data-number="12.8.4.3" class="anchored" data-anchor-id="trend-component"><span class="header-section-number">12.8.4.3</span> Trend component</h4>
<p>We now add a trend component, <span class="math inline">\(\mathit{b} (t)\)</span>, to this system as follows.</p>
<p><span id="eq-ets-AAN-ss"><span class="math display">\[
\begin{align}
  \breve{X}(t + u \; | \; s \le t) &amp;= \ell (t) \; + \; \mathit{b}(t) \times u \\
  \ell (t) &amp;= \alpha \; X(t) + (1 - \alpha) \; \left ( \ell(t - 1) + \mathit{b}(t - 1) \right ) \\
  \mathit{b} (t) &amp;= \beta \; \left ( \ell(t) - \ell(t - 1) \right ) \; + \; (1 - \beta) \; \mathit{b}(t - 1) \\
  \\
  &amp;\text{where } \alpha, \beta \in (0, 1)
\end{align}
\qquad(12.110)\]</span></span></p>
<p>This forecast system is known as Holt’s trend method, published decades ago. Since then a modification has been introduced to dampen the effect of the trend term <span class="math inline">\(\mathit{b}(t)\)</span> on long-term forecasts, via damping coefficient <span class="math inline">\(\phi \in (0, 1)\)</span>, as follows.</p>
<p><span id="eq-ets-AAN-dampen-ss"><span class="math display">\[
\begin{align}
  \breve{X}(u + t \; | \; s \le t) &amp;= \ell (t) \; + \; \mathit{b}(t) \times \sum_{\nu = 1}^u \phi^\nu \\
  \ell (t) &amp;= \alpha \; X(t) + (1 - \alpha) \; \left ( \ell(t - 1) + \phi \;  \mathit{b}(t - 1) \right ) \\
  \mathit{b} (t) &amp;= \beta \; \left ( \ell(t) - \ell(t - 1) \right ) \; + \; (1 - \beta) \; \phi \;  \mathit{b}(t - 1) \\
  \\
  &amp;\text{where } \alpha, \beta, \phi \in (0, 1)
\end{align}
\qquad(12.111)\]</span></span></p>
</section>
<section id="seasonal-component" class="level4" data-number="12.8.4.4">
<h4 data-number="12.8.4.4" class="anchored" data-anchor-id="seasonal-component"><span class="header-section-number">12.8.4.4</span> Seasonal component</h4>
<p>Starting with <a href="#eq-ets-AAN-ss" class="quarto-xref">Equation&nbsp;<span>12.110</span></a>, we now add a seasonal component, <span class="math inline">\(\mathscr{s}(u + t \; | \; s \le t)\)</span>, where a fixed positive integer <span class="math inline">\(\sigma\)</span> denotes the number of successive time indices comprising a single season. For <span class="math inline">\(t, (u + t) \in \tau_T\)</span> we fit <span class="math inline">\(\mathscr{s}(u + t \; | \; s \le t)\)</span> recursively. Then for <span class="math inline">\((u + t) &gt; t_f\)</span> we refer back to the most recent fitted value <span class="math inline">\(\mathscr{s} (\tilde{t})\)</span> such that <span class="math inline">\(\tilde{t} \equiv (u + t) \bmod \sigma\)</span>.</p>
<p><span id="eq-ets-AAA-ss"><span class="math display">\[
\begin{align}
  \breve{X}(u + t \; | \; s \le t) &amp;= \ell (t) \; + \; \mathit{b}(t) \times u \; + \; \mathscr{s}(u + t \; | \; s \le t) \\
  \ell (t) &amp;= \alpha \; (X(t) - \mathscr{s}(t - \sigma)) + (1 - \alpha) \; \left ( \ell(t - 1) + \mathit{b}(t - 1) \right ) \\
  \mathit{b} (t) &amp;= \beta \; \left ( \ell(t) - \ell(t - 1) \right ) \; + \; (1 - \beta) \; \mathit{b}(t - 1) \\
  \mathscr{s} (t) &amp;= \gamma \left ( X(t) - \ell (t-1) - \mathit{b} (t-1) \right ) \; + \; (1 - \gamma) \mathscr{s}(t - \sigma) \\
  \\
  &amp;\text{where } \alpha, \beta, \gamma \in (0, 1)\\
  \\
  &amp;\text{and where } \\
  &amp; \mathscr{s}(u + t \; | \; s \le t) = \mathscr{s} (\tilde{t}), \text{ with} \\
  &amp; \tilde{t} \equiv (u + t) \bmod \sigma, \text{ and with}  \\
  &amp; \tilde{t} \in (t - \sigma, \; t]
\end{align}
\qquad(12.112)\]</span></span></p>
</section>
<section id="ets-model-designation" class="level4" data-number="12.8.4.5">
<h4 data-number="12.8.4.5" class="anchored" data-anchor-id="ets-model-designation"><span class="header-section-number">12.8.4.5</span> ETS model designation</h4>
<p>Each ETS model can be categorized according to the status of model components level, trend, and season.<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a></p>
<p>The status of a component is one of the following:</p>
<ul>
<li><span class="math inline">\(\mathbf{A}\)</span>: used as an additive component</li>
<li><span class="math inline">\(\mathbf{Ad}\)</span>: damped or otherwise adjusted</li>
<li><span class="math inline">\(\mathbf{M}\)</span>: used as a multiplicative component</li>
<li><span class="math inline">\(\mathbf{N}\)</span>: not used</li>
</ul>
<p>Here are some examples of this scheme.</p>
<ul>
<li>ETS(A, N, N): simple exponential smoothing</li>
<li>ETS(A, A, N): Holt’s linear trend method</li>
<li>ETS(A, Ad, N): damped linear trend method</li>
<li>ETS(A, A, A): Holt-Winters with additive seasonality</li>
<li>ETS(A, A, M): Holt-Winters with multiplicative seasonality</li>
</ul>
<p>Note: <span class="citation" data-cites="hyndman2021forecasting">(<a href="references.html#ref-hyndman2021forecasting" role="doc-biblioref"><strong>hyndman2021forecasting?</strong></a>)</span> warn that certain combinations may lead to numerical instability when estimating parameters. For example they recommend using <span class="math inline">\(ETS(M, *, M)\)</span> rather than <span class="math inline">\(ETS(A, *, M)\)</span>. Much of this guidance is contained in the documentation (and curated options) for function <code>fable:ETS()</code>.</p>
</section>
</section>
<section id="seasonal-arima-models-txt-sarima-models" class="level3" data-number="12.8.5">
<h3 data-number="12.8.5" class="anchored" data-anchor-id="seasonal-arima-models-txt-sarima-models"><span class="header-section-number">12.8.5</span> Seasonal ARIMA Models <a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a></h3>
<p>Both societies and natural phenomena follow cycles determined by the earth’s daily rotation and annual revolution about the sun. Other cycles also arise naturally or through societal conventions. In modeling a random process one can account for these cycles in several ways. In this section we introduce methods and notation for incorporating seasonality into ARIMA models.</p>
<section id="pure-seasonal-arma-model" class="level4" data-number="12.8.5.1">
<h4 data-number="12.8.5.1" class="anchored" data-anchor-id="pure-seasonal-arma-model"><span class="header-section-number">12.8.5.1</span> Pure Seasonal ARMA model</h4>
<p>As in <a href="#sec-ETS" class="quarto-xref"><span>Section 12.8.4</span></a> we denote by <span class="math inline">\(\sigma &gt; 1\)</span> the number of consecutive time indices that comprise one season (or cycle). The phase of time index <span class="math inline">\(t\)</span> in this cycle is <span class="math inline">\(t \bmod \sigma\)</span>.</p>
<p><span class="math inline">\(ARMA(P, Q)_\sigma\)</span> (with capital letters and a <span class="math inline">\(\sigma\)</span> subscript) denotes the following form of pure seasonal ARMA model.</p>
<p><span id="eq-ARMA-pure-seasonal"><span class="math display">\[
\begin{align}
  \Phi ( \mathcal{B}^\sigma ) \; X(t) &amp;= \Theta ( \mathcal{B}^\sigma ) \; W(t) \\
  \\
  &amp; \text{where } \\
  \\
  \Phi (z) &amp;= 1 - \sum_{j = 1}^P \Phi_j z^j \\
  \Theta (z) &amp;= 1 + \sum_{k = 1}^Q \Theta_k z^k
\end{align}
\qquad(12.113)\]</span></span></p>
<p>ARMA polynomial requirements: As with a standard ARMA model, this pure seasonal model has a causal form (in which <span class="math inline">\(X(t)\)</span> is expressed in terms of current and past white noise terms <span class="math inline">\(W(\cdot)\)</span>) if and only if <span class="math inline">\(\Phi (\mathcal{B})^{-1}\)</span> exists. It has an inverted form (in which <span class="math inline">\(W(t)\)</span> is expressed in terms of current and past process terms <span class="math inline">\(X(\cdot)\)</span>) if and only if <span class="math inline">\(\Theta (\mathcal{B})^{-1}\)</span> exists. The existence of these multiplicative inverses is equivalent to the roots of the respective polynomials having magnitudes greater than unity, which we shall assume. To ensure the model is expressed as simply as possible, we also require that the two polynomials have no common factors, that is, have no roots in common.</p>
</section>
<section id="mixed-seasonal-arma-model" class="level4" data-number="12.8.5.2">
<h4 data-number="12.8.5.2" class="anchored" data-anchor-id="mixed-seasonal-arma-model"><span class="header-section-number">12.8.5.2</span> Mixed Seasonal ARMA Model</h4>
<p>We can combine the seasonal and nonseasonal operators into the following <em>multiplicative seasonal auto-regressive moving average model</em>, denoted by <span class="math inline">\(ARMA(p, q) \times (P, Q)_\sigma\)</span></p>
<p><span id="eq-ARMA-mixed-seasonal"><span class="math display">\[
\begin{align}
  \Phi ( \mathcal{B}^\sigma ) \; \phi ( \mathcal{B} ) \; X(t)
  &amp;= \Theta ( \mathcal{B}^\sigma ) \; \theta ( \mathcal{B} ) \; W(t) \\
  \\
  &amp; \text{where } \\
  \\
  \Phi (z) &amp;= 1 - \sum_{j = 1}^P \Phi_j z^j \\
  \Theta (z) &amp;= 1 + \sum_{k = 1}^Q \Theta_k z^k \\
  \\
  \phi (z) &amp;= 1 - \sum_{j = 1}^p \phi_j z^j \\
  \theta (z) &amp;= 1 + \sum_{k = 1}^q \theta_k z^k
\end{align}
\qquad(12.114)\]</span></span></p>
<p>Multiplicative seasonal ARMA polynomial requirements: We require that the seasonal and non-seasonal components separately meet the ARMA requirements described above. Now, the seasonal component acts on <span class="math inline">\(\mathcal{B}^\sigma\)</span>, with <span class="math inline">\(\sigma &gt; 1\)</span>, whereas the non-seasonal component acts on <span class="math inline">\(\mathcal{B}\)</span>. We want to ensure that the polynomial <span class="math inline">\(\Phi (z^\sigma) \times \phi (z)\)</span> has no roots in common with the polynomial <span class="math inline">\(\Theta (z^\sigma) \times \theta (z)\)</span>. Therefore the additional requirements here are that: (1) <span class="math inline">\(\{ \Phi (z^\sigma), \theta (z) \}\)</span> have no roots in common; and (2) <span class="math inline">\(\{ \Theta (z^\sigma), \phi (z) \}\)</span> also have no roots in common.</p>
</section>
<section id="sarima-model" class="level4" data-number="12.8.5.3">
<h4 data-number="12.8.5.3" class="anchored" data-anchor-id="sarima-model"><span class="header-section-number">12.8.5.3</span> SARIMA Model</h4>
<p>We now add seasonal and non-seasonal differencing to the previous model to define the following <em>multiplicative seasonal auto-regressive integrated moving average model</em> (also called SARIMA), denoted by <span class="math inline">\(ARIMA(p, d, q) \times (P, D, Q)_\sigma\)</span></p>
<p><span id="eq-SARIMA"><span class="math display">\[
\begin{align}
  \Phi ( \mathcal{B}^\sigma ) \; \phi ( \mathcal{B} ) \; \nabla_\sigma^D \; \nabla^d \; X(t)
  &amp;= \Theta ( \mathcal{B}^\sigma ) \; \theta ( \mathcal{B} ) \; W(t) \\
  \\
  &amp; \text{where } \\
  \\
  \Phi (z) &amp;= 1 - \sum_{j = 1}^P \Phi_j z^j \\
  \Theta (z) &amp;= 1 + \sum_{k = 1}^Q \Theta_k z^k \\
  \nabla_\sigma^D &amp;= (\mathcal{I} - \mathcal{B}^\sigma)^D \\
  \\
  \phi (z) &amp;= 1 - \sum_{j = 1}^p \phi_j z^j \\
  \theta (z) &amp;= 1 + \sum_{k = 1}^q \theta_k z^k \\
  \nabla^d &amp;= (\mathcal{I} - \mathcal{B})^d
\end{align}
\qquad(12.115)\]</span></span></p>
<p>The polynomial requirements remain unchanged from the seasonal ARMA case described above.</p>
</section>
</section>
<section id="co2-example-txt-cardox-xmpl" class="level3" data-number="12.8.6">
<h3 data-number="12.8.6" class="anchored" data-anchor-id="co2-example-txt-cardox-xmpl"><span class="header-section-number">12.8.6</span> CO2 Example <a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a></h3>
<p><a href="#fig-cardox" class="quarto-xref">Figure&nbsp;<span>12.15</span></a> shows the atmospheric concentration of carbon dioxide <span class="math inline">\((CO_2)\)</span> in parts per million (ppm). These are monthly averages from March 1958 to March 2023 provided by the Mauna Loa Observatory. The data are available in <code>R</code> as the module <code>asta::cardox</code>.</p>
<p><a href="#fig-cardox-per-month-spaghetti" class="quarto-xref">Figure&nbsp;<span>12.6</span></a> distinguishes the trend from the seasonality of the same data.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cardox-per-month-spaghetti" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cardox-per-month-spaghetti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-cardox-per-month-spaghetti-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cardox-per-month-spaghetti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.6: CO2 per month within each year
</figcaption>
</figure>
</div>
</div>
</div>
<p>To build a SARIMA model of the data we first remove (or at least reduce) trend and seasonality by applying differencing operators <span class="math inline">\(\nabla\)</span> and <span class="math inline">\(\nabla_{12}\)</span>. <a href="#fig-cardox-diff-diff-12" class="quarto-xref">Figure&nbsp;<span>12.7</span></a> shows the residuals, <span class="math inline">\(R(t) = \nabla \nabla_{12} X(t)\)</span>, from this double differencing.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cardox-diff-diff-12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cardox-diff-diff-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-cardox-diff-diff-12-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cardox-diff-diff-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.7: Residuals from applying trend and seasonal differencing
</figcaption>
</figure>
</div>
</div>
</div>
<p>Using <code>R</code> function <code>astsa::sarima()</code> the authors arrive at an <span class="math inline">\(ARMA(1, 1) \times ARMA(0, 1)_{12}\)</span> model of the residuals <span class="math inline">\(R(t)\)</span>. For the process <span class="math inline">\(X(t)\)</span> this is an <span class="math inline">\(ARIMA(1, 1, 1) \times ARIMA(0, 1, 1)_{12}\)</span> model. A five-year forecast is shown below, using <code>R</code> function <code>astsa::sarima.for()</code>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cardox-forecast" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cardox-forecast-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-cardox-forecast-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cardox-forecast-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.8: CO2 5-year forecast based on ARIMA(1, 1, 1) * (0, 1, 1)_12
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="closing-remarks" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="closing-remarks"><span class="header-section-number">12.9</span> Closing Remarks</h2>
<p>This technical note introduces families and examples of models used to forecast time series data. The presentation is based on <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span> along with the supporting <code>R</code> package <code>astsa</code> <span class="citation" data-cites="astsa">(<a href="references.html#ref-astsa" role="doc-biblioref"><strong>astsa?</strong></a>)</span>.<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a></p>
<p>This introduction sets aside for another time the building and evaluation of time series models. Here are some online resources for those interested.</p>
<ul>
<li><a href="https://dsstoffer.github.io/Rtoot.html#regression-and-time-series-primer">Regression and Time Series Primer</a> gives a more detailed introduction to the topics presented here.</li>
<li><a href="https://github.com/nickpoison/astsa/tree/master/fun_with_astsa">Fun with asta</a> introduces <code>R</code> package <code>astsa</code>.</li>
<li><a href="https://otexts.com/fpp3/">Forecasting: Principles and Practice (3e)</a> is another highly regarded textbook available online.</li>
<li><a href="https://www.stat.berkeley.edu/~ryantibs/timeseries-f23/">Berkeley Intro to Time Series</a> contains excellent lecture notes.</li>
<li><a href="https://cran.r-project.org/web/views/TimeSeries.html">CRAN</a>, from the Comprehensive R Archive Network, describes time series packages in <code>R</code>.</li>
</ul>
</section>
<section id="a-glossary-of-math-symbols" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="a-glossary-of-math-symbols"><span class="header-section-number">12.10</span> A: Glossary of Math Symbols</h2>
<p><span class="math display">\[
\begin{align}
  {a_{\bullet, \bullet} (u)}
    \quad &amp; \quad \text{matrix of filter coefficients at time-shift } u \\
  \\
  \mathcal{B}
     \quad &amp; \quad \text{back-shift operator } \\
  \\
  \nabla
     \quad &amp; \quad \text{difference operator } \mathcal{I - B} \\
  \\
  \nabla_\sigma
     \quad &amp; \quad \text{seasonal difference operator } \mathcal{I - B}^\sigma \\
  \\
  \Delta_u
     \quad &amp; \quad \{ 1, 2, \ldots, u-1 \} \text{ for } u \ge 2 \\
  \\
  \phi(\cdot)
     \quad &amp; \quad \text{AR polynomial, } 1 - \sum_{\nu = 1}^p \phi_\nu \; z^\nu \\
  \\
  \Gamma
     \quad &amp; \quad \text{auto-covariance matrix, } \Gamma [j, \nu] = \gamma_X (\nu - j) \text{, for } j, \nu \in \nu_T \\
  \\
  \gamma_{\bullet, \bullet} (u)
     \quad &amp; \quad \text{auto-covariance matrix at time-shift } u \\
  \\
  \mathcal{I}
     \quad &amp; \quad \text{identity operator } \\
  \\
  MSE
     \quad &amp; \quad \text{mean squared error } \\
  \\
  m_\bullet (t)
     \quad &amp; \quad E \{ X_\bullet (t) \} \text{, expected value of } X_\bullet (t) \\
  \\
  \mu_X
     \quad &amp; \quad E\{ X (t_0) \} \text{, expected value of } X (t_0) \\
  \\   
  \hat{\mu}_X
     \quad &amp; \quad \text{sample mean, } S_{X, T} / T \\
  \\   
  \mathcal{N}(\cdot)
     \quad &amp; \quad \text{a univariate, stationary, Gaussian process having mean 0} \\
  \\   
  \tilde{\mathcal{N}} (u + t_f \; | \; \wp (t_f))
     \quad &amp; \quad E \left \{ \mathcal{N} (u + t_f) \; | \; \mathcal{N} (s), \; s \le t_f \right \} \\
  \\
  \nu_T
     \quad &amp; \quad \{ 0, 1, \dots, T-1 \} \\
  \\
  \wp (t_f)
     \quad &amp; \quad \{ s \in \mathbb{Z} \; | \; s \le t_f \} \\
  \\
  \rho_{\bullet, \bullet} (u)
     \quad &amp; \quad \text{autocorrelation matrix at time-shift } u \\
  \\
  \rho (u | u)
     \quad &amp; \quad corr(X(t+u) - \hat{X}(t+u \; | \; - \Delta_u), \; X(t) - \hat{X}(t \; | \; \Delta_u)) \\
  \\
  S_{X, T}
     \quad &amp; \quad \text{the sample sum of T observations of } X (\cdot) \\
  \\
  \sigma_X^2
     \quad &amp; \quad Var \{ X (t_0) \} \text{, variance of } X (t_0) \\
  \\
  T
     \quad &amp; \quad \text{number of time series observations } \\
  \\
  t_0
     \quad &amp; \quad \text{initial time index of time series observations } \\
  \\
  t_f
     \quad &amp; \quad \text{final time index of time series observations } \\
  \\
  \tau_T
     \quad &amp; \quad t_0 + \{ 0, 1, \ldots, T-1  \} \\
  \\
  \theta(\cdot)
     \quad &amp; \quad \text{MA polynomial, } 1 + \sum_{\nu = 1}^q \theta_\nu \; z^\nu \\
  \\
  W(\cdot)
     \quad &amp; \quad \text{white noise process, } W(\cdot) \sim wn(0, \sigma_W^2)  \\
  \\
  X_\bullet (t)
     \quad &amp; \quad (X_1 (t), \ldots, X_d (t)) \text{, a multivariate random process } \\
  \\
  \hat{X}(t \; | \; \Delta_u)
     \quad &amp; \quad \text{approximates } X(t) \text{ as affine function of } \{ X(t + \nu) \}_{\nu \in \Delta_u} \\
  \\
  \hat{X} (u + t_f \; | \; \wp (t_f))
     \quad &amp; \quad \text{predicts } X(u + t_f) \text{ as affine function of } \{ X(t) \}_{t \in \wp (t_f)} \\
  \\
  \breve{X} (t \; | \; \tau_T)
     \quad &amp; \quad \text{trucnated predictor of } X(t) \text{ based on } \{ X(t) \}_{t \in \tau_T}
\end{align}
\]</span></p>
</section>
<section id="b-data-examples" class="level2" data-number="12.11">
<h2 data-number="12.11" class="anchored" data-anchor-id="b-data-examples"><span class="header-section-number">12.11</span> B: Data Examples</h2>
<section id="hare-and-lynx-populations" class="level3" data-number="12.11.1">
<h3 data-number="12.11.1" class="anchored" data-anchor-id="hare-and-lynx-populations"><span class="header-section-number">12.11.1</span> Hare and Lynx Populations</h3>
<p>One of the classic studies of predator–prey interactions is based on the record of lynx (<code>astsa::Lynx</code>) and snowshoe hare (<code>astsa::Hare</code>) pelts purchased by the Hudson’s Bay Company of Canada from 1845 to 1935. Assuming pelt purchases are proportional to animals in the wild, the data are an indirect measure of predation.</p>
<p>These predator–prey interactions often lead to cyclical patterns of predator and prey abundance. The units of the data shown in <a href="#fig-pred-prey" class="quarto-xref">Figure&nbsp;<span>12.9</span></a> are thousands of pelts per year.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-pred-prey" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pred-prey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-pred-prey-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pred-prey-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.9: Hare and lynx: purchased pelts
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="jj-quarterly-earnings" class="level3" data-number="12.11.2">
<h3 data-number="12.11.2" class="anchored" data-anchor-id="jj-quarterly-earnings"><span class="header-section-number">12.11.2</span> JJ Quarterly Earnings</h3>
<p><a href="#fig-jj" class="quarto-xref">Figure&nbsp;<span>12.10</span></a> below shows Johnson &amp; Johnson quarterly earnings per share in US dollars from 1960 through 1980 (<code>astsa::jj</code>). The bottom panel shows the same data on a <span class="math inline">\(\log_e\)</span> scale. Superimposed on the upward trend is an annual pattern, including a sharp rise to first quarter earnings from those of the previous quarter.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-jj" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-jj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-jj-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-jj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.10: J&amp;J: earnings per share at linear and log scales
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="global-temperatures" class="level3" data-number="12.11.3">
<h3 data-number="12.11.3" class="anchored" data-anchor-id="global-temperatures"><span class="header-section-number">12.11.3</span> Global Temperatures</h3>
<p><a href="#fig-gtemp-lo" class="quarto-xref">Figure&nbsp;<span>12.11</span></a> below shows the time series <code>gtemp_land</code> and <code>gtemp_ocean</code> from <code>R</code> package <code>astsa</code>. The series are annual temperature deviations (in ◦C) from averages for the period 1991-2020. The temperatures are based on averages over the Earth’s land area and over the part of the ocean that is free of ice at all times (open ocean). The time period is from 1850 to 2023. Note that the trend is not linear, with periods of leveling off followed by sharp increases.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-gtemp-lo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gtemp-lo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-gtemp-lo-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gtemp-lo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.11: Deviations in global surface temperatures
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="dow-jones-industrial-average" class="level3" data-number="12.11.4">
<h3 data-number="12.11.4" class="anchored" data-anchor-id="dow-jones-industrial-average"><span class="header-section-number">12.11.4</span> Dow Jones Industrial Average</h3>
<p><a href="#fig-djia" class="quarto-xref">Figure&nbsp;<span>12.12</span></a> shows the trading day closings and returns (percent change)<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a> of the Dow Jones Industrial Average (DJIA, <code>astsa::djia</code>) from 2006 to 2016. It is easy to spot the financial crisis of 2008.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-djia" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-djia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-djia-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-djia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.12: Dow Jones Industrial Average
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="cardiovascular-mortality-in-los-angeles" class="level3" data-number="12.11.5">
<h3 data-number="12.11.5" class="anchored" data-anchor-id="cardiovascular-mortality-in-los-angeles"><span class="header-section-number">12.11.5</span> Cardiovascular Mortality in Los Angeles</h3>
<p><a href="#fig-cmort" class="quarto-xref">Figure&nbsp;<span>12.13</span></a> shows data from a study <span class="citation" data-cites="shumway1988">(<a href="references.html#ref-shumway1988" role="doc-biblioref"><strong>shumway1988?</strong></a>)</span> of the possible effects of temperature and pollution on weekly cardiovascular mortality in Los Angeles County. Note the strong seasonal components in all of the series corresponding to winter–summer variations and the downward trend in the cardiovascular mortality over the 10-year period. The data are available as <code>R</code> module <code>astsa::cmort</code>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cmort" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cmort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-cmort-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cmort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.13: Weekly levels of mortality, temperature, and particulates in Los Angeles
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="el-niño-and-fish-population" class="level3" data-number="12.11.6">
<h3 data-number="12.11.6" class="anchored" data-anchor-id="el-niño-and-fish-population"><span class="header-section-number">12.11.6</span> El Niño and Fish Population</h3>
<p><a href="#fig-soi" class="quarto-xref">Figure&nbsp;<span>12.14</span></a> shows the Southern Oscillation Index (SOI, <code>astsa::soi</code>) and associated Recruitment (<code>astsa::rec</code>), an index of the number of young fish entering the cohort available for commercial fishing. Both series consist of 453 monthly values ranging over the years 1950–1987.</p>
<p>The two time series show two types of oscillation: an annual cycle (warm in the summer, cool in the winter), and a slower cycle that seems to repeat about every 4 years.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-soi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-soi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-soi-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-soi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.14: Southern Oscillation Index and fish recruitment
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="co2-mauna-loa-observatory" class="level3" data-number="12.11.7">
<h3 data-number="12.11.7" class="anchored" data-anchor-id="co2-mauna-loa-observatory"><span class="header-section-number">12.11.7</span> CO2, Mauna Loa Observatory</h3>
<p>The concentration of <span class="math inline">\(CO_2\)</span> in the atmosphere is a key indicator of global warming, and in recent years has reached unprecedented levels. In March 2015, the average of all of the global measuring sites showed a concentration above 400 parts per million (ppm). The observatory at Mauna Loa has recorded monthly <span class="math inline">\(CO_2\)</span> concentrations consistently above 400 ppm since late 2015.</p>
<p><a href="#fig-cardox" class="quarto-xref">Figure&nbsp;<span>12.15</span></a> shows the monthly Mauna Loa readings, <span class="math inline">\(X(t)\)</span> (<code>asta::cardox</code>), from March 1958 to March 2023 at the Mauna Loa Observatory. The trend and seasonal persistence are evident in the plot.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cardox" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cardox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ts-forecast_files/figure-html/fig-cardox-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cardox-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12.15: Monthly mean carbon dioxide (in ppm) measured at Mauna Loa Observatory, Hawaii.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="references" class="level2" data-number="12.12">
<h2 data-number="12.12" class="anchored" data-anchor-id="references"><span class="header-section-number">12.12</span> References</h2>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>We assume the components of <span class="math inline">\(X_\bullet(t)\)</span> to be real-valued, unless stated otherwise.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>For a univariate time series <span class="math inline">\(X(t)\)</span> we denote distributional parameters either omitting a subscript or else using the subscript <span class="math inline">\(X\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The assumption of second-order (or wide-sense) stationarity suffices for most time series models used in practice, but some cases may call for the assumption of <em>strict stationarity</em>. This means that for any finite set of times <span class="math inline">\((t_1, \ldots, t_K)\)</span> and any time-shift <span class="math inline">\(s\)</span>, the joint probability distribution of <span class="math inline">\((X_\bullet (t_1), \ldots, X_\bullet (t_K))\)</span> is identical to that of <span class="math inline">\((X_\bullet (t_1 - s), \ldots, X_\bullet (t_K - s))\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>That is, we assume that <span class="math inline">\(\mathcal{N}(t)\)</span> is a stationary, mean-zero, autocorrelated process such that for any set of distinct time indices <span class="math inline">\(t_1, \ldots, t_K\)</span> the joint probability distribtuion of <span class="math inline">\((\mathcal{N}(t_1), \ldots, \mathcal{N}(t_K)\)</span> is multivariate normal.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Normality allows us to define approximations as conditional expectations. To define PACF more generally, replace conditional expectation with the affine function that minimizes mean squared approximation error.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Recall that for a univariate process the auto-covariance and autocorrelation functions are even, e.g., <span class="math inline">\(\rho (-u) = \rho (u)\)</span>. The PACF of a univariate process is also defined to be an even function.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>The filters described here are called “auto-regressive” or “finite impulse response”. Filtering is a large subject only touched on here.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>The authors consider various forms for the function <span class="math inline">\(f(t)\)</span>: a linear function, a quadratic function, or a locally smoothed version of <span class="math inline">\(X_1(t)\)</span>.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>An ARMA model is said to be <em>causal</em> if the modeled process <span class="math inline">\(X(t)\)</span> can be expressed as a function of current and past white noise terms, <span class="math inline">\(\{ W(t-u) \}\)</span> for <span class="math inline">\(u \ge 0\)</span>. See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>A model is said to be <em>invertible</em> if the model’s white noise process <span class="math inline">\(W(t)\)</span> can be expressed as a function of current and past terms of the modeled process, <span class="math inline">\(\{ X(t-u) \}\)</span> for <span class="math inline">\(u \ge 0\)</span>. See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>Requiring the roots of a polynomial <span class="math inline">\(p(\cdot)\)</span> to be greater than unity in magnitude ensures that <span class="math inline">\(p(z)^{-1}\)</span> is finite and analytic for <span class="math inline">\(|z| \le 1\)</span> and thus has a power series expansion whose radius of convergence is the minimum magnitude of the roots of <span class="math inline">\(p(\cdot)\)</span>. As a consequence the coefficients of that power series are absolutely summable.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>Transform the second equation into the first as follows. For all time indices <span class="math inline">\(\nu\)</span> set <span class="math inline">\(U(\nu) = 5 V(\nu - 1)\)</span>, and then reverse time by setting <span class="math inline">\(Y'(\nu) = Y(- \nu)\)</span> and <span class="math inline">\(U'(\nu) = U(- \nu)\)</span>.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>The rational function <span class="math inline">\(\theta(z) \; \div \phi(z)\)</span> can be expressed as a power series, namely the power series of <span class="math inline">\(\phi(z)^{-1}\)</span> multiplied by the polynomial <span class="math inline">\(\theta(z)\)</span>.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>This inversion requires each root of polynomial <span class="math inline">\(\theta(\cdot)\)</span> to have magnitude greater than unity.<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p><span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span> introduce the <span class="math inline">\(\Gamma\)</span> matrix.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>By “degenerate process” we mean a process <span class="math inline">\(X(\cdot)\)</span> for which there is a filter <span class="math inline">\(a(u)\)</span>, equal to zero for all but finitely many lags <span class="math inline">\(u\)</span>, that annihilates <span class="math inline">\(X(\cdot)\)</span>: <span class="math inline">\(a * (X - \mu_x) = 0\)</span>.<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>Any two time indices within the <span class="math inline">\(T\)</span> observations can differ by no more than <span class="math inline">\(T-1\)</span>. Yet the needed estimates include <span class="math inline">\(\gamma_X(T + u - 1 - j)\)</span>, with <span class="math inline">\(u \ge 1\)</span> and <span class="math inline">\(0 \le j &lt; T\)</span>. For <span class="math inline">\(u = 1\)</span> and <span class="math inline">\(j = 0\)</span> we have <span class="math inline">\(\gamma_X(T)\)</span>, which exceeds the limits of direct estimation.<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p><span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span><a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>When attempting to predict future values <span class="math inline">\(\mathcal{N} (u + t_f)\)</span> from data values indexed by <span class="math inline">\(t \in (t_0, \ldots, t_f)\)</span> the index symbol <span class="math inline">\(h\)</span> is often used in place of the generic indexing difference <span class="math inline">\(u\)</span> to denote forecast <em>horizon</em>.<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p>Recall that the formula given here for the mean squared error of prediction assumes that the coefficients of the model and the parameters of the process are known. In applications these quantities must be estimated, which increases mean squared prediction error.<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p><span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span><a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38"><p>The formula is obtained by expanding <span class="math inline">\(\frac{1 + \theta_1 z}{1 - \phi_1 z}\)</span> in a power series.<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39"><p>The simulation code is based on a similar example in <a href="https://github.com/nickpoison/astsa/tree/master/fun_with_astsa">Fun with asta</a>.<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41"><p>From covariance calculations, <span class="math inline">\(\nabla \; X(\cdot)\)</span> is an <span class="math inline">\(AR(1)\)</span> process.<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43"><p><span class="math inline">\(\hat{X}(t \; | \; \wp (t-1))\)</span> is the orthogonal projection of <span class="math inline">\(X(t)\)</span> onto the space spanned by <span class="math inline">\(\{ X(s), s &lt; t \}\)</span>. See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44"><p>In the case <span class="math inline">\(u = 1\)</span> the sum is over an empty set of indices <span class="math inline">\(\nu\)</span> such that <span class="math inline">\(\nu \ge 1\)</span> and <span class="math inline">\(\nu \le 0\)</span>. We define such empty sums to be zero.<a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45"><p>See <span class="citation" data-cites="hyndman2021forecasting">(<a href="references.html#ref-hyndman2021forecasting" role="doc-biblioref"><strong>hyndman2021forecasting?</strong></a>)</span> [chapter 8] and <span class="citation" data-cites="ryantibs2023timeseries">(<a href="references.html#ref-ryantibs2023timeseries" role="doc-biblioref"><strong>ryantibs2023timeseries?</strong></a>)</span>.<a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46"><p>See <span class="citation" data-cites="holt1957forecasting">(<a href="references.html#ref-holt1957forecasting" role="doc-biblioref"><strong>holt1957forecasting?</strong></a>)</span> and <span class="citation" data-cites="winters1960forecasting">(<a href="references.html#ref-winters1960forecasting" role="doc-biblioref"><strong>winters1960forecasting?</strong></a>)</span>.<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47"><p><a href="#eq-ets-ANN-ss-wn" class="quarto-xref">Equation&nbsp;<span>12.109</span></a> is the state-space version of <a href="#eq-SES-plus-wn" class="quarto-xref">Equation&nbsp;<span>12.100</span></a>.<a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48"><p>See <span class="citation" data-cites="hyndman2002state">(<a href="references.html#ref-hyndman2002state" role="doc-biblioref"><strong>hyndman2002state?</strong></a>)</span>.<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn50"><p>See <span class="citation" data-cites="shumway2025">(<a href="references.html#ref-shumway2025" role="doc-biblioref"><strong>shumway2025?</strong></a>)</span>.<a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn51"><p>Another textbook, <span class="citation" data-cites="brillinger2001">(<a href="references.html#ref-brillinger2001" role="doc-biblioref"><strong>brillinger2001?</strong></a>)</span>, treats a time series as a function of time, denoted <span class="math inline">\(X(t)\)</span>, which is the practice followed in this note.<a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn52"><p>The return is here calculated as <span class="math inline">\(log_e\)</span> of the ratio: current day’s closing price divided by that of the preceding day. The median value is about 6 basis points.<a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/github\.com\/tthrall\/eda4ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ts-intro.html" class="pagination-link" aria-label="Time Series Data Analysis">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Time Series Data Analysis</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ts-fourier.html" class="pagination-link" aria-label="Time Series Spectrum Analysis">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Time Series Spectrum Analysis</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/tthrall/eda4ml/edit/main/ts-forecast.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/tthrall/eda4ml/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>