# Linear Algebra for Fitting Models to Data {#sec-la-intro}

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(
  echo    = FALSE, 
  error   = FALSE, 
  message = FALSE, 
  warning = FALSE
)
```

```{r}
#| label: CRAN-libraries

library(assertthat)
library(dslabs)
library(GGally)
library(ggimg)
library(HistData)
library(here)
library(keras)
library(latex2exp)
library(OECD)
library(plotly)
library(rgl)
library(R.utils)
library(reticulate)
library(scatterplot3d)
library(tensorflow)
library(tidyverse)
library(tinytex)
library(UsingR)
```

```{r}
#| label: local-source

source(here("code", "mnist_file_mgmt.R"))
source(here("code", "oecd_bli.R"))
source(here("code", "z_score.R"))
```

------------------------------------------------------------------------

Vectors and matrices are the central objects of linear algebra.  And central to data science and machine learning is the notion of a _data matrix_, in which each row is composed of different types of values that represent a single case of data.  For example, a single row of a data matrix might represent the recorded characteristics of an individual participant, item, or unit in some study.  In contrast, each column (known as a _feature vector_ or _data variable_) represents multiple instances of just one of these prescribed types of value. [^data-variable]  Let's consider some examples.

[^data-variable]: To be more precise, every _data variable_ qualifies as a _feature vector_, but a feature vector may also be some function of the data and other information.

## Data Examples

### Heights of Parents and Oldest Child

```{r}
#| label: galton_3d

data(HistData::GaltonFamilies)
galton_3d <- GaltonFamilies |> 
  tibble::as_tibble() |> 
  dplyr::group_by(family) |> 
  dplyr::filter(childNum == 1) |>  # Oldest child
  dplyr::select(father, mother, childHeight, gender) |> 
  dplyr::rename(child = childHeight) |> 
  dplyr::ungroup()

# cite the following values in the narrative
n_children_all      <- nrow(GaltonFamilies)
n_families          <- nrow(galton_3d)
n_child_1_daughters <- sum(galton_3d$ gender == "female")
n_child_1_sons      <- sum(galton_3d$ gender == "male")
```

In 1885 Sir Francis Galton examined the heights (in inches) of parents and their adult children to determine the strength of evidence to support height as a hereditary trait.  The corresponding `R` data set `HistData::GaltonFamilies` consists of `r n_children_all` adult children from a total of `r n_families` families.  Restricting attention to the oldest child in each family, there were `r n_child_1_daughters` daughters and `r n_child_1_sons` sons.

The table below shows a portion of this data matrix.  Each row represents a family and consists of: a family identifier, the father's height, the mother's height, the oldest child's height, and the oldest child's gender.

```{r}
#| label: tbl-galton-3d
#| tbl-cap: "Family heights in inches: father, mother, oldest child"

galton_3d |> 
  dplyr::slice_head(n = 6) |> 
  knitr::kable(
  caption = "Family heights: father, mother, oldest child"
)
```

The figure below represents all the families, with the gender of the oldest child distinguished by color: red for daughters and blue for sons.

```{r}
#| label: fig-mfc-scat3d
#| fig-cap: "Height of oldest child: daughters (red), sons (blue)"

g_mfc_s3d <- scatterplot3d::scatterplot3d(
  x = galton_3d$ mother, xlab = "mother", 
  y = galton_3d$ father, ylab = "father", 
  z = galton_3d$ child,  zlab = "child", 
  pch = 16, 
  color = dplyr::if_else(
    galton_3d$ gender == "male", "steelblue", "red"), 
  main = "Family heights: mother, father, child", 
  sub  = "daughters in red, sons in blue"
)
```

In @sec-conditioning we regressed the son's height on the father's height.  We obtained the regression line, which approximates the graph of averages: the average son's height per father's height.  The linear regression can be interpreted as a linear prediction of the height of a son whose father is of some given height.

We can now expand on this idea by regressing the son's height on the heights of both the mother and the father.  This is a model in which the predicted son's height, $\hat{s}$, is some constant plus some linear combination of the parents' heights.

$$
\begin{align}   
  \hat{s} & = \mathcal{l}_{R}(m, f) \\   
  &= \beta_0 \; + \; \beta_m \times m \; + \; \beta_f \times f 
\end{align} 
$$ {#eq-s-per-mf-regression-plane}

where 

$$
\begin{align} 
  \hat{s} &= \text{predicted height of son} \\ 
  m &= \text{height of mother}  \\ 
  f &= \text{height of father} 
\end{align} 
$$ {#eq-s-per-mf-2}

Each set of coefficient values determines some plane in the 3-dimensional space of (mother, father, son) heights.  The coefficients $(\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f)$ obtained by linear regression determine the _regression plane_ (@fig-mfs-scat3d) that gives the best linear approximation $(\hat{s})$ to the son's height for a given pair of parent heights $(m, f)$. [^least-squares-approximation]

[^least-squares-approximation]: "Best" in the sense of minimizing the squared residuals of actual minus predicted sons' heights.

```{r}
#| label: s-mf-lm

s_mf_lm <- lm(
  data    = galton_3d |> dplyr::filter(gender == "male"), 
  formula = child ~ mother + father)
```

```{r}
#| label: s-mf-tbl

# append fitted values (predictions)
s_mf_tbl <- galton_3d |> 
  dplyr::filter(gender == "male") |> 
  dplyr::rename(son = child) |> 
  dplyr::mutate(s_hat = s_mf_lm$ fitted.values)
```

```{r}
#| label: fig-mfs-scat3d
#| fig-cap: "Son's height given (mother, father) heights: predicted (plane) and observed (point)"

g_mfs_pt <- scatterplot3d(
  x = s_mf_tbl$ mother, xlab = "mother", 
  y = s_mf_tbl$ father, ylab = "father", 
  z = s_mf_tbl$ son,    zlab = "son", 
  pch = 16, 
  color = "steelblue",
  angle = 55, 
  main = "Family heights: mother, father, son", 
  sub  = "The regression plane contains the predicted heights of sons.")

# Add the regression plane ("g_mfs_plane")
s_mf_lm |> g_mfs_pt$ plane3d(
  lty          = "solid",
  lty.box      = "solid",
  draw_polygon = TRUE,
  col          = "grey")
```

In vector-matrix notation we are seeking a vector $(\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f)$ of coefficient values that yields the least-squares solution to the following linear approximation problem.

$$
\begin{align} 
  s_\bullet &\approx (1_\bullet, m_\bullet, f_\bullet) \times 
    \begin{pmatrix} 
      \hat{\beta}_0 \\ \hat{\beta}_m \\ \hat{\beta}_f
    \end{pmatrix} 
\end{align} 
$$ {#eq-s-per-mf-3}

where 

$$
\begin{align} 
  s_\bullet &= \text{data column vector: heights of sons} \\ 
  1_\bullet &= \text{column vector } (1, \ldots, 1) \\ 
  m_\bullet &= \text{data column vector: heights of mothers}  \\ 
  f_\bullet &= \text{data column vector: heights of fathers} 
\end{align} 
$$ {#eq-s-per-mf-4}

This is a statistical estimation problem that corresponds to the following linear algebra problem and notation.

$$
\begin{align} 
  b_\bullet &\approx A_{\bullet, \bullet} \times x_\bullet
\end{align} 
$$ {#eq-b-Ax}

where 

$$
\begin{align} 
  b_\bullet &= s_\bullet \\ 
  A_{\bullet, \bullet} &= (1_\bullet, m_\bullet, f_\bullet) \\ 
  x_\bullet &= (\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f) 
\end{align} 
$$ {#eq-b-Ax-2}

It turns out that the least squares solution $(\hat{\beta}_0, \hat{\beta}_m, \hat{\beta}_f)$ can be obtained as the vector of coefficients of an orthogonal projection of vector $s_\bullet$ onto the 3-dimensional subspace spanned by vectors $(1_\bullet, m_\bullet, f_\bullet)$.  More on this later.

### Survey Data: Better Life Index

```{r}
#| label: bli-long
bli_long <- get_bli_long()
```

```{r}
#| label: bli-wide

bli_wide <- assert_bli_wide(bli_long)

# record constants to be used in narrative
n_bli_loc   <- nrow(bli_wide)
n_bli_comps <- ncol(bli_wide) - 2L
```

We now turn to a data set having several data columns, namely the OECD's Better Life Index (BLI). [^OECD-about] The following table shows a portion of the data.

[^OECD-about]: The OECD (Organisation for Economic Co-operation and Development) works with 100+ countries to collect and analyze data in order to promote public policy.  The OECD's 38 Member countries span the world, from North America and South America to Europe and Asia-Pacific.

```{r}
#| label: tbl-bli-wide
#| tbl-cap: "Better Life Index (BLI)"

bli_wide |> print(n = 6)
```

Each row of this data matrix gives specified measurements of an identified country.  The first two columns give, respectively, each country's OECD code and name.  The remaining `r n_bli_comps` columns are measures pertaining to the well-being of the populace.

The column name of each measures consists of a two-letter prefix followed by a suffix.  The prefix is associated with a broad indicator of social well-being.  The suffix pertains to a particular component of this indicator.  Here is an expansion of these prefixes.

```{r}
#| label: bli-comp-prefix
bli_comp_prefix <- assert_bli_comp_prefix()
```

```{r}
#| label: tbl-bli-comp-prefix
#| tbl-cap: "BLI Indicators and Sub-Components"

bli_comp_prefix |> 
  dplyr::rename(components = comps) |> 
  knitr::kable(
    caption = "BLI Indicators and Sub-Components")
```

The component indicators (corresponding to the suffix of the column name) are elaborated in the following table.

```{r}
#| label: bli-components
bli_components <- assert_bli_component_indicators()
```

```{r}
#| label: tbl-bli-components
#| tbl-cap: "BLI Component Indicators"

bli_components |> 
  dplyr::select(prefix, suffix, unit, post_name, dscr) |> 
  dplyr::rename(
    name = post_name, 
    description = dscr
  ) |> 
  knitr::kable(
    caption = "BLI Component Indicators")
```

The `unit` column in the above table gives the unit of measure, with `PC` meaning percent, `YR` meaning number of years, and so on.

We now turn to a statistical and algebraic treatment of the BLI data matrix of @tbl-bli-wide.  Consider the indicator component `SW_LIFS` (Life Satisfaction) as a response variable, with the remaining `r n_bli_comps -1L` indicator components serving as explanatory variables.  As with the previous data example, we want to approximate or predict the response variable by a constant $\beta_0$ plus a linear combination of the explantory variables, as follows.

$$
\begin{align} 
  L_\bullet &\approx (1_\bullet, C_{1, \bullet}, \ldots, C_{d, \bullet}) \times 
    \begin{pmatrix} 
      \hat{\beta}_0 \\ \hat{\beta}_1 \\ \vdots \\ \hat{\beta}_d
    \end{pmatrix} 
\end{align} 
$$ {#eq-L-per-C}

where 

$$
\begin{align} 
  L_\bullet &= \text{life satisfaction indicator per country} \\ 
  1_\bullet &= \text{column vector } (1, \ldots, 1) \\ 
  C_{k, \bullet} &= k^{th} \text{ indicator component per country} \\ 
  d &= \text{number of explanatory indicators}
\end{align} 
$$ {#eq-L-per-C-2}

We now have more explanatory variables than in the previous example, a fact that merits some comment.

On the one hand, the approach to determining least-squares regression coefficients $\hat{\beta}_0, \ldots, \hat{\beta}_d$ is unchanged.  We project the response vector, now $L_\bullet$, onto the space spanned by the constant vector $1_\bullet$ along with the explanatory variables, that is onto the space spanned by $(1_\bullet, C_{1, \bullet}, \ldots, C_{d, \bullet})$.  The fitted coefficients yield a function of the explanatory variables that forms a regression hyperplane of dimension `r n_bli_comps - 1L` that passes through a cloud of data points, $(C_{1, \bullet}, \ldots, C_{d, \bullet}, L_\bullet)$, in a space of dimension `r n_bli_comps`.

On the other hand, we are now estimating `r n_bli_comps` regression coefficients based on observations from just `r n_bli_loc` countries.  From a statistical perspective, this paucity of observations relative to the number of estimates leads to large standard errors for the set of estimated coefficients.  From the perspective of numerical linear algebra, the vector of fitted coefficients $(\hat{\beta}_0, \ldots, \hat{\beta}_d)$ is less stable (more sensitive to error in the data) than it was in the previous example.

### MNIST: Images of Handwritten Digits

The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten decimal digits consisting of 60,000 training images and 10,000 testing images. [^MNIST-refs]

[^MNIST-refs]: See @LeCun_Cortes_Burges_2005 and @wiki_MNIST.

The history of this database goes back to 1988, when the US Postal Service constructed images of digits appearing on handwritten zip codes.  Around the same time the US Census Bureau requested NIST to evaluate optical character recognition (OCR) systems.  In 1992, NIST and the Census Bureau sponsored a competition in which participating teams were given images of Handwriting Sample Forms (HSFs), including handwritten decimal digits.  The initial version of MNIST was constructed sometime before summer 1994.

```{r}
#| label: read-xmpl-train
# Read a tibble (created in another project) that represents 
# a minimal subset of one (training set) image per distinct label.

xmpl_train_image_tbl <- read_tsv(here::here(
  "data", "mnist_xmpl_per_digit", "xmpl_train_image_tbl.txt"
))

```

Here's an example of each handwritten digit from the training set of images.

```{r}
#| label: fig-xmpl-train
#| fig-cap: "Example images of handwritten digits from the MNIST dataset"

g_xampl_train <- xmpl_train_image_tbl |> 
  arrange(label, img_dx, col_dx, row_dx) |> 
  ggplot2::ggplot(aes(x = col_dx, y = row_dx, fill = pixel)) +
  geom_raster() +  
  scale_fill_gradient(low = "white", high = "black") +
  scale_y_reverse() +
  facet_wrap(~ label, nrow = 2, ncol = 5,
             labeller = labeller(label = \(x) paste("Digit:", x))) +
  coord_equal() +
  theme_void() +
  theme(
    strip.text = element_text(size = 11, face = "bold"),
    legend.position = "none",
    panel.spacing = unit(0.5, "lines")
  )
g_xampl_train

```

Each image is represented by a $28 \times 28$ matrix of pixels, with each pixel represented as a grayscale integer value from 0 through 255.  That is, each image represents a single vector in a space of dimension 784 (since $28 \times 28 = 784$).

The 1992 competition prompted the development of algorithms to determine the decimal digit represented by any such image.  This is a classification problem: to label each case of data (image) as belonging to one of several possible categories (decimal digits).

One such method, multinomial logistic regression, assigns a probability that a given image represents a specified digit, resulting in a 10-element probability vector per image. [^wiki-multinomial-regression]

[^wiki-multinomial-regression]: See @wiki_multinomial_regression.

#### Multinomial Logistic Regression

To formulate the model, we convert the representation of an image from a $28 \times 28$ matrix of pixels into a vector of pixels of length 784. [^row-major-format]  We'll denote such a vector as $(P_1, \ldots, P_d)$, where $d = 784$.

[^row-major-format]: The conversion of a matrix of pixels to a vector of pixels is known as raster-to-vector (R2V) conversion, usually in row-major format, whereby the elements of the vector are taken from the top row and then from each succeeding row.  See @wiki_raster.

Let $D$ denote the digit represented by the image.  The ordering of the digits from 0 through 9 is not directly relevant to the image-recognition problem, so let us regard $D$ as a categorical variable having the set $\{ 0, 1, \ldots, 9 \}$ as possible values.  An alternative representation is the set of indicator vectors $e_0 = (1, 0, \ldots, 0)$ through $e_9 = (0, 0, \ldots, 1)$, called "one-hot encoding" in machine learning. [^one-hot]

[^one-hot]: See @wiki_one-hot and @wiki_categorical_variable.

Then the multinomial logistic regression model can be formulated as follows.

$$
\begin{align} 
  \log_e{ \frac{P(D = \nu)}{P(D = 0)} } &= (1, P_1, \ldots, P_d) \times 
    \begin{pmatrix} 
      \beta_0^{(\nu)} \\ \beta_1^{(\nu)} \\ \vdots \\ \beta_d^{(\nu)}
    \end{pmatrix} & \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align} 
$$ {#eq-log-ratio-per-image}

with 

$$
\begin{align} 
  P(D = 0) &= 1 - \sum_{\nu = 1}^9 P(D = \nu)
\end{align} 
$$ {#eq-log-ratio-per-image-2}

For a more compact notation let $X_{\bullet} = (1, P_1, \ldots, P_d)$ and let $\beta_{\bullet}^{(\nu)} = (\beta_0^{(\nu)}, \beta_1^{(\nu)}, \ldots, \beta_d^{(\nu)})$, with the inner product of these two vectors denoted as $X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}$.  Then we have 

$$
\begin{align} 
  \log_e{ \frac{P(D = \nu)}{P(D = 0)} } &= X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)} & \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align} 
$$ {#eq-log-ratio-per-image-3}

Exponentiation of @eq-log-ratio-per-image-3 gives:

$$
\begin{align} 
  \{ P(D = \nu) \} &= \{ P(D = 0) \} \times e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}} & \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align} 
$$ {#eq-log-ratio-per-image-4}

Taking the sum over $\nu$ we have: 

$$
\begin{align} 
  \sum_{\nu = 1}^9 {P(D = \nu)} &= \{ P(D = 0) \} \times \sum_{\nu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}}
\end{align} 
$$ {#eq-log-ratio-per-image-5}

Now applying @eq-log-ratio-per-image-2 we have 

$$
\begin{align} 
  \left \{ 1 - P(D = 0) \right \} &= \{ P(D = 0) \} \times \sum_{\nu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}}
\end{align} 
$$ {#eq-log-ratio-per-image-6}

which yields: 

$$
\begin{align} 
  P(D = 0) &= \frac{1} { 1 + \sum_{\nu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}} }
\end{align} 
$$ {#eq-log-ratio-per-image-7}

Applying @eq-log-ratio-per-image-4 gives: 

$$
\begin{align} 
  P(D = \nu) &= \frac{ e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)}} } { 1 +  \sum_{\mu = 1}^9 e^{X_{\bullet} \boldsymbol\cdot \beta_{\bullet}^{(\mu)}} } & \text{ for } \nu \in \{ 1, \ldots, 9 \}
\end{align} 
$$ {#eq-log-ratio-per-image-8}

#### Matrix Representation

@eq-log-ratio-per-image-3 pertains to the probability that a single image represents a single digit $\nu \in \{1, \ldots, 9 \}$.  Therefore, in a data set of $n$ images, with $i$ denoting the index of a particular image, we have:

$$
\begin{align} 
  \log_e{ \frac{P(D_i = \nu)}{P(D_i = 0)} } &= X_{i, \bullet} \boldsymbol\cdot \beta_{\bullet}^{(\nu)} 
\end{align} 
$$ {#eq-log-ratio-per-image-9}

Expanding the last equation to matrix notation, with $i$ as the row index and $\nu$ as a column index, we have

$$
\begin{align} 
&
\begin{pmatrix} 
  \log_e{ \frac{P(D_1 = 1)}{P(D_1 = 0)} }, & \ldots, & \log_e{ \frac{P(D_1 = 9)}{P(D_1 = 0)} } \\ 
  \vdots & \vdots & \vdots \\
  \log_e{ \frac{P(D_n = 1)}{P(D_n = 0)} }, & \ldots, & \log_e{ \frac{P(D_n = 9)}{P(D_n = 0)} }
\end{pmatrix}  \\ \\ 
&= 
\begin{pmatrix} 
  X_{1, \bullet} \\ 
  \vdots \\
  X_{n, \bullet}
\end{pmatrix}
\begin{pmatrix} 
  \beta_{\bullet}^{(1)}, & \ldots, & \beta_{\bullet}^{(9)}
\end{pmatrix}
\end{align} 
$$ {#eq-log-ratio-per-image-10}

The matrix on the left side of @eq-log-ratio-per-image-10 has dimensions $n \times 9$.  On the right side, the first matrix factor has dimensions $n \times 785$, and the second matrix factor has dimensions $785 \times 9$.

## Notation

The preceding section introduced example data sets along with corresponding models of the following form.

$$
\begin{align} 
  y &= X \; \beta \; + \; \epsilon
\end{align} 
$$ {#eq-generic-lm}

Each of the elements of @eq-generic-lm has alternative names, including the following. [^feature-matrix]

[^feature-matrix]: Matrix $X$ on the right side of @eq-generic-lm is called a _feature matrix_ that may contain original data columns (other than the response or labeling variable) and may also contain columns that are functions of the data or of other information.  A data matrix is model-agnostic, whereas a feature matrix is constructed to support a model of some form.

$$
\begin{align} 
  y &= \text{a } \textit{response, target,} \text{ or } \textit{labeling}  \text{ variable} \\ 
  X &= \text{columns of } \textit{explanatory, predictor,} \text{ or } \textit{feature}  \text{ variables} \\ 
  \beta &= \text{a vector of model } \textit{coefficients} \text{ or } \textit{parameters} \\ 
  \epsilon &= \text{an } \textit{error} \text{ or } \textit{residual} \text{ term}
\end{align} 
$$ {#eq-generic-lm-terms}

This linear regression format follows the more general mathematical notation $y = f(x)$.  In data science and machine learning, however, the response variable $y$ and the feature matrix $X$ have known values, whereas $\beta$ and $\epsilon$ are determined or evaluated over the course of the modeling process.

In the data examples of the preceding section, the response variable took the following form.

  - Family heights: $y =$ oldest child's height
  - Better Life Index: $y =$ the Life Satisfaction indicator
  - MNIST: $y =$ a probability vector $\{ P(D = \nu) \}_{\nu = 0}^9$ assigned to each image

The MNIST example illustrates a _vector-valued_ rather than _scalar-valued_ response variable.

If the data include a labeling or response variable, $y$, then the problem is said to be _supervised_.  In _unsupervised_ problems (that lack a $y$ variable), we may need to find patterns in the given data.  For example we may seek those feature variables (columns of the feature matrix $X$), or linear combinations of feature variables, that account for most of the variability in the entire set of feature variables.  Or we may need to find observations (rows of the feature matrix $X$) that are similar and thereby form groups (or _clusters_) of observations.  In these unsupervised situations we may model the data matrix (or covariance matrix) as the product of other matrices of special form (to be discussed later in this chapter).

In the remainder of this chapter we will focus on ideas and methods that help us to solve @eq-generic-lm, or rather, that help us to determine the value of $\beta$ that minimizes (in some sense) the residual term $\epsilon$.

## Geometry

In this section we'll discuss the example of family heights, in particular the linear regression of the son's height on the heights of the mother and father.  We'll discuss the geometry of the least-squares solution as a reference point for the remainder of this chapter.

### Family heights

Here's the linear regression problem in matrix format.

$$
\begin{align} 
  y_\bullet &= X_{_\bullet, _\bullet} \; \beta_\bullet \; + \; \epsilon_\bullet
\end{align} 
$$ {#eq-generic-lm-mat}

where 

$$
\begin{align} 
  y_\bullet &= \text{heights of sons } = s_\bullet \\ 
  m_\bullet &= \text{heights of mothers } \\ 
  f_\bullet &= \text{heights of fathers } \\ 
  X_{_\bullet, _\bullet} &= \text{feature matrix} = (1_\bullet, m_\bullet, f_\bullet) \\ 
  \beta_\bullet &= \text{coefficient vector} = (\beta_0, \beta_1, \beta_2)^\top \\ 
  \epsilon_\bullet &= \text{residual vector}
\end{align} 
$$ {#eq-generic-lm-2}

```{r}
#| label: smf-smpl

n_smf        <- nrow(s_mf_tbl)
n_smf_smpl   <- 20

set.seed(37)
smf_smpl_idx <- sample.int(n = n_smf, size = n_smf_smpl)
smf_smpl_tbl <- 
  s_mf_tbl [smf_smpl_idx, ] |> 
  dplyr::select(- s_hat)

```

```{r}
#| label: smf-smpl-lm

# fit linear model to sampled data
smf_smpl_lm <- lm(
  data    = smf_smpl_tbl, 
  formula = son ~ mother + father
)

# append columns: (fitted values, residuals)
smf_smpl_tbl <- smf_smpl_tbl |> 
  dplyr::mutate(
    s_hat = smf_smpl_lm$ fitted.values, 
    resid = smf_smpl_lm$ residuals
  )
```

Consider the least-squares estimate $\hat{\beta}_\bullet$ and the consequent predicted height $\hat{y}_\bullet = X_{_\bullet, _\bullet} \hat{\beta}_\bullet$ of the son.  @fig-smf-smpl is based on a random sample of `r n_smf_smpl` families and shows the heights of sons on the vertical axis, along with their vertical displacement (residual) from the predicted value lying on the regression plane. [^son_resid]

[^son_resid]: The residual son's height is the error term in the regression formula.  In the figure, residuals are color-coded according to their sign: black if positive and red otherwise. 

```{r}
#| label: fig-smf-smpl
#| fig-cap: "Sampled son heights: residual = observed - predicted"

g_smf_smpl <- scatterplot3d(
  x = smf_smpl_tbl$ mother, xlab = "mother", 
  y = smf_smpl_tbl$ father, ylab = "father", 
  z = smf_smpl_tbl$ son,    zlab = "son", 
  pch = 16, 
  color = "steelblue",
  angle = 55, 
  main = "Sampled family heights: mother, father, son", 
  sub  = "son residual = observed - predicted")

# Add the regression plane
s_mf_lm |> g_smf_smpl$ plane3d(
  lty          = "solid",
  lty.box      = "solid",
  draw_polygon = TRUE,
  col          = "grey")

# Add line segments from predicted to actual values
# The key is using g_smf_smpl$xyz.convert() to get the 2D coordinates
for(i in 1:nrow(smf_smpl_tbl)) {
    m     <- smf_smpl_tbl$ mother [i]
    f     <- smf_smpl_tbl$ father [i]
    s     <- smf_smpl_tbl$ son    [i]
    s_hat <- smf_smpl_tbl$ s_hat  [i]
    resid <- smf_smpl_tbl$ resid  [i]
    r_clr <- ifelse(resid > 0, "black", "red")
    
  g_smf_smpl$ points3d(
    c(m, m),
    c(f, f),
    c(s_hat, s),
    type = "l", col = r_clr, lwd = 1
  )
}

```

@fig-smf-smpl represents individual rows of data, $\{ (m_i, f_i, s_i) \}_{i = 1}^n$ along with model predictions and residuals in three-dimensional $(m, f, s)$ space.

To gain more insight into linear regression we'll first reduce the regression problem to the simple case in which the response variable and the predictor variables have all been coerced to have an average value of zero, a process called _centering_.  This will eliminate the need for the intercept coefficient, $\beta_0$, and consequently eliminate the need to include the constant vector $1_\bullet$ in the feature matrix $X_{_\bullet, _\bullet}$.

### Centering Data Vectors

The regression plane that we glimpse in @fig-smf-smpl actually spans all the $(m, f)$ combinations that are mathematically possible.  If we imagine infinitesimally short parents with $(m, f) = (0, 0)$, the predicted height of their son would be $\hat{\beta}_0$, which is not zero.  That is, the plane does not pass through the origin $(0, 0, 0)$ and therefore does not qualify as a subspace of $(m, f, s)$ space. [^lin-manifold] But the regression plane determines a parallel subspace (that passes through the origin).

[^lin-manifold]:  The regression plane qualifies as a linear manifold, mathematically speaking.

One way to generate this subspace is to center each of the $(m_i, f_i, s_i)$ data values, that is, to replace data value $v_i$ with its centered version $\dot{v}_i = v_i - \bar{v}$, where $\bar{v}$ denotes the average value (arithmetic mean) of vector $v_\bullet$.

Now it turns out that the regression plane passes through the theoretical _point of averages_, $(\bar{m}, \bar{f}, \bar{s})$.  If we were to center the $(m_\bullet, f_\bullet, s_\bullet)$ vectors, and regress the centered son's height against centered versions of the (mother, father) heights, the new regression plane would pass through the origin and thus qualify as a subspace.  That is, the fitted intercept coefficient of the centered regression problem must be zero.  Therefore we can eliminate the intercept coefficient from the centered linear model, and we can also eliminate the constant vector $1_\bullet$ from the feature matrix $X_{_\bullet, _\bullet}$. Then we have the following centered linear model.

$$
\begin{align} 
  \dot{y}_\bullet &= \dot{X}_{_\bullet, _\bullet} \; \beta_\bullet \; + \; \epsilon_\bullet
\end{align} 
$$ {#eq-generic-lm-ctr}

where 

$$
\begin{align} 
  \dot{y}_\bullet &= \text{centered heights of sons } = \dot{s}_\bullet \\ 
  \dot{m}_\bullet &= \text{centered heights of mothers } \\ 
  \dot{f}_\bullet &= \text{centered heights of fathers } \\ 
  \dot{X}_{_\bullet, _\bullet} &= \text{centered feature matrix} = (\dot{m}_\bullet, \dot{f}_\bullet) \\ 
  \beta_\bullet &= \text{coefficient vector} = (\beta_1, \beta_2)^\top \\ 
  \epsilon_\bullet &= \text{residual vector}
\end{align} 
$$ {#eq-generic-lm-ctr-2}

```{r}
#| label: smf-smpl-ctr

# center data variables using base::scale()
smf_smpl_ctr <- smf_smpl_tbl |> 
  dplyr::select(- c(s_hat, resid)) |> 
  dplyr::mutate(dplyr::across(
    .cols = c(mother, father, son), 
    .fns  = scale, scale = FALSE
  )) |> 
  # change scale() output from matrix to vector
  dplyr::mutate(dplyr::across(
    .cols = c(mother, father, son), 
    .fns  = as.vector
  ))
```

```{r}
#| label: smf-smpl-ctr-lm

# fit linear model to centered variables
smf_smpl_ctr_lm <- lm(
  data    = smf_smpl_ctr, 
  formula = son ~ 0 + mother + father)
```

```{r}
#| label: smf-smpl-ctr-append

# append (fitted values, residuals)
smf_smpl_ctr <- smf_smpl_ctr |> 
  dplyr::mutate(
    s_hat = smf_smpl_ctr_lm$ fitted.values, 
    resid = smf_smpl_ctr_lm$ residuals
  )
```

@fig-smf-smpl-ctr is a version of @fig-smf-smpl corresponding to @eq-generic-lm-ctr.  Geometrically it's the same figure, the difference being that each of the three axes has been shifted, now with 0 as the central value.

```{r}
#| label: fig-smf-smpl-ctr
#| fig-cap: "Centered family heights"

g_smf_smpl_ctr <- scatterplot3d(
  x = smf_smpl_ctr$ mother, xlab = "mother", 
  y = smf_smpl_ctr$ father, ylab = "father", 
  z = smf_smpl_ctr$ son,    zlab = "son", 
  pch = 16, 
  color = "steelblue",
  angle = 55, 
  main = "Centered family heights: mother, father, son", 
  sub  = "son residual = observed - predicted")

# Add the regression plane
smf_smpl_ctr_lm |> g_smf_smpl_ctr$ plane3d(
  lty          = "solid",
  lty.box      = "solid",
  draw_polygon = TRUE,
  col          = "grey")

# Add line segments from predicted to actual values
# The key is using g_smf_smpl_ctr$xyz.convert() to get the 2D coordinates
for(i in 1:nrow(smf_smpl_ctr)) {
    m     <- smf_smpl_ctr$ mother [i]
    f     <- smf_smpl_ctr$ father [i]
    s     <- smf_smpl_ctr$ son    [i]
    s_hat <- smf_smpl_ctr$ s_hat  [i]
    resid <- smf_smpl_ctr$ resid  [i]
    r_clr <- ifelse(resid > 0, "black", "red")
    
  g_smf_smpl_ctr$ points3d(
    c(m, m),
    c(f, f),
    c(s_hat, s),
    type = "l", col = r_clr, lwd = 1
  )
}

```

### Least Squares Solutions

Having centered the data, let's discuss least-squares linear regression, which determines coefficient values $(\hat{\beta}_0, \hat{\beta}_1)$ that minimize the sum of squared residuals.

$$
\begin{align} 
  \sum_{i = 1}^n \epsilon_i^2 &= \lVert \epsilon_\bullet \rVert^2 \\ 
  &= \epsilon_\bullet^\top\epsilon_\bullet \\ 
  &= (\dot{y}_\bullet - \dot{X}_{\bullet, \bullet} \beta_\bullet)^\top (\dot{y}_\bullet - \dot{X}_{\bullet, \bullet} \beta_\bullet)
\end{align} 
$$ {#eq-resid-ss}

To find coefficient values that minimize this sum of squares, one can take derivatives of the above expression with respect to $\beta_\bullet$ and set that result to zero.  This gives the following _normal equations_: 

$$
\begin{align} 
  \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet} \hat{\beta}_\bullet 
  &= \dot{X}_{\bullet, \bullet}^\top \dot{y}_\bullet
\end{align} 
$$ {#eq-lin-reg-nrml}

On the left side of the normal equations we have the matrix factor $\left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )$, which in our case is a multiple of the (mother, father) covariance matrix, a $2 \times 2$ non-negative-definite matrix. In fact the matrix is positive-definite, and thus invertible, provided only that parental heights are not perfectly correlated (which they are not).  Then we can invert this matrix to solve for $\hat{\beta}_\bullet$. 

$$
\begin{align} 
  \hat{\beta}_\bullet 
  &= \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \dot{y}_\bullet
\end{align} 
$$ {#eq-lin-reg-beta-hat}

The predicted vector $\hat{\dot{y}}_\bullet$ is thus: 

$$
\begin{align} 
  \hat{\dot{y}}_\bullet 
  &= \dot{X}_{\bullet, \bullet} \hat{\beta}_\bullet \\ 
  &= \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \dot{y}_\bullet
\end{align} 
$$ {#eq-lin-reg-y-hat}

### Data Vectors in $n-$Dimensional Space

Now whatever the value of the coefficient vector $\beta_\bullet$, the mapping $\beta_\bullet \mapsto \dot{X}_{\bullet, \bullet} \beta_\bullet$ sends $\beta_\bullet$ to a 2-dimensional subspace of $n-$space, where $n$ denotes the number of data cases, i.e., the row dimension of $\dot{X}_{\bullet, \bullet}$.  This 2-dimensional subspace is spanned by the two $n-$vectors $(\dot{m}_\bullet, \dot{f}_\bullet)$, the centered vectors of (mother, father) heights.

The particular coefficient vector $\hat{\beta}_\bullet$ obtained by least squares linear regression has a special form.  That is, @eq-lin-reg-y-hat gives a linear mapping from the centered heights of sons $\dot{y}_\bullet$ to their corresponding linear prediction $\hat{\dot{y}}_\bullet$.  Let $P$ denote the matrix of that mapping.

$$
\begin{align} 
  P
  &= \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top
\end{align} 
$$ {#eq-lin-reg-projection}

Matrix $P$ is symmetric and idempotent, that is, both the transpose and the squaring of $P$ equal $P$. 

$$
\begin{align} 
  P^\top
  &= \left \{ \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \right \}^\top \\ 
  &=  \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \\ 
  &= P \\ \\
  P^2
  &= \left \{ \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \right \} 
  \left \{ \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \right \} \\ 
  &= \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \\ 
  &= P
\end{align} 
$$ {#eq-projection-properties}

An idempotent matrix $M$ is called a _projection_.  If $M^2 = M$, then repeated applications of $M$ to vector $v$ return the initial application, i.e., $M^k v = Mv$ for any positive integer $k$.  If in addition matrix $M$ is symmetric (as is $P$), then $M$ is said to be an _orthogonal projection_.  In this case the complement of $M$, $I - M$, also qualifies as an orthogonal projection and the product of the two matrices is the zero matrix.  Any vector $v$ can be expressed as the sum of two vectors $v = x + y$ that are mutually orthogonal $(x^\top y = 0)$, namely $x = M v$ and $y = (I-M) v$.

In our example, this means that the respective vectors of predicted centered heights $\hat{\dot{y}_\bullet}$ and their residuals $\hat{\epsilon}_\bullet$ are mutually orthogonal.

$$
\begin{align} 
  \hat{\dot{y}}_\bullet &= P \; \dot{y}_\bullet \\ \\ 
  \hat{\epsilon}_\bullet &= \dot{y}_\bullet - \hat{\dot{y}}_\bullet \\ 
  &= (I - P) \; \dot{y}_\bullet \\ \\ 
  \hat{\epsilon}_\bullet^\top \; \dot{y}_\bullet &= \dot{y}_\bullet^\top \; (I - P)^\top P \; \dot{y}_\bullet \\ 
  &= 0
\end{align} 
$$ {#eq-resid-predicted-ortho}

Now let $v_\bullet$ be any vector in the parental space.  Then $v_\bullet$ is some linear combination of the parental vectors $(\dot{m}_\bullet, \dot{f}_\bullet)$ and therefore can be represented as $v_\bullet = \dot{X}_{\bullet, \bullet} \gamma_\bullet$ for some coefficient vector $\gamma_\bullet = (\gamma_1, \gamma_2)^\top$.  It now follows the $P \; v_\bullet = v_\bullet$: 

$$
\begin{align} 
  P \; v_\bullet &= P \; (\dot{X}_{\bullet, \bullet} \gamma_\bullet) \\ 
  &= \dot{X}_{\bullet, \bullet} \left ( \dot{X}_{\bullet, \bullet}^\top \dot{X}_{\bullet, \bullet}  \right )^{-1} \dot{X}_{\bullet, \bullet}^\top \;  (\dot{X}_{\bullet, \bullet} \gamma_\bullet) \\ 
  &= \dot{X}_{\bullet, \bullet} \gamma_\bullet \\ 
  &= v_\bullet
\end{align} 
$$ {#eq-subspace-invariance}

Consequently, the residual vector $\hat{\epsilon}_\bullet$ is orthogonal to any vector $v_\bullet = \dot{X}_{\bullet, \bullet} \gamma_\bullet$ in the parental subspace: 

$$
\begin{align} 
  \hat{\epsilon}_\bullet^\top \; v_\bullet &= \dot{y}_\bullet^\top \; (I - P)^\top v_\bullet \\ 
  &= \dot{y}_\bullet^\top \; (I - P) \; v_\bullet \\ 
  &= \dot{y}_\bullet^\top \; 0_\bullet \\ 
  &= 0
\end{align} 
$$ {#eq-resid-mf-ortho}

It now follows that of all vectors $v_\bullet = \dot{X}_{\bullet, \bullet} \gamma_\bullet$ in the parental subspace, the predicted vector $\hat{\dot{y}}_\bullet$ is closest to the given vector $\dot{y}_\bullet$:

$$
\begin{align} 
  \lVert \dot{y_\bullet} - v_\bullet \rVert^2 
  &= \lVert (\dot{y_\bullet} - \hat{\dot{y}}_\bullet) + (\hat{\dot{y}}_\bullet - v_\bullet) \rVert^2  \\ 
  &= \lVert \hat{\epsilon}_\bullet + (\hat{\dot{y}}_\bullet - v_\bullet) \rVert^2 \\ 
  &= \left ( \hat{\epsilon}_\bullet + (\hat{\dot{y}}_\bullet - v_\bullet) \right )^\top 
  \left ( \hat{\epsilon}_\bullet + (\hat{\dot{y}}_\bullet - v_\bullet) \right ) \\
  &= \hat{\epsilon}_\bullet^\top \hat{\epsilon}_\bullet \; + \; 0 \; + \; 0 \; + \; (\hat{\dot{y}}_\bullet - v_\bullet)^\top (\hat{\dot{y}}_\bullet - v_\bullet) \\ 
  &= \lVert \hat{\epsilon}_\bullet \rVert^2 \; + \; \lVert \hat{\dot{y}}_\bullet - v_\bullet \rVert^2 \\
  &\ge \lVert \hat{\epsilon}_\bullet \rVert^2 \\ 
  &= \lVert \dot{y_\bullet} - \hat{\dot{y}}_\bullet \rVert^2
\end{align} 
$$ {#eq-lin-reg-min-distance}

To summarize, 

  - The parental centered heights $(\dot{m}_\bullet, \dot{f}_\bullet)$ constitute a basis of the 2-dimensional subspace that they span within $n-$space.
  - Matrix $P$ sends the vector $\dot{y}_\bullet$ of sons' centered heights to prediction vector $\hat{\dot{y}_\bullet}$.
  - This mapping is an orthogonal projection onto the parental subspace.
  - Moreover, $(\hat{\beta}_1, \hat{\beta}_2)$ are the coordinates of the prediction vector in this subspace with respect to the parental basis.
  - Of all the vectors in the parental subspace, the prediction vector $\hat{\dot{y}_\bullet}$ is the closest (in Euclidean distance) to the given vector $\dot{y}_\bullet$.

Figure xxx illustrates these points.

```{r}
#| label: fig-ortho-projection
#| fig-cap: "TBD"

# After computing your centered vectors and projection
# Visualize in the 3D subspace spanned by m_dot, f_dot, and y_dot

# map xxx_dot variables below to smf_smpl_ctr columns
m_dot     <- smf_smpl_ctr$ mother
f_dot     <- smf_smpl_ctr$ father
y_dot     <- smf_smpl_ctr$ son
y_hat_dot <- smf_smpl_ctr$ s_hat

# Create basis vectors for visualization
basis1 <- m_dot / norm(m_dot, "2")  # normalized mother heights
basis2 <- f_dot - sum(basis1 * f_dot) * basis1  # Gram-Schmidt
basis2 <- basis2 / norm(basis2, "2")

# Project all relevant vectors onto these 2D + perpendicular direction
coords_m <- c(sum(m_dot * basis1), sum(m_dot * basis2), 0)
coords_f <- c(sum(f_dot * basis1), sum(f_dot * basis2), 0)
coords_y <- c(sum(y_dot * basis1), sum(y_dot * basis2), 
              norm(y_dot - y_hat_dot, "2"))
coords_yhat <- c(sum(y_hat_dot * basis1), sum(y_hat_dot * basis2), 0)

# Create 3D plot with plotly
fig <- plot_ly() %>%
  # Add vectors
  add_trace(x = c(0, coords_m[1]), y = c(0, coords_m[2]), 
            z = c(0, coords_m[3]), type = "scatter3d", mode = "lines+markers",
            line = list(color = "red", width = 4), name = "m̃",
            marker = list(size = 4)) %>%
  add_trace(x = c(0, coords_f[1]), y = c(0, coords_f[2]), 
            z = c(0, coords_f[3]), type = "scatter3d", mode = "lines+markers",
            line = list(color = "blue", width = 4), name = "f̃",
            marker = list(size = 4)) %>%
  add_trace(x = c(0, coords_y[1]), y = c(0, coords_y[2]), 
            z = c(0, coords_y[3]), type = "scatter3d", mode = "lines+markers",
            line = list(color = "green", width = 4), name = "ỹ",
            marker = list(size = 4)) %>%
  add_trace(x = c(0, coords_yhat[1]), y = c(0, coords_yhat[2]), 
            z = c(0, coords_yhat[3]), type = "scatter3d", mode = "lines+markers",
            line = list(color = "purple", width = 4), name = "ŷ̃",
            marker = list(size = 4)) %>%
  # Add residual (perpendicular line)
  add_trace(x = c(coords_yhat[1], coords_y[1]), 
            y = c(coords_yhat[2], coords_y[2]),
            z = c(coords_yhat[3], coords_y[3]), 
            type = "scatter3d", mode = "lines",
            line = list(color = "orange", width = 3, dash = "dash"), 
            name = "residual",
            marker = list(size = 4)) %>%
  layout(scene = list(
    xaxis = list(title = "Basis 1 (m̃ direction)"),
    yaxis = list(title = "Basis 2 (f̃ direction)"),
    zaxis = list(title = "Perpendicular to plane"),
    aspectmode = "cube"
  ))

fig
```

### Least Squares: Two Perspectives

### Data Visualization

- The 3D scatterplot view: each point is (mother_height, father_height, son_height)
- The fitted regression plane: ŷ = β₀ + β₁·mother + β₂·father
- This is how we visualize and interpret, but NOT where the projection occurs
- Dimension: p = 3 (number of features including intercept)

### Sample Space

- Each observation is a vector in ℝⁿ (n = 179)
- The response vector y ∈ ℝ¹⁷⁹ has components [y₁, y₂, ..., y₁₇₉]ᵀ
- Each column of X is also a vector in ℝ¹⁷⁹
- Example: the "mother_heights" column is one vector in 179-dimensional space

### Column Space

- C(X) is the 3-dimensional subspace of ℝ¹⁷⁹ spanned by the columns of X
- All possible linear combinations: Xβ for any β ∈ ℝ³
- Key insight: C(X) contains all predictions our model can make
- Dimension: rank(X) ≤ min(n,p) = 3 (assuming full column rank)

### Orthogonal Projection

- ŷ = Xβ̂ is the orthogonal projection of y onto C(X)
- The residual vector e = y - ŷ is orthogonal to C(X)
- Geometric interpretation: ŷ is the point in C(X) closest to y (in L₂ distance)
- The normal equations: XᵀX β̂ = Xᵀy arise from orthogonality condition Xᵀe = 0
- Include a 2D schematic diagram: y, C(X) as a plane, ŷ as projection, e perpendicular

### Reconciling Perspectives

- The 3D scatterplot shows relationships between variables (p-dimensional)
- The projection happens in observation space (n-dimensional)
- Both are valid and useful for different purposes
- The regression coefficients β connect the two views

### Why This Matters

- Degrees of freedom: n - p (observations minus parameters)
- Overfitting: what happens when p approaches n?
- Preview: other norms (L₁) change the geometry but still live in ℝⁿ
